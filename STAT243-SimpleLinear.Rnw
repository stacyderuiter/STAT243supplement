\Sexpr{set_parent('Math241-S14.Rnw')}

\chapter{Linear Models}

In Chapter~\ref{chap:propagation} we learned how to estimate one quantity based
on its (known) relationship to other quantities.  For example, we estimated the
number of dimes in a sack of dimes from our estimates of the weight of the
dimes and the average weight of a dime.  

In this chapter we will explore how to use data to determine the relationship among
two or more variables when this relationship is not known in advance.  The general
framework we will use is 

\[
Y = f(x_1, x_2, \dots, x_k) + \varepsilon
\]
\begin{itemize}
	\item $Y$ is the \term{response} variable that we are trying to estimate
		from $k$ \term{explanatory} or \term{predictor} variables $x_1, x_2, \dots, x_k$.
	\item
		The relationship between the explanatory variables and the response 
		variables is described by a function $f$.
	\item
		The relationship described by $f$ need not be a perfect fit.  The \term{error}
		term in the model, $\varepsilon$, describes how individual responses
		differ from the value give by $f$.  
		
		We will model $\varepsilon$ with a 
		distribution -- typically a distribution with a mean of 0 -- 
		so another way to think about this model is the for a given 
		values of the predictors, the values of $Y$ have a distribution.  The mean
		of this distribution is specified by $f$ and the shape by $\varepsilon$.
\end{itemize}


\section{The Simple Linear Regression Model}

\[
Y = \beta_0 + \beta_1 x + \varepsilon  \qquad \mbox{where $\varepsilon \sim \Norm(0,\sigma)$.}
\]

In other words:
\begin{itemize}
\item
The mean response for a given predictor value $x$ is given by a linear formula
\[
\mbox{mean response} = \beta_0 + \beta_1 x
\]
This can also be written as 
\[
\E(Y \mid X=x) = \beta_0 + \beta_1 x)
\]
\item
The distribution of all responses for a given predictor value $x$ is normal.
\item
The standard deviation of the responses is the same for each predictor value,
\end{itemize}
Furthermore, in this model the values of $\varepsilon$ are independent.

There are many different things we might want to do with a linear model, for example:
\begin{itemize}
	\item Estimate the coefficients $\beta_0$ and $\beta_1$.
	\item Estimate the value $Y$ associated with a particular value of $x$.
	\item Say something about how well a line fits the data.
\end{itemize}

\section{Fitting the Simple Linear Model}

\subsection{The Least Squares Method}

We want to determine the best fitting line to the data.  The usual method is 
the method of least squares\footnote{In this case, it turns out that the least 
squares and maximum likelihood methods produce exactly the same results.},
which chooses the line that has the 
\emph{ smallest possible sum of squares of residuals}, where residuals are defined by

\[
\mbox{residual} = \mbox{observed} - \mbox{predicted}
\]

\begin{example}
Consider the following small data set.
\begin{multicols}{2}
<<tidy=FALSE>>=
someData <- data.frame(
  x=1:5, 
  y=c(1,3,2,4,4)
)
someData
@
<<tidy=FALSE,fig.height=3.0>>=
xyplot( y ~ x, 
  data=someData, type=c('p','g'), 
  ylim=c(0,5), xlim=c(0,6))
@
\end{multicols}

\begin{enumerate}
	\item
		Add a line to the plot that ``fits the data well".  Don't do any calculations,
		just add the line.
	\item
		Now estimate the residuals for each point relative to your line
	\item
		Compute the sum of the squared residuals, $SSE$.
	\item
		Estimate the slope and intercept of your line.
\end{enumerate}

\newpage
For example, suppose we we select a line that passes through $(0,1)$ and $(5,4)$. 
the equation for this line is $y = 1 + .6 x$, and it looks like a pretty good fit:
<<>>=
xyplot( y ~ x, data=someData, xlim=c(0,6), ylim=c(0,5) )
f <- makeFun( 1 + .6 * x ~ x)
plotFun( f(x) ~ x, add=TRUE, col="gray50" )
@
The residuals for this function are 
<<tidy=FALSE>>=
resids <- with(someData, y - f(x)) ; resids 
@
and $SSE$ is 
<<>>=
sum(resids^2)
@
If your line is a good fit, then $SSE$ will be small.  
The best fitting line will have the smallest possible $SSE$.   
The \function{lm()} function will find this best fitting line for us.
<<tidy=FALSE>>=
model1 <- lm( y ~ x, data=someData ); model1
@
This says that the equation of the best fit line is 
\[
\hat y = \Sexpr{coef(model1)[1]} + \Sexpr{coef(model1)[2]} x
\]

<<>>=
xyplot( y ~ x, data=someData, type=c('p','r') )
@
We can compute $SSE$ using the \function{resid()} function.
<<tidy=FALSE>>=
SSE <- sum ( resid(model1)^2 ); SSE
@
As we see, this is a better fit than our first attempt -- 
at least according to the least squares criterion.
It will better than \emph{any} other attempt -- it is the least 
squares regression line.
\end{example}

\subsection{Properties of the Least Squares Regression Line}
For a line with equation $y = \hat\beta_0 + \hat\beta_1 x$, the residuals are 
\[
e_i = y_i - (\hat\beta_0 + \hat\beta_1 x) 
\]
and the sum of the squares of the residuals is 
\[
SSE = \sum e_i^2  = \sum (y_i - (\hat\beta_0 + \hat\beta_1 x) )^2
\]
Simple calculus (which we won't do here) allows us to compute the 
best $\hat\beta_0$ and $\hat\beta_1$ possible.  
These best values define the least squares regression line.
We always compute these values using software, but it is good to note that 
the least squares line satisfies two very nice properties.
\begin{enumerate}
\item
The point $(\mean x, \mean y)$ is on the line. 

This means that $\mean y = \hat\beta_0 + \hat\beta_1 \mean x$  (and $\hat\beta_0 = \mean y - \hat\beta_1 \mean x$)
\item
The slope of the line is $\displaystyle b = r \frac{s_y}{s_x}$ where $r$ is the 
\term{correlation coefficient}:
\myindex{correlation coefficient}%
\[
r = \frac{1}{n-1} \sum \frac{ x_i - \mean x }{s_x} \cdot \frac{ y_i - \mean y }{s_y}
\]
\end{enumerate}
Since we have a point and the slope, it is easy to compute the equation for the line
if we know $\mean x$, $s_x$, $\mean y$, $s_y$, and $r$.

\subsection{An Example: Estimating OSA}
\begin{example}
In a study of eye strain caused by visual display terminals, researchers wanted
to be able to estimate ocular surface area (OSA) from palpebral fissure (the
horizontal width of the eye opening in cm) because palpebral fissue is easier
to measure than OSA.
<<>>=
require(Devore6)
head(xmp12.01, 3)
# note misspelling in this data set; let's fix it
names(xmp12.01)[1] <- "palpebral"
names(xmp12.01)
x.bar <- mean( ~palpebral, data=xmp12.01)  
y.bar <- mean( ~OSA, data=xmp12.01)
s_x <- sd( ~palpebral, data=xmp12.01)
s_y <- sd( ~OSA, data=xmp12.01)
r <- cor( xmp12.01$palpebral, xmp12.01$OSA)
c( x.bar = x.bar, y.bar=y.bar, s_x=s_x, s_y=s_y, r=r )
slope <- r * s_y/s_x
intercept <- y.bar - slope * x.bar
c(intercept=intercept, slope=slope)
@
\end{example}

Fortunately, statistical software packages do all this work for us, so the
caclulations of the preceding example don't need to be done in practice.

\begin{example}
In a study of eye strain caused by visual display terminals, researchers wanted
to be able to estimate ocular surface area (OSA) from palpebral fissure (the
horizontal width of the eye opening in cm) because palpebral fissue is easier
to measure than OSA.
<<>>=
osa.model <- lm( OSA ~ palpebral, data=xmp12.01) 
osa.model
@
\function{lm()} stands for linear model.  The default output includes the estimates
of the coefficients ($\hat\beta_0$ and $\hat \beta_1$) based on the data.  If that is the 
only information we want, then we can use 
<<>>=
coef(osa.model)
@

This means that the equation of the least squares regression line is 
\[
\hat y = \Sexpr{round(coef(osa.model)[1],3)} + \Sexpr{round(coef(osa.model)[2],3)} x
\]

We use $\hat y$ to indicate that this is not an observed value of the response variable
but an estimated value (based on the linear equation given).

\R\ can add a regression line to our scatter plot if we ask it to.
\begin{center}
<<osa-scatter>>=
xyplot( OSA ~ palpebral, data=xmp12.01, type=c('p','r') )
@
\end{center}

We see that the line does run roughly ``through the middle'' of the data but that
there is some variability above and below the line.
\end{example}

\subsection{Explanatory and Response Variables Matter}
It is important that 
the explanatory variable be the ``\variable{x}'' variable
and the response variable be the ``\variable{y}'' variable
when doing regression.  If we reverse the roles of \variable{OSA} and 
\variable{palpebral}
we do not get the same model.  This is because the residuals are measured vertically 
(in the $y$ direction).  


\section{Estimating the Response}

We can use our least squares regression line to estimate the value of the response
variable from the value of the explanatory variable.

\begin{example}
If the palpebral width is 1.2 cm, then we would estimate OSA to be 

\[
\hat{\texttt{osa}} = 
\Sexpr{round(coef(osa.model)[1],3)} + \Sexpr{round(coef(osa.model)[2],3)} \cdot 1.2 
= \Sexpr{round(coef(osa.model)[1],3) + round(coef(osa.model)[2],3) * 1.2} 
\]

\R\ can automate this for us too.  The \function{makeFun()} function will
create a function from our model.  
If we input a palpebral measurement into this function, the function
will return the estimated OSA.
<<>>=
estimated.osa <- makeFun(osa.model)
estimated.osa(1.2)
@

As it turns out, the 17th measurement in our data set had a
\variable{palpebral} measurement of 1.2 cm.
<<>>=
xmp12.01[17,]
@
The corresponding OSA of 3.76 means that the residual for this observation is 
\[
\mbox{observed} - \mbox{predicted} = 3.76 - \Sexpr{estimated.osa(1.2)} = 
\Sexpr{ 3.76 - estimated.osa(1.2)}  
\]
\end{example}

\subsection{Cautionary Note: Don't Extrapolate\!}
While it often makes sense to generate model predictions corresponding to x-values \emph{within} the range of values measured in the dataset, it is dangerous to \emph{extrapolate} and make predictions for values \emph{outside} the range included in the dataset.  To assume that the linear relationship observed in the dataset holds for explanatory variable values outside the observed range, we would need a convincing, valid justification, which is usually not available.  If we extrapolate anyway, we risk generating erroneous or even nonsense predictions.  The problem generally gets worse as we stray further from the observed range of explanatory-variable values.

\section{Parameter Estimates}
\subsection{Interpreting the Coefficients}
The coefficients of the linear model tell us how to construct the linear function
that we use to estimate response values, but they can be interesting in their own
right as well.

The intercept $\beta_0$ is the mean response value when the 
explanatory variable is 0.  This may or may not be interesting.
Often $\beta_0$ is not interesting because we are not interested
in the value of the response variable when the predictor is 0.  (That might not 
even be a possible value for the predictor.)  Furthermore, 
if we do not collect data with values of the explanatory variable near 0, then
we will be extrapolating from our data when we talk about the intercept. (Extrapolating is dangerous because we can't really be sure that the relationships we've uncovered with our model really hold for variable values outside the range we measured.)

The estimate for $\beta_1$, on the other hand, is nearly always of interest.
The slope coefficient $\beta_1$ tells us how quickly the response variable changes 
per unit change in the predictor.  This is an interesting value in many more situations.
Furthermore, when $\beta_1 = 0$, then our model does not depend on the predictor at all.
So if we construct a confidence interval for $\beta_1$, and it contains 0, then we do \emph{not} have sufficient evidence to be convinced that our predictor is of any use in predicting the response.

\subsection{Estimating $\sigma$}

There is one more parameter in our model that we have been mostly ignoring so far: $\sigma$ (or 
equivalently $\sigma^2$).  This is the parameter that describes how tightly things should 
cluster around the regression line.  We can estimate $\sigma^2$ from our residuals:

\begin{align*}
\hat\sigma^2 & = MSE = \frac{ \sum_i e_i^2 }{ n -2 }
\\
\hat\sigma & = RMSE = \sqrt{MSE} = \sqrt{\frac{ \sum_i e_i^2 }{ n -2 } }
\end{align*}
The acronyms $MSE$ and $RMSE$ stand for \term{Mean Squared Error} and 
\term{Root Mean Squared Error}.
The numerator in these expressions is the sum of the squares of the residuals
\[
SSE = \sum_i e_i^2 \;.
\]
This is precisely the quantity that we were minimizing to get our least squares fit.
\[
MSE = \frac{SSE}{DFE} 
\]
where $DFE = n-2$ is the \term{degrees of freedom} associated with the
estimation of $\sigma^2$ in a simple linear model.  We lose two degrees of
freedom when we estimate $\beta_0$ and $\beta_1$, just like we lost 1 degree of
freedom when we had to estimate $\mu$ in order to compute a sample variance.

$RMSE = \sqrt{MSE}$ is listed in the summary output for the linear model as the
\term{residual standard error} because it is the estimated standard deviation of 
the error terms in the model.
<<>>=
summary(osa.model)
@
We will learn about other parts of this summary output shortly.
Much is known about the estimator $\sigma^2$, including 
\begin{itemize}
	\item $\hat \sigma^2$ is unbiased (on average it is $\sigma^2$), and 
	\item
		the sampling distribution is a Chi-Squared distribution with
		$n-2$ degrees of freedom.  
		(Chi-Squared distributions are a special case of Gamma distributions.)
\end{itemize}


\section{Checking Assumptions}
\subsection{What have we assumed?}
In fitting a linear regression model, we have assumed:
\begin{itemize}
\item A linear relationship between the explanatory and response variables
\item The errors ($\epsilon$) are Normally distributed
\item Independence of the errors (in particular, no correlation over time between successive errors for data points collected over time)
\item Homoscedasticity of the errors -- this means that the variance (spread) of the errors is constant over time, and over the full range of explanatory and predictor variables
\end{itemize}

\subsection{Don't Fit a Line If a Line Doesn't Fit}

The least squares method can be used to fit a line to any data -- even if a line
is not a useful representation of the relationship between the variables.
When doing regression we should always look at the data to see if a line 
is a good fit.  If it is not, then the simple linear model is not a good choice and 
we should look for some other model that does a better job of describing the 
relationship between our two variables.  

\subsection{Checking the Residuals}
We look at the residuals (not just the data scatter plot) because some of our assumptions refer specifically to them.  Also, often, it is easier to assess the linear fit by looking at a plot of the 
residuals than by looking at the natural scatter plot, because on the scale 
of the residuals, violations of our assumptions are easier to see.

So, to verify that our linear regression assumptions are sensible, we can examine the model residuals. Residuals should be checked to see that their distribution looks approximately
normal and that thier standard deviation (the spread of the residuals) remains consistent across the range of our data (and across time).

In addition, especially if the data were collected over time (measurements made in order during an experiment; data points collected at a series of time points), it is important to verify that the residuals are \emph{independent} of one another over time.  To look for this problem, we can look at a scatter plot of the residuals as a function of time, and suspect a problem if we see series of very large, or very small, residuals all in a row.  Another plot that can help us look for non-independence in the residuals is a plot of the autocorrelation function (ACF), obtained using the \texttt{acf()} function in R.  This function computes and plots the correlation coefficient R for the residuals at various ``lags".  For example, the correlation coefficient for lag 1 is the correlation coefficient between each residual (corresponding to the \textit{i}th datapoint) and the preceding one (the \textit{i-1}th data point).  Lag 2 is between the \textit{i}th and \textit{i-2}th data point, and so on.  If the residuals are not independent, then these coefficients will have large absolute values.  (Note: the ``lag 0" coefficient measures the correlation of the \textit{i}th residual with itself, so it is always 1.  This does NOT indicate any problem with the linear regression model.)

In general, we might want to check the following plots of the residuals:
\begin{itemize}
\item Residuals as a function of ``fitted values", or model predictions for the x-values obseved in the actual data set.
\item Residuals as a function of the observed values of the explanatory variable (from the actual data set).
\item Normal quantile-quantile plot of the residuals (note: in Chapter 4, we made these by hand for any distribution; you may also use the shortcut function qqmath() as illustrated below, to make them for the Normal distribution.)
\item Residuals as a function of time (if you know the order in which they were collected, or if the explanatory variable is a time-related one).
\item Residual autocorrelation function plot (if the data points were collected over time, or if the explanatory variable is a time-related one).
\end{itemize}

For all the scatter plots, we want to make sure the residuals ``look random" -- the extent of the spread of the residuals should not vary with time or x or y (``trumpet"-shaped plot).  If there is a pattern, it suggests a problem with the homoscedasticity assumption.  There should not be long runs of similar residuals, especially over time; if there are, it suggests non-independence of the residuals.  There should also be no apparent trends in the plot, linear or non-linear; if there are, it suggests that the relationship between the predictor and response variables was not linear.  In an autocorrelation plot, the correlation coefficients (except for lag 0) should not be too large, far exceeding the dotted guide-lines on the plot; if they are, there is probably a problem with the independence assumption.

\begin{example}
Returning to our OSA data, we can obtain the residuals using the 
\function{resid()} function and plot them.
<<>>=
osa.hat <- makeFun(osa.model)
preds <- osa.hat(xmp12.01$palpebral)
xyplot( resid(osa.model) ~ preds, data=xmp12.01,
        main="Residuals versus fitted values")
qqmath( ~ resid(osa.model),  data=xmp12.01 , main='Normal QQ plot')
xyplot( resid(osa.model) ~ palpebral, data=xmp12.01,
        main="Residuals versus explanatory variable")
@

<<acfplot, fig.width=6, fig.height=5>>=
acf(resid(osa.model), ylab="Residual ACF", main="")
@
\end{example}
If the assumptions of the model are correct, there should be no distinct patterns to these scatter
plots of the residuals, and the normal-quantile plot should be roughly linear
(since the model says that differences between observed responses and the true linear fit
should be random noise following a normal distribution with constant standard deviation).

In this case things look pretty good. 

\subsection{Outliers in Regression}

Outliers can be very influential in regression, especially in small data sets,
and especially if they occur for extreme values of the explanatory variable.
Outliers cannot be removed just because we don't like them, but they should be
explored to see what is going on (data entry error? special case? etc.)

Some researchers will do ``leave-one-out'' analysis, or ``leave some out'' analysis
where they refit the regression with each data point left out once.  If the regression summary changes very little when we do this, this means that the regression line
is summarizing information that is shared among all the points relatively equally.
But if removing one or a small number of values makes a dramatic change, then
we know that that point is exerting a lot of influence over the resulting
analysis (a cause for caution).   

This kind of analysis can be very helpful, especially if you have one or several large potential outliers in your data set, but in this class, we will not generally do it as a matter of course (it's not a required part of model assessment for coursework).

\section{How Good Are Our Estimates?}
Assuming our diagnostics indicate that fitting a linear model is reasonable for our data,
our next question is \emph{How good are our estimates?}
Notice that there are several things we have estimated:
\begin{itemize}
	\item The intercept coefficient $\beta_0$ 
		[estimate: $\hat \beta_0$]
	\item The slope coefficient $\beta_1$ 
		[estimate: $\hat \beta_1$]
	\item Values of $y$ for given values of $x$. 
		[estimate: $\hat y = \hat \beta_0 + \hat \beta_1 x$]
\end{itemize}

We would like to be able to compute uncertainties and confidence intervals for these.
Fortunately, \R\ makes this straightforward.

\subsection{Estimating the $\beta$s}

\begin{example}
	\question
	Returning to the OSA data, compute standard uncertainties and 95\% confidence intervals
	for $\beta_0$ and $\beta_1$.

	\answer
	The \function{summary()} function provides additional information about the 
	model:
<<>>=
summary(osa.model)
@
	We don't know what to do with all of the information displayed here, but we can see
	some familiar things in the coefficient table.  
If we only want the coefficients part of the summary output we can get that using
<<>>=
coef(summary(osa.model))
@
	From this we see the estimates ($\hat \beta$'s)
	displayed again.  Next to each of those is a standard error.  That is the standard
	uncertainty for these estimates.  So we could report our estimated coefficients as 
	\[
	\beta_0: \Sexpr{sprintf("%.2f", coef(osa.model)[1])} \pm 
		\Sexpr{round(sqrt(diag(vcov(osa.model))[1]), 2)}
	\qquad
	\beta_1: \Sexpr{sprintf("%.2f", coef(osa.model)[2])} \pm 
		\Sexpr{round(sqrt(diag(vcov(osa.model))[2]), 2)}
	\]

	A confidence interval can be computed using
	\[
	\hat\beta_i \pm t_* SE_{\beta_i}
	\]
	because 
	\begin{itemize}
		\item
	the sampling distribution for $\hat \beta_i$ 
			is normal, 
		\item
	the sampling distribution for $\hat \beta_i$ 
			is unbiased (the mean is $\beta_i$), and
		\item
			the standard deviation of the sampling distribution depends on $\sigma$ 
			(and some other things), but
		\item
			we don't know $\sigma$, so we have to estimate it using $RMSE = \sqrt{MSE}$.
	\end{itemize}

<<tidy=FALSE>>=
t.star <- qt(.975, df=28); t.star    # n-2 degrees of freedom for simple linear regression
t.star * 0.151
@
	So a 95\% confidence interval for $\beta_1$ is 
	\[
	3.08 \pm \Sexpr{round(t.star * 0.151, 2)}
	\]
	The degrees of freedom used are $DFE = n-2$, the same as used in the estimate of $\sigma^2$.
	(We are using a t-distribution instead of a normal distribution because we don't know 
	$\sigma$.  The degrees of freedom are those associated with using $RMSE = \sqrt{MSE}$
	as our estimate for $\sigma$.)

	\R\ can compute confidence intervals for both parameters using the function
	\function{confint()}:
<<>>=
confint(osa.model)
@
A 68\% confidence interval should have a margin of error of approximately 1 standard
uncertainty:
<<>>=
confint(osa.model, level=0.68)
apply(confint(osa.model, level=0.68) , 1, diff) / 2  # half width of CIs
@

\end{example}

\subsection{Confidence and Prediction Intervals for the Response Value}
We can also create interval estimates for the response.    \R\ will compute
this if we simply ask:
<<>>=
estimated.osa <- makeFun(osa.model)
estimated.osa( 1.2, interval="confidence")
estimated.osa( 0.8, interval="confidence")
@
These intervals are confidence intervals for the \emph{mean} response.  Sometimes it
is desirable to create an interval that will have a 95\% chance of containing a new 
\emph{observation} -- that is, including the anticipated error as well as the mean response.  These intervals are called \term{prediction intervals} to distinguish
them from the usual confidence interval.
<<>>=
estimated.osa <- makeFun(osa.model)
estimated.osa( 1.2, interval="prediction")
estimated.osa( 0.8, interval="prediction")
@
Prediction intervals are typically much wider than confidence intervals.  We have to ``cast a wider net" 
to create an interval that contains a new observation (which might be quite a bit above 
or below the mean) than it is to contain the mean of the distribution.

The widths of both types of intervals depend on the value(s) of the explanatory
variable(s) from which we are making the estimate.  Estimates are more
precise near the mean of the predictor variable and become less precise as we move
away from there.  Extrapolation beyond the observed data range is both less precise, and risky,
because we don't have data to know whether the linear pattern seen in the data
extends into that region.

The plot below illustrates both confidence (dotted) and prediction (dashed) 
intervals.
Notice how most of the dots are within the prediction bands, but not within the 
confidence bands.
<<fig.width=6, out.width=".9\\textwidth", fig.height=3>>=
xyplot( OSA ~ palpebral, data=xmp12.01, panel=panel.lmbands )
@

\subsubsection{A Caution Regarding Prediction Intervals}
Prediction intervals are much more sensitive to the normality assumption
than confidence intervals are because the Central Limit Theorem does not 
help when we are thinking about individual observations (essentially samples of 
size 1).  So if the true distribution of errors is not really normal, then the prediction intervals we compute using the normality assumption will not be accurate.

\begin{problem}
	Use the output below to answer some questions about rainfall volume  and 
	runoff volume (both in $m^3$) for a particular stretch of a Texas highway.
<<echo=FALSE>>=
TexasHighway <- ex12.16
names(TexasHighway) <- c('rainfall','runoff')
rain.model <- lm(runoff ~ rainfall, data=TexasHighway)
summary(rain.model)
@
	\begin{enumerate}
		\item
			How many times were rainfall and runoff recorded?
		\item
			What is the equation for the least squares regression line?
		\item
			Report the slope together with its standard uncertainty.
		\item
			Give a 95\% confidence interval for the slope of this line.
		\item
			What does this slope tell you about runoff on this
			stretch of highway?
		\item
			What is $\hat\sigma$?
	\end{enumerate}
\end{problem}

\begin{solution}
	\begin{enumerate}
		\item
			The residual degrees of freedom is $13$, so there were $13 + 2 = 15$
			observations.
\item
	$\variable{runoff} = -1  + 0.83 \cdot \variable{rainfall}$
\item
	$0.83 \pm 0.04$
\item
<<>>=
confint( rain.model, "rainfall" )
@
	We can compute this from the information displayed:
<<tidy=FALSE>>=
t.star <- qt( .975, df=13 ) # 13 df listed for residual standard error
t.star
SE <- 0.0365
ME <- t.star * SE; SE
0.8270 + c(-1,1) * ME  # CI as an interval
@
We should round this using our rounding rules (treating the margin of error
like an uncertainty).
\item
	The slope tells us how much additional run-off there is per additional
	amount of rain that falls.  Since both are in the same units ($m^3$) and
	since the intercept is essentially 0, we can interpret this slope as a
	proportion.  Roughly 83\% of the rain water is being measured as runoff. 
\item
	$5.24$
	\end{enumerate}
\end{solution}

\begin{problem}
Data from a 1993 study to see how well lichens serve as an indicator for air pollution
are in the \dataframe{ex12.20} data set in the \pkg{Devore6} package.  In that paper,
a simple linear model was fit to see how the wet deposition of $\mathrm{NO}_{3}^{-}$
($g N/m^2$) related to the percentage dry weight of lichen.
\begin{enumerate}
	\item What are the least squares estimates for the intercept and slope of a line
		that can be used to estimate deposition
		from the amount of lichen?
	\item
		What is the estimated value of $\sigma$?
	\item
		Predict the amount of deposition if the dry weight of the 
		lichen is measured to be $0.7$.
	\item
		Give a 95\% confidence interval for the mean amount deposition among
		among samples with a dry weight of lichen measured to be $0.7\%$.
\end{enumerate}
\end{problem}

\begin{solution}
<<>>=
require(Devore6)
lichen.lm <- lm( deposition ~ LichenN, data=ex12.20)
# a)
lichen.lm       
coef(lichen.lm)      # this includes standard errors; could also use summary() 
# b)
sigmaHat(lichen.lm)  # can also be read off of the summary() output
# c)
estimated.dep <- makeFun(lichen.lm)
estimated.dep(0.7)
# d) 
estimated.dep(0.7, interval="confidence")
@
\end{solution}

\begin{problem}
	The \dataframe{KidsFeet} data set contains variables giving the widths and lengths
	of feet of some grade school kids.
	\begin{enumerate}
		\item
			Perform our usual diagnostics to see whether there are any reasons
			to be concerned about using a simple linear model in this situation.
		\item Based on this data, what estimate would you give for the width of a 
			Billy's foot if Billy's foot is 24 cm long?  
			(Use a 95\% confidence level.)
		\item Based on this data, what estimate would you give for the average width of a 
			kids' feet that 24 cm long?
			(Use a 95\% confidence level.)
	\end{enumerate}
\end{problem}

\begin{solution}
<<fig.height=3.5>>=
foot.model <- lm( width ~ length, data=KidsFeet)
plot(foot.model, w=1)
plot(foot.model, w=2)
@
Our diagnostics look pretty good.  The residuals look randomly distributed with
similar amounts of variability throughout the plot.  The normal-quantile plot
is nearly linear.

<<>>=
f <- makeFun(foot.model)
f(24, interval="prediction")
f(24, interval="confidence")
@
We can't estimate Billy's foot width very accurately (between 8.0 and 9.6 cm),
but we can estimate the average foot width for all kids with a foot length of
24 cm more accurately (between 8.67 and 8.96 cm).
\end{solution}

\begin{problem}
	Some traffic engineers were interested to study interactions between bicycle and 
	automobile traffic.  One part of the study involved comparing the amount of 
	``available space'' for a bicyclist 
	(distance in feet from bicycle to centerline of the roadway) and 
	``separation distance'' 
	(the average distance between cyclists and passing car, also measured in feet, 
	determined by averaging based on photography over an extend period of time).
	Data were collected at 10 different sites with bicycle lanes.
	The data are available in the \dataframe{ex12.21} data set in 
	the \pkg{Devore6} package.

	\begin{enumerate}
		\item Write out an equation for the least squares regression line for 
			predicting separation distance from available space.
		\item Given an estimate (with uncertainty) for the slope and interpret it.
		\item A new bicycle lane is planned for a street that has 15 feet of available
			space.  Give an interval estimate for the separation distance 
			on this new street.  Should you use a confidence interval or a prediction
			interval?  Why?
		\item
			Give a scenario in which you would use the other kind of interval.
	\end{enumerate}
\end{problem}

\begin{solution}
<<>>=
bike.model <- lm( distance ~ space, data=ex12.21 )
coef(summary(bike.model))
f <- makeFun(bike.model)
f( 15, interval="prediction" )
@
We would use a confidence interval to estimate the average separation distance 
for all streets with 15 feet of available space.
<<>>=
f( 15, interval="confidence" )
@
\end{solution}

\begin{problem}
	Select only the non-diabetic men from the \dataframe{pheno} data set using
<<>>=
men <- subset(pheno, sex=="M" & t2d=="control")  # note the double == and quotes here
head(men,3)
@
This data set contains some phenotype information for subjects in
a large genetics study.  You can find out more about the data set with
<<tidy=FALSE,eval=FALSE>>=
?pheno
@
\begin{enumerate}
	\item
		Using this data, fit a linear model that can be used 
		to predict weight from height.  What is the equation 
		of the least squares regression line?
	\item
		Give a 95\% confidence interval for the slope of this regression
		and interpret it in context.  (Hint: what are the units?)
	\item
		Give a 95\% confidence interval for the mean weight of all 
		non-diabetic men who are 6 feet tall.  
		
		Note the heights are in cm and the weights are in kg, so you will need to convert 
		units to use inches and pounds.  (2.54 cm per inch, 2.2 pounds per kg)
	\item
		Perform regression diagnostics.  Is there any reason to be concerned
		about this analysis?
\end{enumerate}

\end{problem}

\begin{solution}
	\begin{enumerate}
		\item
<<>>=
model <- lm( weight ~ height, data=men )
coef(summary(model))
@
			So the equation is 
			\[
			\variable{weight} = 
			\Sexpr{round(coef(model)[1])} + 
			\Sexpr{round(coef(model)[2],2)} \cdot \variable{height}
			\]
		\item
<<>>=
# we can ask for just the parameter we want, if we like
confint(model, parm="height")
@
The slope tells us how much the average weight (in kg) increases per 
cm of height.
		\item
<<>>=
f <- makeFun(model)
# in kg
f( 6 * 12 * 2.54, interval="confidence" ) 
# in pounds
f( 6 * 12 * 2.54, interval="confidence" ) * 2.2
@
		\item
<<>>=
xyplot( resid(model) ~ fitted(model) )
qqmath( ~ resid(model) )
@
We could also have used
<<fig.height=3.5>>=
plot(model, which=1)
plot(model, which=2)
@
The residual plot looks fine.  There is bit of a bend to the normal-quantile plot, indicating
that the distribution of residuals is a bit skewed (to the right -- the heaviest men are farther above
the mean weight for their height than the lightest men are below).

In this particular case, a log transformation of the weights improves the
residual distribution.  There is still one man whose weight is quite high for
his height, but otherwise things look quite good.
<<>>=
model2 <- lm( log(weight) ~ height, data=men )
coef(summary(model2))
xyplot( resid(model2) ~ fitted(model2) )
qqmath( ~resid(model2) )
@
This model says that
\[
\log( \variable{weight} ) 
	= \Sexpr{round(coef(model2)[1],2)} + \Sexpr{round(coef(model2)[2],4)} \cdot \variable{height}
\]
So
\[
\variable{weight}  
= \Sexpr{round(exp(coef(model2)[1]),1)} 
		\cdot (\Sexpr{round(exp(coef(model2)[2]),3)})^{\variable{height}}
\]
\end{enumerate}
\end{solution}

\begin{problem}
	The \dataframe{anscombe} data set contains four pairs of explanatory 
	(\variable{x1}, \variable{x2}, \variable{x3}, and \variable{x4})
	and response
	(\variable{y1}, \variable{y2}, \variable{y3}, and \variable{y4})
	variables.  These data were constructed by Anscombe 
	\cite{Anscombe:1973:Graphs}.
	\begin{enumerate}
		\item 
			For each of the four pairs, us \R\ to fit a linear model and 
			compare the results.  Use, for example,
<<eval=FALSE>>=
model1 <- lm( y1 ~ x1, data=anscombe ); summary(model1)
@
			Briefly describe what you notice looking at this output.  (You do not have
			to submit the output itself -- let's save some paper.)
		\item
			For each model, create a scatterplot that includes the regression line.
			(Make the plots fairly small and submit them.
			Use \texttt{fig.width} and \texttt{fig.height} to control the size of 
			the plots in RMarkdown.)
		\item
			Comment on the results.  Why do you think Anscombe invented these data?
	\end{enumerate}
\end{problem}

\begin{solution}
  Anscombe's data show that it is not sufficient to look only at the 
  numerical summaries produced by regression software.  His four data
  sets produce nearly identical output of \verb!lm()! and \verb!anova()!
  yet show very different fits.  An inspection of the residuals (or even
  simple scatterplots) quickly reveals the various difficulties.
\end{solution}

\begin{problem}
	Find an article from the engineering or science literature that uses 
	a simple linear model and report the following information:
	\begin{enumerate}
		\item
			Print the first page of the article (with title and abstract) and write
			a full citation for the article on it.  Staple this at the end of your
			assigment.
		\item
			If the article is available online, provide a URL where it can be found.
			(You can write that on the printout of the first page of the article, too.)
		\item
			How large was the data set used to fit the linear model?  How do you know?  (How 
			did the authors communicate this information?)
		\item
			What are the explanatory and response variables?
		\item
			Did the paper give an equation for the least squares regression line 
			(or the coefficients, from which you can determine the regression equation)?
			If so, report the equation
		\item
			Did the paper show a scatter plot of the data?  Was the regression line 
			shown on the plot?
		\item
			Did the paper provide confidence intervals or uncertainties for the 
			coefficients in the model?
		\item
			Did the paper show any diagnostic plots (normal-quantile, residuals plots, etc.)?
			If not, did the authors say anything in the text about checking that 
			a linear model is appropriate in their situation?
		\item
			What was the main conclusion of the analysis of the linear model?
		\item
				If there is an indication that the data are available online,
				let me know where in case we want to use these data for an example.
	\end{enumerate}
	Google scholar might be a useful tool for this.  JSTOR (available through Heckman
	Library) also has a large number of scientific articles.  Or you might ask an
	engineering or physics professor for an appropriate engineering journal to
	page through in the library.  Since the chances are small that two students
	will find the same article if working independently, I expect to see lots
	of different articles used for this problem.

	If your article looks particularly interesting or contains statistical 
	things that you don't understand but would like to understand, let me know,
	and perhaps we can do something later in the semester with your article.
	It's easiest to do this if you can give me a URL for locating the paper online.
\end{problem}


\newpage
\section*{Exercises}
\shipoutProblems


\ifsolutions
\ifsolutionslocal
\newpage
\section*{Solutions}
\shipoutSolutions
\fi
\fi

