\Sexpr{set_parent('Math241-S14.Rnw')}

<<include=FALSE>>=
require(gdata)
haveInternet <- tryCatch( {
	read.xls("http://www.calvin.edu/data/m241/MilledSteel.xls", 1); FALSE },
	error=function(e) TRUE)
@

\chapter{Beyond Linear Regression}
\section{How big is your $R^2$?}
One part of regression model diagnostics is to check the fitted model's $R^2$ value, which gives an indication of the proportion of the variance in the response that has been "explained" by the model.  A low value (closer to 0) means that data points are spread far around the best fit line; a high one (close to 1) means that data points are clustered very tightly around the line.  A model with a low $R^2$ value is not necessarily "bad" -- it may still provide helpful information about a real relationship between your response and predictor.  However, that relationship is very "noisy," which means that your model will have poor predictive power - it will be unable to make predictions with the accuracy and precision you might hope for.

Often, the predictive power of a model, and the $R^2$ value, can be improved by adding additional explanatory variables -- that is, fitting a model with more than one explanatory variable.  It could have two predictors, three, or as many as you can (sensibly) come up with. This kind of model is called multiple regression. Mathematically, it means fitting a model of the form:

$$ y ~ \beta_0 + \beta_1*x_1 + \beta_2*x_2 + \beta_3*x_3 ...$$

Multiple regression often makes sense when you are studying a complex process where there is most likely "more than one thing going on."  For example, you might consider modelling population growth rates worldwide using a data set including a set of social, economic, health, and political indicators compiled using data from the World Health Organization and partner organizations. The dataset description is available online: \url{http://www.exploredata.net/Downloads/WHO-Data-Set}.  One idea might be to look for a linear relationship between per-capita income and population growth rate:
<<who_slm, fig.show="hold", tidy=FALSE>>=
whodat <- read.csv("http://www.exploredata.net/ftp/WHO.csv", 
                   header=TRUE, strip.white=TRUE, sep=",")
#simplify some variable names
names(whodat)[10] <- "PopGrowthRate"
names(whodat)[6] <- "IncomePerCapita"
names(whodat)[7] <- "FemaleSchoolEnrollment"
xyplot(PopGrowthRate~IncomePerCapita, data=whodat)
who.m1 <- lm(PopGrowthRate~IncomePerCapita , data=whodat)
summary(who.m1)
@ 

The $R^2$ value of this model is very low. But that could be because, unsurprisingly, there are \emph{many} factors contributing to population growth rate, not \emph{just} income.  For example, what about education? Perhaps more-educated women have fewer children, lowering the population growth rate.  So we might want to model population growth rate as a function of \emph{both} income and education.

In R, a multiple regression model can be fitted with a call to \texttt{lm()}.  We just add additional predictors to the right hand side of the model formula, separated by $+$ signs. For the WHO example discussed above, for example, we could try:
<<who_slm2, fig.show="hold">>=
xyplot(PopGrowthRate~FemaleSchoolEnrollment, data=whodat)
who.m2 <- lm(PopGrowthRate~IncomePerCapita + FemaleSchoolEnrollment, data=whodat)
summary(who.m2)
@ 

We would need to follow up with our diagnostics to fully assess these two models, but comparison of the $R^2$ values immediately shows that $R^2$ is much higher for the second model. In other words, the multiple regression has succeeded in explaining more of the variance in population growth rates than the simple linear regression with only one predictor.  


\section{Violations of Linear Regression Assumptions}
In the previous chapter, we learned how to carry out regression diagnostics -- to check whether or not the assumptions of linear regression analysis were valid for a particular analysis.  If the assumptions are violated, then the conclusions (parameter estimates, but especially standard errors) will be incorrect, and the model results can not be trusted.  

For each type of violation, there are some fixes or modifications we can try in order to fit a valid, trustworthy model to our data and still draw reliable conclusions.  In this course, we will focus mainly on type of "fix":  applying transformations to linearize non-linear relationships, and allow us to apply linear regression to the transformed data.  This approach is covered in detail in the rest of this chapter.

Before beginning our detailed discussion of transformation, we will briefly discuss several other types of "fixes".  The mathematical foundations of these more complex models are essentially beyond the scope of this class, but you should understand when they might be useful (for example, if you see a certain type of pattern in residual diagnostic plots, which technique might help solve the problem?) and be able to implement them in R.

The table below provides an overview of various problems you might uncover as you do regression diagnostics, along with possible solutions.  Each entry in the table is covered in a bit more detail in the subsequent sections of this chapter.

\begin{center}
  	\begin{tabular}{p{1.5in}|p{2in}|p{2.5in}}
			\hline
			Assumption & Description of Problem & Options \\
			 \hline
			Linearity & Scatterplot (or residual plots) indicate nonlinear relationship & Transform explanatory and or response variables. Alternative: fit a non-linear model using the R function \texttt{nls()} \\
			Normality of errors & Residual QQ plots indicates departure from normality & First check if other assumptions may also be violated, and try options listed there. If that fails, you may need to add additional predictor variables to your model; or to fit a generalized linear model, a more sophisticated type of regression that we will not cover in this course. \\
			Independence of errors & ACF plot indicates strong dependence of errors over time (or space) & Fit a "autoregressive" model, where this relationship between subsequent or nearby measurements is expected and accounted for.  To do this in R, replace \texttt{lm(y$\sim$x)} with something like \texttt{gls(y$\sim$x, correlation = corAR1(form=$\sim$1))}.  \\
      Homoscedasticity & Variance of errors is not constant over the full range of response values, or over time & First, make sure that the linearity assumption is not violated. Next, if you have the option of including additional predictors in your model, it may be helpful.  Next, transforming the response variable may help. Finally, if none of those options provide a solution, you can fit a model with non-constant error variance. For example, if variance increases with fitted response values, you can replace \texttt{lm(y$\sim$x)} with something like \texttt{nls(y$\sim$x, weights=varPower())}\\ 
			\hline
	\end{tabular}
	\end{center}
\section{Non-Normal Errors}
Sometimes, during diagnostics for a linear regression model, you will find that residual quantile-quantile plots indicate that linear regression residuals are far from normally distributed.  In this case, before trying to modify your model in any way, it is useful to check whether any \emph{other} assumptions of the linear regression have \emph{also} been violated.  If they have, it is worthwhile to try to deal with those problems first, and see if solving them makes the residuals more normal.

If non-normal residuals are the only apparent problem with a linear regression model, adding additional explanatory variables \emph{might} help in some cases.  Most of the time, you would have to turn to a more sophisticated regression model called a generalized linear model (GLM).  Fitting GLMs is beyond the scope of this class, and you will not be asked to do it.

\section{Non-Independence of Errors}
Sometimes, regression diagnostics (particularly a plot of residuals as a function of time, or an ACF plot) will show that the residuals are not independent.  This happens most often when the predictor variable is a temporal or spatial one; data points collected at similar times, or similar locations, are often similar to each other rather than independent.

We will consider a simple example using the price of chicken over time (in constant dollars, adjusted for inflation over time).  It seems to make sense to try to predict the price of chicken as a function of time (it's been getting progressively cheaper for the last century or so):
<<chickn, fig.show='hold'>>=
chickn <- read.csv("http://www.calvin.edu/~sld33/data/chickn.csv", header=TRUE)
xyplot(Price~Year, data=chickn, type=c("p", "r"))
chick.mod <- lm(Price~Year, data=chickn)
summary(chick.mod)
@ 

However, there seems to be a problem with non-independence of the residuals.  Price is not independent from year to year; if you know the price was a bit high one year, it's likely to remain so for the next several years:
<<chick_acf, fig.show='hold', fig.height=5>>=
chick.resid <- resid(chick.mod)
acf(chick.resid)
@

This is a big problem, because it tends to result in standard error estimates that are artificially small. In other words: we think we have estimated our slope and intercept parameters \emph{much} more precisely than we really have, and would report falsely narrow confidence intervals.
To fix the problem, we can consider replacing our simple linear regression:
$$ y ~ \beta_0 + \beta_1x + \epsilon$$
(where $\epsilon \sim N(0, \sigma)$) with a model that expects that subsequent residuals to depend on previous ones, so that the residual for the data point collected at time $t$ is:
$$ e_t = \rho e_{t-1} + \epsilon$$
(where, still, $\epsilon \sim N(0, \sigma)$; and $\rho$ is a new parameter indicating how strong the dependence over time is.)  This is called an AR(1) process, or an auto-regressive process of order 1.  It can be fit easily in R using the function \texttt{gls()} instead of \text{lm()}. \texttt{gls()} does "generalized least-squares" fitting, and is found in the package \texttt{nlme}.  The function call syntax illustrated in this example will work any time the explanatory variable is the time (or space) one that is causing the non-independence.

<<ar1_fit, tidy=FALSE>>=
require(nlme)
chick.mod2 <- gls(Price~Year, data=chickn, 
                  correlation=corAR1(form=~1))
summary(chick.mod2)
@

If you plot the residuals of this new model, and plot the ACF, you will see that the correlation coefficients \emph{still have high vaules}.  However, in the new \texttt{gls()} fit, this correlation has now been taken into account in the standard errors (which are larger -- compare the coefficient tables to verify it), so it is OK now to trust the model parameter estimates and predictions.

\section{Heteroscedasticity (Non-constant Error Variance)}
Sometimes, model diagnostics for a linear regression indicate that variance of errors is not constant over the full range of response values.  Often, it is the case that the error variance grows larger as the predicted response value grows larger, resulting in a "trumpet-like" shape in the plot of residuals versus fitted values.

If you spot this problem, first, make sure that the linearity assumption is not violated. Next, if you have the option of including additional predictors in your model, it may be helpful.  Next, transforming the response variable may help.  Specifically, a log or square-root transformation of the response variable may be useful.  (See more details and examples later in this chapter, when transformations are discussed in detail.) 

Finally, if none of the previous options provide a solution, you can fit a model that actually \emph{expects} and accounts for non-constant error variance. We will not cover this topic in any detail, but this brief example is included for your future reference (outside this class).  Example: if variance increases with fitted response values, you can fit an appropriate model with the \texttt{gls} function from the \texttt{nlme} package.  To do so, replace \texttt{lm(y~x)} with something like \texttt{nls(y~x, weights=varPower())}.

Here is a brief example, using the \texttt{Ornstein} dataset from the \texttt{car} package.  It gives data on 248 Canadian companies, collected in the mid-1970s.  The variable \texttt{assets} gives each company's assets in millions of dollars, and \texttt{interconnects} gives the number of director and executive positions that are shared with other firms.  A scatter plot shows that the richest companies have many of these "interlocks", so we might model assets as a function of interlocks...however, the residuals have non-constant variance:
<<Ornstein>>=
require(car)
xyplot(assets ~interlocks, data=Ornstein)
om <- lm(assets ~interlocks, data=Ornstein)
om.resid <- resid(om)
f <- makeFun(om)
fitted <- f(Ornstein$interlocks)
xyplot(om.resid ~ fitted)
@

We can try to correct for this problem by fitting a model that "expects" this non-constant variance, by using the function \texttt{gls} with the input \texttt{weights=varPower()}.  (There are many other ways to model non-constant error variance; this small example gives you just a taste, and for this course, you would not be expected to deal with any cases other than ones like this, where error variance increases with fitted values.)
<<Ornstein_gls>>=
om2 <- gls(assets ~interlocks, data=Ornstein, weights=varPower())
@ 

As with the non-independence case, if you plot the residuals for this model, you will see that they DO still have non-constant variance...but again, now it is OK because our model has taken it into account, and computed parameter estimates and standard errors appropriately.

\section{Non-linear Relationships}
The rest of this chapter will provide detailed information on how to deal with some non-linear relationships in regression.

Linear regression assumes a linear relationship between predictor and response variables, but not all relationships between pairs of quantitative variables are linear.  There are two common ways to deal with
nonlinear relationships:
\begin{enumerate}
	\item
		Transform the data before beginning linear regression analysis, so that there \emph{is} a linear relationship between the (transformed) variables.
	\item
		Fit a model that explicitly expects, and accounts for, the nonlinear relationship between the two variables. 
\end{enumerate}


\section{Transformations in Linear Regression}
\myindex{transformation!of data}%

The applicability of linear models can be extended through the use of 
various transformations of the data.  
There are several reasons why one might consider a transformation of
the predictor or response (or both).

\begin{itemize}
  \item To correspond to a theoretical model.

	Sometimes we have \emph{a priori} information that tells us what 
	kind of non-linear relationship we should anticipate.  As a simple example,
	if we have an apparatus that can accelerate objects by applying a constant
	(but unknown) force $F$, 
	then since $F = ma$, we would expect the relationship between 
	the mass of the objects tested and the acceleration measured to satisfy
	\[
	a = \frac{F}{m}
	\;.
	\]
	This might lead us to fit a model with no intercept 
	(e.g., in R, lm(y ~ 0 + x)) after applying an inverse transformation
	to the predictor $m$:
	\[
	a = \beta_1 \cdot \frac{1}{m}
	\;.
	\]
	The parameter $\beta_1$ corresponds to the (unknown) force being applied
	and could be estimated by fitting this model.

	Alternatively, we could use a logarithmic transformation
	\begin{align*}
	\log(a) &= \beta_0 + \beta_1 \log(m) \,,
	\\
	a &= e^{\beta_0}  m^{\beta_1}  \;.
	\end{align*}
	In this model, $e^\beta_0$ corresponds to the unknown force and 
	we can test whether $\beta_1 = -1$ is consistent with our data.

	Many non-linear relationships can be transformed to linearity.  
	Exercise~\ref{prob:transformations} presents several examples -- similar to the force example above --
	and asks you to determine a suitable transformation.

  \item To obtain a better fit.

	If a scatterplot or residual plot shows a clearly non-linear pattern 
	to the data, then it would be inappropriate to fit a linear regression (and conclusions drawn from that model would be incorrect and misleading).  
	In the absence of theoretical reasons to expect a particular mathematical relationship between the variables being studied, we may select 
	transformations based on the shape of the relationship as 
	revealed in a scatterplot.  Section~\ref{sec:ladderOfReexpression}
	provides some guidance for selecting transformations in
	this situation.

  \item To obtain better residual behavior.

	Some transformations are used to improve the agreement between the
	data and the assumptions about the error terms in the model.  For example, if data are heteroscedastic -- for example, if the variance in the response appears to increase as the predictor 
	increases -- a logarithmic or square root transformation of the response may help.  
\end{itemize}

In practice, all three of these issues are intertwined.  A transformation that
improves the fit, for example, may or may not have a good theoretical 
interpretation.  
Similarly, a transformation performed to achieve \term{homoskedasticity} 
\myindex{heteroskedasticity}%
\myindex{homoskedasticity}%
(equal variance; the opposite is called \term{heteroskedasticity}) 
may result in a fit that does not match the overall shape of the data very well.
Despite these potential problems, there are many situations where a relatively
simple transformation is all that is needed to greatly improve the model.  Here, when we say "improve" the model, we mean that the assumptions of the model are satisfied, and the model fits the data acceptably well.

\subsection{Three Important ``Laws"}
In the sciences, relationships between variables based on some scientific theory
are often referred to as laws.  Many of these fall into one of three categories that
are easily handled by transforming the data and fitting a linear regression model.

\subsubsection{Linear Laws}
We've already talked about linear relationships, but it is worth mentioning them again because there
are so many situations in which a linear relationship arises.

\subsubsection{Power Laws}

Relationships of the form 
\[ y = A x^p \]
are often called power laws.  The two parameters are the exponent $p$ and a constant of proportionality $A$.
Power laws can be linearized by taking logarithms:
\[ \log(y) = \log(A x^p) = \log(A) + p \log(x) \]
So if we fit a model of the form
<<eval=FALSE>>=
lm( log(y) ~ x )
@
Then $\beta_0 = \log(A)$ and $\beta_1 = p$.  
If a power law is a good fit for the data then
<<eval=FALSE>>=
xyplot( log(y) ~ log(x) )
@
will produce a roughly linear plot.

Fitting a power law results in estimates for the parameters $\beta_0 = \log(A)$ and $\beta_1 = p$.
Note that we can use logarithms with any base for this transformation.  Typically natural logarithms are used
(that's what \function{log()} does in \R).  
In some specific applications we might use base 10 logarithms (\function{log10()} in \R) 
or base 2 logarithms (\function{log2()} in \R); 
this yields the commonly used scale for 
$\beta_0 = \log(A)$, the constant of proportionality.

Some common situations that are modeled with power laws include drag force vs speed, velocity vs. force, and
frequency vs. force.

\subsubsection{Exponential Laws}

Relationships of the form 
\[ y = A B^x = A e^{Cx} \]
are often called exponential laws.  
The two parameters are the base $B = e^C$ and a constant of proportionality $A$.
Exponential laws can also be linearized by taking logarithms:
\[ \log(y) = \log(A B^x) = \log(A) + x \log(B) \]
So if we fit a model of the form
<<eval=FALSE>>=
lm( log(y) ~ x )
@
Then $\beta_0 = \log(A)$ and $\beta_1 = \log(B) = C$.  
If an exponential law is a good fit for the data then
<<eval=FALSE>>=
xyplot( log(y) ~ x )
@
will produce a roughly linear plot.

Fitting an exponential law results in estimates for the parameters $\beta_0 = \log(A)$ and $\beta_1 = \log(B) = C$.
Again, we will generally use natural logarithms. In this course, if you see a \texttt{log()} without an indication of the base of the logartihm, you can assume it is base "e", a natural logarithm.  Similarly, remember that for R, the function \texttt{log()} takes the natural logarithm.

Some common situations that are modeled with exponential laws include
population growth and radioactive decay.  Note that exponential growth models
are typically only good approximations over a limited range since exponential
functions eventually grow quickly, and often some external constraints will
limit this growth.  For example, a culture of bacteria may grow roughly
exponentially for a while, but eventually, limits on space and nourishment will
make it impossible for exponential growth to continue.

\subsubsection{Log-log and semi-log plots}

Graphs of $\log(y)$ vs. $\log(x)$ (log-log) or $\log(y)$ vs $x$ (semi-log)
can be used to assess whether the power law or exponential law appears to apply
in a given situation.  If the law were a perfect description of the situation,
all the points on the log-log or semi-log plot would fall along a straight line.
In practice, the fit won't be perfect, but the plot is a useful diagnostic.  For example, you can compare a plot of $y$ as a function of $x$ with a log-log or semi-log plot, and see which one shows the most linear relationship between the two variables.

\R\ can easily create plots with transformed scales.  Simply add the input \texttt{scales} to your call to \texttt{xyplot()}, as detailed in the example below:
<<>>=
x <- 1:10
y <- 3 * x^1.5
xyplot(log(y) ~ log(x))
xyplot(log(y) ~ x)
xyplot(y ~ x, scales=list(x=list(log=TRUE), y=list(log=TRUE)))
xyplot(y ~ x, scales=list(y=list(log=TRUE)))
@

\subsection{Other Models That Can Be Transformed to Linear}

The three laws above are not the only kinds of relationships that
can be transformed to linear.

\begin{example}
A chemical engineering text book suggest a law of the form
\[
\log( - \frac{dC}{dt} ) =  \log(k) + \alpha \log(C)
\]
where $C$ is concentration and $t$ is time.

This is equivalent to 
\begin{align*}
 - \frac{dC}{dt}  &=  k \cdot C^\alpha
\\
 - \int C^{-\alpha} \; dC  &=  \int k \;dt 
 \\
 - \frac{1}{1-\alpha} C^{1-\alpha} &=  k t + d 
 \\
 \frac{1}{\beta} C^{-\beta} & = k t + d
 \\
C^{-\beta} & = \beta k t + \beta d
\end{align*}

If we know $\beta = \alpha - 1$ (i.e., if we know $\alpha$), 
then we can fit a linear model using
<<eval=FALSE>>=
lm( C^(-1/beta) ~ t )
@
The intercept of such a model will be $\beta d$ and the slope will be $\beta k$,
from which we can easily recover $d$ and $k$.

Alternatively, if we know $d=0$ (i.e., if we know that $C = 0$ when $t=0$), 
then we can use
\begin{align*}
	\log( C^{-\beta} )  = -\beta \log(C) &= \log(\beta k t ) = \log(\beta k) + \log t
 \\
 \log(C) &= - \frac{\log(\beta k)}{\beta} - \frac{1}{\beta} \log t
\end{align*}
Now if we fit a model of the form 
<<eval=FALSE>>=
lm(  C ~ log(t) )
@
the intercept will be $\frac{-\log(\beta k)}{\beta}$ and the slope will be 
$\frac{-1}{\beta}$.  From this we can solve for $k$ and $\beta$.
\end{example}

\begin{example}
Continuing the previous example, we will fit the following data 
<<tidy=FALSE>>=
Concentration <- data.frame(
  time=c(0, 50, 100, 150, 200, 250, 300),               # minutes
  concentration=c(50, 38, 30.6, 25.6, 22.2, 19.5, 17.4) # mol/dm^3 * 10^3
)
xyplot(concentration ~ time, data=Concentration)
@
under the assumption that 
$\alpha=2$, so $\beta = 1$.  In this case, our relationship becomes
\[
\frac{1}{C}  = - k t - d \;.
 \]
We can now fit a model and see how well it does.
<<>>=
conc.model <- lm( 1/concentration ~ time, data=Concentration)
summary(conc.model)
confint(conc.model)
@
This provides estimates for 
the intercept $- \beta d$ 
and the slope $- \beta k$ 
of our model.
We can divide by $-\beta$ to obtain estimates for $d$ and $k$.

Of course, we should always look to see whether the fit is a good fit.
<<>>=
xplot(conc.model, w=1:2)  # residual plot and qq-plot of residuals
@
Notice that these residuals are very small relative to the values for
concentration.  (We can see this from the vertical scale of the plot and 
also from the small value for residual standard error in the summary output.)
The shape of the residual plot would be more disturbing if the
magnitudes were larger and if there were more data.  
As is, even if there is some systematic problem, it is roughly five orders of 
magnitude smaller than our concentration measurements, which likely can't be 
measured to that degree of accuracy.

If we want to show the fit on top of the original data, we must remember to
untransform the response, since the model we fitted is a model for $1/C$, but we want to show a models for $C$:
<<fig.keep='last'>>=
C <- makeFun(conc.model)
xyplot( concentration ~ time, data=Concentration )
plotFun( 1/C(t) ~ t, add=TRUE)
@
\end{example}



\subsection{The Ladder of Re-expression}
\label{sec:ladderOfReexpression}%
\label{sec:tukey-bulge}%
\myindex{bulge rule}%
\myindex{ladder of re-expression}%
\myindex{Tukey, J.}%
\myindex{Tukey, J.}%
Sometimes we have data for which there is no theory (yet) to suggest the form
of a model.  In such a case, we may let the data help suggest a model.
If we find a model that fits well, we can return to the question of whether 
there is an explanation for that type of model.

In the 1970s, Mosteller and Tukey introduced what they called
the \term{ladder of re-expression} and \term{bulge rules} 
\cite{Tukey:1977:EDA,Mosteller:1977:DataAnalysis} that can be used to 
suggest an appropriate transformation to improve the fit when the 
relationship between two variables ($x$ and $y$ in our examples) 
is monotonic and has a single bend.  
Their idea was to apply a power transformation to 
$x$ or $y$ or both -- that is, to work with $x^a$ and $y^b$ for
an appropriate choice of $a$ and $b$.  Tukey called 
this ordered list of transformations the \emph{ladder of re-expression}.
The identity transformation has power~$1$.
The logarithmic transformation is a special case and is included in the 
list associated with a power of $0$.  
The direction of the required transformation can be obtained
from Figure~\ref{fig:TukeyBulge}, which shows four bulge types,
represented by the curves in each of the four quadrants.  
A bulge can potentially be straightened by 
applying a transformation to one or both variables, moving up 
or down the ladder as indicated by the arrows.  More severe bulges
require moving farther up or down the ladder.  
\authNote{ed: There is a standard ordering of quadrants in mathematics, so labeling
isn't needed.  We could use directions (upper right), but I don't want to add 
quadrant numbers to the figure.  Also in next example.  --- 2010-11-8}%
A curve bulging in the 
same direction as the one in the first quadrant of Figure~\ref{fig:TukeyBulge},
for example, might be straightened by moving up the ladder of transformations
for $x$ or $y$ (or both), while a curve like the one in the second quadrant, 
might be straightened by moving up the ladder for $y$ or down the ladder for $x$.
\begin{figure}
\begin{center}
\begin{tikzpicture}[scale=1.6]
\draw(175:1cm) arc (175:95:1cm);
\draw(85:1cm) arc (85:5:1cm);
\draw(-5:1cm) arc (-5:-85:1cm);
\draw(265:1cm) arc (265:185:1cm);

\draw[->](175:1cm) -- ++(180:0.3cm);
\draw[->](185:1cm) -- ++(180:0.3cm);

\draw[->](95:1cm) -- ++(90:0.3cm);
\draw[->](85:1cm) -- ++(90:0.3cm);

\draw[->](5:1cm) --  ++ (0:0.3cm); 
\draw[->](-5:1cm) -- ++ (0:0.3cm);

\draw[->](265:1cm) -- ++ (270:0.3cm);
\draw[->](275:1cm) -- ++ (270:0.3cm);

\draw(0,2.2) node {\ };
\draw(0,0) + (0:1.3) node[anchor=west]   { move $x$ up };
\draw(0,0) + (270:1.3) node[anchor=north]{ move $y$ down };
\draw(0,0) + (180:1.3) node[anchor=east] { move $x$ down };
\draw(0,0) + (90:1.3) node[anchor=south] { move $y$ up };
\end{tikzpicture}
\hfill
\begin{minipage}{0.35\textwidth}
\vspace*{-1.5in}
\begin{tabular}{rl}
\multicolumn{2}{c}{\textbf{ladder of re-expression}}
\\
\hline
\\[-1mm]
{power} & {transformation}\\
$\vdots$ & $\vdots$ \\
$3$ & $x \mapsto x^3$ \\[1mm]
$2$ & $x \mapsto x^2$ \\[1mm]
%\hline
$1$ & $x \mapsto x$ \\[1mm]
%\hline
$\frac12$ & $x \mapsto \sqrt{x}$ \\[1mm]
$0$ & $x \mapsto \log(x)$ \\[1mm]
$-1$ & $x \mapsto \frac1x$ \\[1mm]
$-2$ & $x \mapsto \frac{1}{x^2}$ \\[1mm]
$\vdots$ & $\vdots$ \\
\end{tabular}
\end{minipage}
\end{center}
%\figureskip
\caption{Bulge rules and ladder of re-expression.}
\label{fig:TukeyBulge}%
\end{figure}
%

This method focuses primarily on transformations designed to improve
the overall fit.  The resulting models may or may not have 
a natural, or obvious, interpretation.  These transformations also affect the 
shape of the distributions of the explanatory and response variables
and, more importantly, of the residuals from the linear model  
(see Exercise~\ref{prob:TukeyBulgeSkew}).
When several different transformations lead to reasonable linear fits, these other
factors may lead us to prefer one over another.


\begin{example}
\myindex{Tukey bulge|exampleidx}%
\question
The scatterplot in Figure~\ref{fig:tukey-bulge-example}
shows a curved relationship between
$x$ and $y$.
What transformations of $x$ and $y$ improve the linear fit?
\begin{figure}
<<tukey-bulge1,echo=FALSE>>=
n <- 20  
x <- runif(n,2,10)
y <- exp(0.3*x)
e <- exp(rnorm(n,0,0.1))
y <- y * e
foo <- function(x) {
    a <- x[2]
    x <- x[1]
    if (a == 0) { return (log(x)) }
    return (x^a)
}

power <- function(x,a) {
    M <- cbind(x,a)
    return  (apply(M, 1, foo))
}
powers <- c(0,0.5,1,2,3); np <- length(powers)
a <- rep(rep(powers, each=n),each=np)
b <- rep(rep(powers, each=n), times=np)
x <- rep(x,times=n *np)
y <- rep(y,times=n *np)
X <- power(x,a)
Y <- power(y,b)
original <- (a==1 & b==1)
ddd <- data.frame(X=X,Y=Y,a=a,b=b,original=original)
xyplot(y~x)
@
\caption{A scatterplot illustrating a non-linear relationship between $x$ and $y$.}
\label{fig:tukey-bulge-example}%
\end{figure}

\answer
This type of bulge appears in quadrant IV of Figure~\ref{fig:TukeyBulge},
so we can hope to improve the fit by moving 
up the ladder for $x$ or down the ladder for $y$.
As we see in Figure~\ref{fig:TukeyBulgeMany}, 
the fit generally improves as we move down and to the right -- but not too
far, lest we over-correct.
A $\log$-transformation of the response ($a=1$, $b=0$) seems 
to be especially good in this case. Not only is the resulting relationship
quite linear, but the residuals appear to have a better distribution as well.
\end{example}

\begin{figure}
<<tuky-bulge-many,echo=FALSE,fig.width=6,fig.height=7>>=
xyplot(Y~X|paste('a=',a,sep="") + paste("b=",b,sep=""),
            ddd, groups=original, cex=.7,
			xlab='x', ylab='y',
            scales=list(relation='free', draw=FALSE))
@
\caption{Using the ladder of re-expression to find a better fit.}
\label{fig:TukeyBulgeMany}%
\end{figure}

\begin{example}
\myindex{physics|exampleidx}%
\Rindex{balldrop}%
\label{example:balldrop}%
Some physics students conducted an experiment in which they dropped steel 
balls from various heights and recorded the time until the ball hit the 
floor.   We begin by fitting a linear model to this data.
<<balldrop>>=
ball.model <- lm(time~height,balldrop)
summary(ball.model)
xyplot(time~height,balldrop,type=c('p','r'))
xplot(ball.model,w=1)
@


\myindex{coefficient of determinism|exampleidx}%
At first glance, the large value of $r^2$ and the reasonably good fit 
in the scatterplot might leave us satisfied that we have found a good model.
But a look at the residual plot 
reveals a clear curvilinear pattern in this data.  
A knowledgeable physics student knows that
(ignoring air resistance) the time should be proportional to the 
\emph{square root} of the height.  
This transformation agrees with Tukey's ladder of re-expression, which suggests
moving down the ladder for \variable{height} or up the ladder for \variable{time}.

<<balldrop-transT>>=
ball.modelT <- lm(time ~ sqrt(height),balldrop)
summary(ball.modelT)
xyplot(time~height,balldrop, panel=panel.lm, model=ball.modelT)
xplot(ball.modelT, w=1)
@
This model does indeed fit better, but the residual plot indicates that
there may be some inaccuracy in the measurement of the height.  
In this experiment, the apparatus was set up once for each height and then several observations were made.  So any error in this set-up affected all time measurements for
that height in the same way.  This could explain why the residuals for 
each height are clustered the way they are since it violates the assumption
that the errors are \emph{independent}.  (See Example~\ref{example:balldropRM}
for a simple attempt to deal with this problem.)
\end{example}

%It is also important to note that although $r^2$ is very large in this model,
%it no longer has the usual interpretation because the model does not
%have an intercept term.  Exercise~\ref{prob:r2NoIntercept} explores 
%this further.

\begin{problem}
\Rindex{balldrop}%
In Example~\ref{example:balldrop}, we applied a square root transformation
to the height.  Is there another transformation that yields an even 
better fit?
\end{problem}

%\medskip   % to move one line of R output onto next page
\begin{example}
\myindex{physics|exampleidx}%
\label{example:balldropRM}%
One simple way to deal with the lack of independence in the previous example
is to average all the readings made at each height.  
(This works reasonably well in our example because we have nearly
equal numbers of observations at each height.)
We pay for this 
data reduction in a loss of degrees of freedom, but it may be easier to 
justify that the errors in average times at each height are independent 
(if we believe that the errors in the height set-up are independent and 
not systematic).

%\pagebreak
% # balldropavg <- aggregate(balldrop$time, by=list(balldrop$height), mean)
% # names(balldropavg) <- c('height','time')
<<balldrop-avg, message=FALSE>>=
require(plyr)  # to access ddply
balldropavg <- ddply( balldrop, .(height), summarise, time=mean(time) )
balldropavg
ball.modelA <- lm(time ~ sqrt(height),balldropavg)
summary(ball.modelA)
xyplot(time~height,balldropavg, panel=panel.lm,model=ball.modelA)
xyplot(resid(ball.modelA) ~ fitted(ball.modelA))
@
Using a square root transformation on averaged \texttt{height}
measurements in the \texttt{balldrop} data gives a similar fit but
a very different residual plot.  The interpretation of this model
is also different.

Notice that the parameter estimates are essentially the same as in 
the preceding example.  The estimate for $\sigma$ has decreased some.
This makes sense since we are now estimating the variability in 
\emph{averaged} measurements rather than in individual measurements.

Of course, we've lost a lot of degrees of freedom, and as a result,
the standard error for our parameter estimate is about twice as 
large as before.  This might have been different; had the mean values
fit especially well, our standard error might have been smaller despite
the reduced degrees of freedom.

One disadvantage of the data reduction is that it is hard to interpret
the residuals (because there are fewer of them).  
At first glance there appears to be a downward trend
in the residuals, but this is largely driven by the fact that the largest
residual happened to be for the smallest fit.  
\end{example}


\begin{example}
\label{example:soap}%
\myindex{soap|exampleidx}%
\Rindex{soap}%
\question
Rex Boggs of Glenmore State High School in Rockhampton, Queensland, had an
interesting hypothesis about the rate at which bar soap is used in the shower.
He writes:
\begin{quote}
    I had a hypothesis that the daily weight of my bar of soap [in grams] 
	in my shower wasn't a linear function, the reason being that the tiny 
	little bar of soap at the end of its life seemed to hang around for just 
	about ever. I wanted to throw it out, but I felt I shouldn't do so until 
	it became unusable. And that seemed to take weeks.

    Also I had recently bought some digital kitchen scales and felt I needed
    to use them to justify the cost. I hypothesized that the daily weight of
    a bar of soap might be dependent upon surface area, and hence would be a
    quadratic function \dots .

The data ends at day 22. On day 23 the soap broke into two pieces and one
piece went down the plughole. 
\end{quote}
The data indicate that although Rex showered daily, he failed to record the 
weight for some of the days.

What do the data say in regard to Rex's hypothesis?

\answer
Rex's assumption that weight should be a (quadratic) function of time 
does not actually fit his intuition.  His intuition corresponds roughly to the 
differential equation
\[
\Partial{t}{W} = k W^{2/3}\,,
\]
for some negative constant $k$ since the rate of change should be 
proportional to the surface area remaining.  
(We are assuming that the bar shrinks in such a way 
that its shape remains proportionally unaltered.)
Solving this equation (by separation of variables) gives
%\[
%\log(W) = k t + C
%\;.
%\]
\[
W^{1/3} = k t + C
\;.
\]
We can fit untransformed and transformed models 
(\verb!Weight^(1/3) ~ Day!) to this data and compare.
<<lm-soap>>=
soap.model1 <- lm(Weight~Day,soap)
summary(soap.model1)
@
The scatterplot in Figure~\ref{fig:soap} 
(darker line) indicate that the untransformed model is already a good fit.
%
% \footnote{
% For now, it suffices
% to know that larger values of $r^2$ generally indicate a better fit.
% We will discuss $r^2$ and what it measures in Section~\ref{sec:Rsquared}.
%}
<<lm-soap-trans>>=
soap.model2 <- lm(I(Weight^(1/3))~Day,soap)
summary(soap.model2)
@
\begin{figure}
<<echo=FALSE>>=
daysToFit <- seq(1,22,by=0.5)
linfits <- predict(soap.model1,newdata=data.frame(Day=daysToFit))
transfits <- predict(soap.model2,newdata=data.frame(Day=daysToFit))^3
soap.plot <- xyplot(Weight~Day,data=soap, 
    panel = function(x,y,...) {
        panel.xyplot(daysToFit,linfits, lwd=2, type="l", 
            col=trellis.par.get('superpose.line')$col[1])
        panel.xyplot(daysToFit,transfits, lwd=2, type="l",
            col=trellis.par.get('superpose.line')$col[2])
        panel.xyplot(x,y,cex=1.0,...)
    }
    )
soap.plot
@

\caption{Comparing untransformed (darker) and transformed 
(lighter) fits to soap use data.}
\label{fig:soap}%
\end{figure}
%
The transformed model in this case actually fits worse.
The higher value of $r^2$ for the untransformed model is an indication 
that the untransformed model explains a larger proportion of the variance in soap weights.  It is left as an exercise for you to examine diagnostic plots of the model residuals in both cases; you should see that neither one looks markedly better than the other. (There is perhaps an issue with a small amount of non-independence, or correlation over time, of the residuals; we might expect that with data collected over time.  However, the dataset is so small that it is hard to tell for sure if the problem is real and worth worrying about.)  
Figure~\ref{fig:soap} shows a scatterplot
with both fits.  
The data do not support Rex's assumption that a transformation
is necessary.  
% We can also fit a quadratic model of the form \verb!Weight~I(Day^2)!,
% but this model is worse still.  Fitting a full quadratic model requires 
% two predictors (\verb!Day! and \verb!Day^2!) and so will have to wait
% until our discussion of multiple linear regression.  
The scatterplot and especially the residual plots both show that the 
residuals are mostly positive near the ends of the data and negative
near the center.  Part of this is driven by a flattening of the pattern
of data-points near the end of the measurement period.  Perhaps as the soap
became very small, Rex used slightly less soap than when the soap was
larger.
Exercise~\ref{prob:soap} asks you to remove the last few observations
and see how that affects the models.

Finally, since a linear model appears to fit at least reasonably well
(but see Exercise~\ref{prob:soap}), 
we can give a confidence interval for $\beta_1$, 
the mean amount of soap Rex uses each shower.
\authNoted{check regression diagnostics for soap models -- some cause for worry}%
\authNote{in text reference for quote from Rex? --2010-11-27}%
\Rindex{confint()}%
<<lm-soap-ci>>=
confint(soap.model1)
@
\end{example}


\begin{problem}
\label{prob:soap}%
Remove the last few days from the \dataframe{soap} data set and 
refit the models in Example~\ref{example:soap}.
How much do things change?  Do the residuals look better, or 
is there still some cause for concern?
\end{problem}


\begin{problem}
\myindex{transformation!of data|probidx}%
\label{prob:transformations}%
  For each of the following relationships between a response $y$ and an
  explanatory variable $x$, 
  if possible find a pair of transformations $f$ and $g$ so that
  $g(y)$ is a linear function of $f(x)$:
  \[
  g(y) = \beta_0 + \beta_1 f(x) \;.
  \]
  For example, if 
	  $y = a e^{bx}$, 
	  then $\log(y) = \log(a) + bx$, so 
	  $g(y) = \log(y)$,
	  $f(x) = x$, 
	  $\beta_0= \log(a)$, and 
	  $\beta_1 = b$.

\begin{multicols}{2}
  \begin{enumerate}
	\item 
	  \( y = a b^x \).
	\item 
	  \( y = a x^b \).
	\item
	  \( y = \frac{1}{a + bx} \).
	\item
	  \( y = \frac{x}{a + bx} \).
	\item
	  \(y = a x^2 + b x + c\).
	\item
	  \( \displaystyle y = \frac{1}{1+e^{a+bx}} \).
	  \smallskip
	\item
	  \( \displaystyle y = \frac{100}{1+e^{a+bx}} \).
  \end{enumerate}
\end{multicols}
\end{problem}

\begin{solution}
  \begin{enumerate}
  \item
	$\log(y) = \log(a) + x \log(b)$, 
	so $g(y) = \log(y)$,
	$f(x) = x$, 
	$\beta_0=\log(a)$,
	and $\beta_1=\log(b)$.
  \item
	$\log(y) = \log(a) + b \log(x)$, 
	so $g(y) = \log(y)$,
	$f(x) = \log(x)$, 
	$\beta_0=\log(a)$,
	and $\beta_1=b$.
  \item
	$\frac{1}{y} = a + b x$, 
	so $g(y) = \frac{1}{y}$,
	$f(x) = x$, 
	$\beta_0=a$,
	and $\beta_1=b$.
  \item
	$\frac{1}{y} = \frac{a}{x} + b$, 
	so $g(y) = \frac{1}{y}$,
	$f(x) = \frac{1}{x}$, 
	$\beta_0=b$,
	and $\beta_1=a$.
   \item
	not possible
  \item
	$\frac{1}{y} = 1 + e^{a + bx}$, so
	$\log(\frac{1}{y} - 1) = \log( \frac{1-y}{y} ) = {a + bx}$, 
	so $g(y) = \log(\frac{1-y}{y})$,
	$f(x) = {x}$, 
	$\beta_0=a$,
	and $\beta_1=b$.
  \item
	$\frac{100}{y} = 1 + e^{a + bx}$, so
	$\log(\frac{100}{y} - 1) = \log( \frac{100-y}{y} ) = {a + bx}$, 
	so $g(y) = \log(\frac{100-y}{y})$,
	$f(x) = {x}$, 
	$\beta_0=a$,
	and $\beta_1=b$.
  \end{enumerate}
\end{solution}

\begin{problem}
%\probNote{Would make a good partial solution for students.}%
  What happens to the role of the error terms ($\varepsilon$) when we transform 
  the data?  For each transformation from Exercise~\ref{prob:transformations},
  start with the form
  \[
  g(y) = \beta_0 + \beta_1 f(x) + \varepsilon
  \]
  and transform back into a form involving the untransformed $y$ and $x$ to
  see how the error terms are involved in these transformed linear regression
  models.

  It is important to remember that when we fit a linear model to transformed
  data, the usual assumptions of the model are that the errors in the (transformed) 
  linear form are additive and normally distributed.  The errors may appear
  differently in the untransformed relationship.
\end{problem}

\begin{problem}
\myindex{Tukey bulge|probidx}%
\label{prob:TukeyBulgeSkew}%
The transformations in the ladder of re-expression also affects the shape
of a distribution.  
\begin{enumerate}
\item
If a distribution is symmetric, how does the shape change as we 
move up the ladder?
\item
If a distribution is symmetric, how does the shape change as we 
move down the ladder?
\item
If a distribution is left skewed, in what direction should we move to 
make the distribution more symmetric?
\item
If a distribution is right skewed, in what direction should we move to 
make the distribution more symmetric?
\end{enumerate}
\end{problem}

\begin{solution}
Moving up the ladder will spread the larger values more than the 
smaller values, resulting in a distribution that is right skewed.
\end{solution}

\begin{problem}
\Rindex{pendulum}%
\myindex{physics|probidx}%
By attaching a heavy object to the end of a string, 
it is easy to construct pendulums of different lengths.  Some physics students
did this to see how the period (time in seconds until a pendulum
returns to the same location) depends on the length (in meters) 
of the pendulum.  
The students constructed pendulums of lengths varying from
$10$ cm to $16$ m and recorded the period length (averaged over several
swings of the pendulum).  The resulting data are in
the \dataframe{pendulum} data set.

\begin{enumerate}
	\item
		Fit a power law to this data using a transformation and
		a linear model.  
		How well does the power law fit?  
		What is the estimated power in the power law based on this model?
\item
Fit a power law to this data using a nonlinear model.
How well does the power law fit?  
What is the estimated power in the power law based on this model?

\item
	Compare residual plots and normal-quantile plots for the residuals
	for the two models.  How do the models compare in this regard?
\end{enumerate}
\end{problem}

\begin{solution}
	At first glance, the two models might appear equally good.  In each case the 
	power is a bit below 2 and the fits look good on top of the raw data.  (Note: the 
	function produced by \function{makeFun()} does not know how to invert the log
	transformation on the response variable, so we have to do that ourselves.)
<<warning=FALSE>>=
model <- lm( log(period) ~ log(length), data=pendulum )
model2 <- nls( period ~ A * length^power, data=pendulum, start=list(A=1, power=2) )
f <- makeFun(model)
g <- makeFun(model2)
xyplot(period ~ length, data=pendulum)
plotFun(exp(f(x)) ~ x, col='gray50',add=TRUE)
xyplot(period ~ length, data=pendulum)
plotFun(g(x) ~ x, col='red', lty=2, add=TRUE)
coef(summary(model))
coef(summary(model2))
@
	But if we look at the residuals, we see that the linear model is clearly 
	better in this case.  The non-linear model suffers from heteroskedasticity.
<<>>=
xyplot(resid(model) ~ fitted(model), type=c('p','smooth'))
xyplot(resid(model2) ~ fitted(model2), type=c('p','smooth'))
@
	Both residual distributions are reasonably close to normal, but not perfect.
	In the ordinary least squares model, the largets few residuals are not as large
	as we would expect.
<<>>=
qqmath(~resid(model))
qqmath(~resid(model2))
@
	
	The residuals in the non-linear model show a clear change 
	in variance as the fitted value increases.  This is counteracted by the logarithmic
	transformation of the explanatory variable.  (In other cases, the non-linaer model
	might have the preferred residual distribution.)
\end{solution}



\begin{problem}
The \dataframe{pressure} data set contains 
data on the relation between temperature in degrees Celsius and 
vapor pressure in millimeters (of mercury).
With temperature as the predictor and pressure as the response,
use transformations or nonlinear models as needed to obtain a good fit.
Make a list of all the models you considered and explain
how you chose your best model.
What does your model say about the relationship between 
pressure and temperature?
\end{problem}

\begin{problem}
\Rindex{cornit}%
\Rindex{faraway}%
The \dataframe{cornnit} data set in the package \pkg{faraway} 
contains data from a study investigating the relationship between 
corn yield (bushels per acre) and nitrogen (pounds per acre) fertilizer 
application in Wisconsin.
Using nitrogen as the predictor and corn yield as the response,
use transformations (if necessary) to obtain a good fit.
Make a list of all the models you considered and explain
how you chose your best model.
\end{problem}



\begin{problem}
\Rindex{actgpa}%
\myindex{ACT}%
\myindex{grade point average}%
The data set \verb!actgpa!
%  \url{http://www.calvin.edu/~stob/courses/m344/S07/data/actgpanona.csv}
contains the ACT composite scores and GPAs of some randomly selected seniors 
at a Midwest liberal arts college.

  \begin{enumerate}
	\item
	  Give a 95\% confidence interval for the mean ACT score 
	  of seniors at this school.
	\item
	  Give a 95\% confidence interval for the mean GPA 
	  of seniors at this school.
	\item
	  Use the data to estimate with 95\% confidence 
	  the average GPA for all students who score 25 on the ACT.
	\item
	  Suppose you know a high school student who scored 30 on the ACT.
	  Estimate with 95\% confidence his GPA as a senior in college.
	\item
	  Are there any reasons to be concerned about the analyses you
	  have just done?  Explain.
  \end{enumerate}
\end{problem}

\begin{solution}
	\begin{enumerate}
		\item
			We can build a confidence interval for the mean by fitting 
			a model with only an intercept term.
<<act-gpa, tidy=FALSE>>=
grades <- actgpa
confint(lm(ACT ~ 1, data=grades))
@
But this isn't the only way to do it.  Here are some other ways.
<<>>=
# here's another way to do it; but you don't need to know about it
t.test(grades$ACT)
# or you can do it by hand
x.bar <- mean(~ACT, data=grades); x.bar
n <- nrow(grades); n
t.star <- qt(.975, df=n-1); t.star
SE <- sd(~ACT, data=grades) / sqrt(n); SE
ME <- t.star * SE; ME
@
			So the CI is $\Sexpr{x.bar} \pm \Sexpr{ME}$.  Of course, that is too many digits, we 
			should do some rounding to
			$\Sexpr{round(x.bar,1)} \pm \Sexpr{round(ME,1)}$.  
		\item
<<>>=
confint(lm(GPA ~ 1, data=grades))
# this could also be done the other ways shown above.
@
		\item
<<>>=
grades.model <- lm(GPA~ACT,grades)
f <- makeFun(grades.model)
f(ACT=25, interval="confidence")
@
		
		\item
<<>>=
f(ACT=30, interval="prediction")
@
		\item
<<>>=
xyplot(GPA~ACT,data=grades,panel=panel.lmbands)
xyplot(resid(grades.model) ~ fitted(grades.model),type=c('p','smooth'))
@
			There are no major concerns with the regression model. The
			residuals look pretty good.  (There is perhaps a bit more variability
			in GPA for the lower ACT scores and if you said you were worried about
			that, I would not argue.)  

			The prediction intervals are very wide and hardly useful, however.
			It's pretty hard to give a precise estimate for an individual
			person -- there's just too much variability from preson to person, 
			even among people with the same ACT score. 
	\end{enumerate}
\end{solution}

\begin{problem}
\Rindex{drag}%
\myindex{physics|probidx}%
\label{prob:drag1}
In the absence of air resistance, a dropped object will continue to accelerate
as it falls.  But if there is air resistance, the situation is different.
The drag force due to air resistance depends on the velocity of an object
and operates in the opposite direction of motion.  Thus as the object's velocity
increases, so does the drag force until it eventually equals the force
due to gravity.  At this point the net force is $0$ and the object 
ceases to accelerate, remaining at a constant velocity called the 
terminal velocity.
\myindex{terminal velocity|probidx}

Now consider the following experiment to determine how terminal velocity
depends on the mass (and therefore on the downward force of gravity) of 
the falling object.  A helium balloon is rigged with a small basket and 
just the right ballast to make it neutrally buoyant.  Mass is then added
and the terminal velocity is calculated by measuring the time it takes to
fall between two sensors once terminal velocity has been reached.

The \verb!drag! data set contains the results of such an experiment
conducted by some undergraduate physics students.  Mass is measured 
in grams and velocity in meters per second.  
(The distance between the two sensors used for determining
terminal velocity is given in the \verb!height! variable.)

By fitting models to this data, determine which of the following ``drag laws'' matches the data best:
\begin{itemize}
\item
Drag is proportional to velocity.
\item
Drag is proportional to the square of velocity.
\item
Drag is proportional to the square root of velocity.
\item
Drag is proportional to the logarithm of velocity.
\end{itemize}
\end{problem}

\begin{solution}
The best of these four models is a model that says drag is proportional
to the square of velocity.
Given the design of the experiment, it makes the most sense to fit
velocity as a function of drag force.  Here are several ways we could 
do the fit:
<<drag>>=
model1 <- lm(velocity^2 ~ force.drag, drag)
model2 <- lm(velocity ~ sqrt(force.drag), drag)
model3 <- lm(log(velocity) ~ log(force.drag), drag)
@
<<>>=
coef(summary(model1))
@
<<>>=
coef(summary(model2))
@
<<>>=
coef(summary(model3))
@
Note that \verb!model1!, \verb!model2!, and \verb!model3! are not equivalent, 
but they all tell roughly the same story.
<<warning=FALSE>>=
xyplot( velocity ~ force.drag, data=drag)
f1 <- makeFun(model1)
f2 <- makeFun(model2)
f3 <- makeFun(model3)
plotFun( sqrt(f1(x)) ~ x, add=TRUE, alpha=.4 )
plotFun( f2(x) ~ x, add=TRUE, alpha=.4, col='red', lty=2 )
plotFun( exp(f3(x)) ~ x, add=TRUE, alpha=.4, col='brown', lty=3 )
@

The fit for these models reveals some 
potential errors in the design of this experiment.  Separating out the data
by the height used to determine velocity suggests that perhaps some of the 
velocity measurements are not yet at terminal velocity.
In both groups, the velocities for the greatest drag forces are 
not as fast as the pattern of the remaining data would lead us to expect.

<<drag-plots>>=
xyplot(velocity^2 ~ force.drag, drag, groups=height)
xplot(model1,w=1)
xyplot(velocity ~ force.drag, drag, scales=list(log=T),groups=height)
xplot(model3,w=1)
@
\end{solution}

\begin{problem}
\Rindex{drag}%
\myindex{physics|probidx}%
\label{prob:drag-problems}%
Construct a plot that reveals a likely systematic problem with the 
\verb!drag! (see Exercise~\ref{prob:drag1}) data set.
Speculate about a potential cause for this.
\end{problem}

\begin{solution}
See previous problem.
\end{solution}

\begin{problem}
\Rindex{drag}%
\myindex{physics|probidx}%
Exercise~\ref{prob:drag-problems} suggests that some
of the data should be removed before analyzing the \verb!drag! data set.
Redo Exercise~\ref{prob:drag1} after removing this data.
\end{problem}

\begin{problem}
\Rindex{spheres}%
\myindex{physics|probidx}%
The \verb!spheres! data set contains measurements of the diameter (in meters)
and mass (in kilograms) of a set of steel ball bearings.  We would expect
the mass to be proportional to the cube of the diameter.  Fit a model 
and see if the data reflect this.
\end{problem}


\begin{problem}
\Rindex{spheres}%
\myindex{physics|probidx}%
The \verb!spheres! data set contains measurements of the diameter (in meters)
and mass (in kilograms) of a set of steel ball bearings.  We would expect
the mass to be proportional to the cube of the diameter.  
Using appropriate transformations fit two models: one that predicts 
mass from diameter and one that predicts diameter from mass.  
How do the two models compare?
\end{problem}

\begin{problem}
\label{prob:utilities-therms-by-temp}%
\Rindex{utilities}%
The \verb!utilities! data set has information from utilities
bills at a Minnesota residence.
Fit a linear model that predicts \variable{thermsPerDay} from \variable{temp}.
\begin{enumerate}
\item
What observations should you remove from the data before doing the 
analysis? Why?  
\item
Are any transformations needed?
\item
How happy are you with the fit of your model?  Are there any reasons
for concern?
\item
Interpret your final model (even if it is with some reservations listed in
part c)).  
What does it say about the relationship between average monthly temperature 
and the amount of gas used at this residence?  What do the parameters represent?
\end{enumerate}
\end{problem}


\section{Nonlinear Least Squares}

Another approach to non-linear relationships is called \term{nonlinear least squares}
or \term{nonlinear regression}.  In this approach, instead of attempting to transform
the relationship until it becomes linear, we fit a nonlinear function by minimizing the 
the sum of the squared residuals relative to that (paramterized) nonlinear
function (form).  That is, our model now becomes
\[
y = f(x) + \varepsilon
\]
where $f$ may be any parameterized function.

The \R\ function for fitting these
models is \function{nls()}.  This function works much like \function{lm()},
but there are some important differences:
	\begin{enumerate}
		\item
			Because the model does not have to be linear, we have
			to use a more verbose description of the model.
		\item
			Numerical optimization is used to fit the model, and the algorithm
			used needs to be given a reasonable starting point for its search.
			Specifying this starting point simultaneously lets \R\ know what the 
			parameters of the model are.  (Each quantity with a starting value
			is considered a parameter, and the algorithm will adjust all the parameters
			looking for the best fit -- i.e., the smallest MSE (and hence also
			the smallest SSE and RMSE).
	\end{enumerate}

Let's illustrate with an example.

\begin{example}
	Returning to the ball dropping experiment, let's fit 
	\begin{align}
	\variable{time} &= \alpha_0 + \alpha_1 \sqrt{\variable{height}}
	\label{eq:balldrop}
	\end{align}
	using nonlinear least squares.  
<<tidy=FALSE>>=
nls.model <- nls( time ~ alpha0 + alpha1 * sqrt(height), 
                  data=balldrop, 
                  start=list(alpha0=0, alpha1=1) )
@
	Notice how the model formula compares with the formula in (\ref{eq:balldrop}).
	The starting point for the algorithm is specified with 
	\code{start=list(alpha0=0, alpha1=1)}, which also declares that 
	the parameters to be fit.

	We can obtain the coefficients of the fitted model with
<<>>=
nls.model
@
or
<<>>=
coef(nls.model)
@
A more complete summary can be obtained by
<<>>=
summary(nls.model)
@
We can restrict our attention to the coefficients table with
<<>>=
coef(summary(nls.model))
@

<<>>=
f <- makeFun(nls.model)
xyplot( time ~ height, data=balldrop )
plotFun( f(height) ~ height, add=TRUE, col='gray40' )
xyplot( resid(nls.model) ~ fitted(nls.model) )
@

We can compare this to the ordinary least squares model by plotting both together on the same plot.
<<fig.keep='last'>>=
lm.model <- lm( time ~ sqrt(height), data=balldrop)
g <- makeFun(lm.model)
xyplot( time ~ height, data=balldrop )
plotFun( f(height) ~ height, add=TRUE, col='gray40', lwd=3 )
plotFun(g(height) ~ height, add=TRUE, col='red', lwd=1, lty=2)
@
In this particular case, there is very little difference between the two models, but this is not always the case.
<<>>=
coef(nls.model)
coef(lm.model)
@
\end{example}

\begin{example}
	Here is example where we fit a different model to the \dataframe{balldrop} data, namely
	\[
	\variable{time} = \alpha * \variable{height}^p
	\]
<<tidy=FALSE>>=
power.model <- nls( time ~ alpha * height^power, data=balldrop, 
                    start=c(alpha=1, power=.5) )
coef(summary(power.model))
@

A power law can also be fit using \function{lm()} by using a log-log transformation.
<<>>=
power.model2 <- lm( log(time) ~ log(height), data=balldrop )
coef(summary(power.model2))
@
Again, the parameter estimates (and uncertainties) are very similar.  Recall that to 
compare our intercept in the second model to the $\alpha$ value in the first model,
we must untransform:
<<>>=
exp(coef(power.model2)[1])
@
We can use the delta method to estimate the uncertainty.  Since 
$\frac{d}{dx} e^x = e^x$ the uncertainty is approximately
\[ 
\Sexpr{exp(coef(power.model2)[1])} \cdot \Sexpr{coef(summary(power.model2))[1,2]}
=
\Sexpr{exp(coef(power.model2)[1]) * coef(summary(power.model2))[1,2]}
\]

\end{example}

\begin{example}
	In addition to comparing estimated parameters and their uncertainties, we should
	always look at the residuals of our model.  For both the linear regression and 
	the nonlinear least squares models, the assumption is that the error terms are
	independent, normally distributed, and have a common standard deviation.
	From the plots below we see
	\begin{enumerate}
		\item The nonlinear least squares model is a better match for these assumptions
			than the linear regression model.
		\item
			Both models reveal a lack of independence -- at a given height, the 
			residuals move up or down as a cluster as was discussed in the 
			previous section.  Neither model is designed to handle 
			this flaw in the design of the experiment.
		\end{enumerate}
<<>>=
qqmath(resid(power.model2))
qqmath(resid(power.model))
xyplot( resid(power.model) ~ fitted(power.model) )
xyplot( resid(power.model2) ~ fitted(power.model2) )
@

\end{example}

Now let's take a look at an example where we need the extra flexibilty
of the nonlinear least squares approach.

\begin{example}
	A professor at Macalester College put hot water in a mug and recorded the temperature as it cooled. 
	Let's see if we can fit a reasonable model to this data
<<>>=
xyplot( temp ~ time, data=CoolingWater, ylab="temp (C)", xlab="time (sec)")
@

Our first guess might be some sort of exponential decay
<<tidy=FALSE, fig.keep='last'>>=
cooling.model1 <- nls( temp ~ A * exp( -k * time), data=CoolingWater, 
                       start=list(A=100, k=0.1) )
f1 <- makeFun(cooling.model1)
xyplot( temp ~ time, data=CoolingWater, xlim=c(-50,300), ylim=c(0,110), 
        ylab="temp (C)", xlab="time (sec)")
plotFun( f1(time) ~ time, add=TRUE)
@

That doesn't fit very well, and there is a good reason.  The model says that eventually the water will freeze because
\[
\lim_{t \to \infty} A e^{-k t} = 0
\]
when $k >0$.  But clearly our water isn't going to freeze sitting on a lab table.  We can fix this by 
adding in an offset to account for the ambient temperature:
<<tidy=FALSE, fig.keep="last">>=
cooling.model2 <- nls( temp ~ ambient + A * exp( k * (1+time) ), data=CoolingWater,
                      start=list(ambient=20, A=80, k=-.1) )
f2 <- makeFun(cooling.model2)
xyplot( temp ~ time, data=CoolingWater, xlim=c(-50,300), ylim=c(0,110),
        ylab="temp (C)", xlab="time (sec)")
plotFun( f1(time) ~ time, add=TRUE, lty=2, col="gray80")
plotFun( f2(time) ~ time, add=TRUE, col = "steelblue")
@
This fits much better.  Furthermore, this model can be derived from a differential equation
\[
\frac{dT}{dt} = -k (T_0 - T_{\mathrm{ambient}})
\;,
\]
known as Newton's Law of Cooling.
\myindex{Newton's Law of Cooling}%

Let's take a look at the residual plot
<<>>=
xyplot( resid(cooling.model2) ~ time, data=CoolingWater ) 
plot(cooling.model2, which=1)
@
Hmm.  These plots show a clear pattern and very little noise.
The fit doesn't look as good when viewed this way.  
It suggests that Newton's Law of Cooling does not take into account all that is going on here.
In particular, there is a considerable amount of evaporation (at least at the beginning when the 
water is warmer).  More complicated models that take this into account can fit even better.
For a discussion of a model that includes evaporation, 
see \url{http://stanwagon.com/public/EvaporationPortmannWagonMiER.pdf}.\footnote{
The model with evaporation adds another complication in that the resulting
differential equation cannot be solved algebraically, so there is no algebraic
formula to fit with \function{nls()}.  
But the method of least squares can still be used by creating a parameterized
numerical function that computes the sum of squares and using a numerical
minimizer to find the optimal parameter values.  Since the use of numerical
differential equation solvers is a bit beyond the scope of this course, we'll
leave that discussion for another day.}
\end{example}

\subsection{Choosing Between Linear and Non-linear Models}

So how do we choose between linear and non-linear models?  Let's enumerate some
of the differences between them:

\begin{enumerate}
	\item
		Some models cannot be expressed as linear models, even after transformations.

		In this case we only have one option, the non-linear model.

	\item
		Linear models can be fit quickly and accurately without numerical
		optimization algorithms because they satisfy nice linear algebra
		properties.

		The use of numerical optimizers in non-linear least squares models
		makes them subject to potential problems with the optimizers.  They may
		not converge, may converge to the wrong thing, or convergence may depend
		on choosing an appropriate starting point for the search.

	\item
		The two types of models make different assumptions about the error terms.

		In particular, when we apply transformations to achieve a linear model,
		those transformations often affect the distribution of the error terms as 
		well.  For example, if we apply a log-log transformation to fit a power law,
		then the model is
		\begin{align*}
		\log( y ) &= \beta_0 + \beta_1 \log(x) + \varepsilon
		\\
		y &= e^{\beta_0}  x^{\beta_1} e^\varepsilon
		\\
		y &= \alpha  x^{\beta_1} e^\varepsilon
	\end{align*}
	So the errors are multiplicative rather than additive and
	they have a normal distribution \emph{after} applying the logarithmic
	transformation.  This implies that the relative errors should be about
	the same magnitude rather than the absolute errors.
	
	This is potentially very different from the nonlinear
	model where the errors are additive:
	\[
	y = \alpha x^\beta + \varepsilon
	\]

		Plots of residuals vs. fits and qq-plots for residuals can help us diagnose
		whether the assumptions of a model are reasonable for a particular 
		data set.

	\item
		Linear models provide an easy way to produce confidence intervals for 
		a mean response or an individual response.

		The models fit using \function{nls()} do not have this capability.
\end{enumerate}


\newpage
\section*{Exercises}
\shipoutProblems


\ifsolutions
\ifsolutionslocal
\newpage
\section*{Solutions}
\shipoutSolutions
\fi
\fi
