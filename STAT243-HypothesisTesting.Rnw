\Sexpr{set_parent('Math241-S14.Rnw')}

\chapter{Hypothesis Testing}

This chapter is concerned with statistical hypothesis testing, and how we can use it to make inferences and draw conclusions from data about questions of scientific interest.

\section{Experimental Design in Statistics}
Before we begin to talk about hypothesis testing, let's review the general process of designing and carrying out a statistical experiment.

\begin{enumerate}
  \item Determine the question of interest.

	Just what is it we want to know?  It may take some effort to 
	make a vague idea precise.  The precise questions may not exactly
	correspond to our vague questions, and the very exercise of 
	stating the question precisely may modify our question.  Sometimes
	we cannot come up with any way to answer the question we really want
	to answer, so we have to live with some other question that is 
	not exactly what we wanted but is something we can study and will
	(we hope) give us some information about our original question.


  \item 
	Determine the \term{population}. 
	\myindex{population}%

	Just who or what do we want to know about?  For example, are we only interested in
	one specific person, or women in general, or all women, or all people?  Or, are we interested in the energy efficiency of one particular device, or all the machines in a certain factory, or all machines of a certain type, or all machines of a certain class, or all factories in a certain industry?

  \item
	Select \term{measurements}.

	We are going to need some data.  
	We get our data by making some measurements.
	These might be physical measurements with some device (like a ruler
	or a scale).
	But there are other sorts of measurements too, 
	like the answer to a question on a form.
	Sometimes it is tricky to figure out just what to measure.
	(How do we measure happiness or intelligence, for example?)
	Just how we do our measuring will have important consequences 
	for the subsequent statistical analysis.
	The recorded values of these measurements are called
	\term{variables} (because the values vary from one individual to another).

	  \item
	Determine the \term{sample}.
	\myindex{sample}%

	Usually we cannot measure every individual in our population; we have 
	to select some to measure.  
	But how many and which ones?  
	These are important questions that must be answered.
	Generally speaking, bigger is better, but it is also more expensive.
	Moreover, no size is large enough if the sample is selected inappropriately.

  For example, if we wanted to draw conclusions about energy use across a whole industry, we would have to be careful not to sample from just a single factory, or a single type of manufacturing device.  If we wanted to draw conclusions about all people, we would have to be careful not to study only male college students.  The sample should be a random selection from the whole population (or as close as we can get to that standard).	


  \item
	Make and record the measurements.

	Once we have the design figured out, we have to do the legwork of 
	data collection.  This can be a time-consuming and tedious process.
	A study of public opinion may require many thousands of phone calls or 
	personal interviews.
	In a laboratory setting, each measurement might be the result 
	of a carefully performed laboratory experiment.

  \item Organize the data.

	Once the data have been collected, it is often necessary or useful
	to organize them.  Data are typically stored in spreadsheets or 
	in other formats that are convenient for processing with 
	statistical packages.  Very large data sets are often stored in 
	databases.  
	
	Part of the organization of the data may involve producing graphical and
	numerical summaries of the data.  These summaries may give us initial
	insights into our questions or help us detect errors that may have occurred
	to this point.

  \item Draw conclusions from data.

	Once the data have been collected, organized, and analyzed, we need
	to reach a conclusion.  
	What is the answer to our scientific question? Is our idea or hypothesis about the way things work incorrect, or do the data support it? How sure are we about these conclusions?

  \item Produce a report.

		Typically the results of a statistical study are reported in 
		some manner.  This may be as a refereed article in an academic 
		journal, as an internal report to a company, or as a solution
		to a problem on a homework assignment.  These reports may themselves
		be further distilled into press releases, newspaper articles,
		advertisements, and the like.  The mark of a good report
		is that it provides the essential information about each 
		of the steps of the study.

\end{enumerate}


\section{A General Framework}
In statistical hypothesis testing, we can follow the following general procedure.  We usually begin with some idea about how the process we are studying should work.  For example, we might have a hunch that highway bridges with higher traffic flows are in poorer condition, and therefore merit more frequent repairs.  For statistical hypothesis testing, we must translate that ``hunch" into a \emph{testable} \textbf{null hypothesis}, often called $H_0$:  one that can be demonstrated to be very unlikely in light of our data.  In the case of the bridges, a testable null hypothesis might be that bridge condition \emph{does not} depend on traffic flow.  Then, if our data shows a strong apparent relationship between condition and traffic, we have evidence to \emph{reject} the null hypothesis, and the data support the idea that there is some relationship between condition and traffic.

Null hypothesis are usually ``boring," no-result hypotheses:  there is no pattern; there is no relationship between the variables of interest; there is no difference between the two samples of interest.  If we can reject the null hypothesis in a certain case, we have some evidence -- but \emph{NOT} proof -- that there \emph{is} an interesting pattern in our data, and thus in the population we are trying to draw conclusions about. 

The alternative to the null hypothesis is called the alternative hypothesis, often called $H_1$.  The alternative hypothesis, stated most generally, is usually some form of ``the null hypothesis is not true" -- so there IS a pattern in the data, or a difference between the samples, etc.

\begin{description}
	\item[hypothesis] A statement that can be true or false.
	\item[statistical hypothesis] A hypothesis about a parameter or parameters.
\end{description}

In our bridge example, a statistical null hypothesis $H_0$ might be: the true slope of the regression of condition as a function of traffic is 0.

\begin{examples}
	The following are examples of null hypotheses.
	\begin{enumerate}
		\item
			$H_0: \mu = 0$.  (The population mean is 0.)
		\item
			$H_0: \beta_1=0$.  (The ``true'' slope is 0 -- assuming a model like $\E(Y) = \beta_0 + \beta_1 x$.)  
		\item
			$H_0: \beta_1 = \beta_2$ (Two parameters in the model are equal.)
		\item
			$H_0: \beta_2 = \beta_3 = 0$ (Two parameters in the model are both equal to 0.)
	\end{enumerate}
\end{examples}

\subsection{The Four Step Process}
Hypothesis testing generally follows a four-step process.
\begin{enumerate}
	\item 
		State the null ($H_0$) and alternative ($H_1$) hypotheses.

		The null hypothesis is on trial and innocent until proven guilty.  We will
		render one of two verdicts:  Reject the null hypothesis (guilty) or do not reject
		the null hypothesis (not guilty).  \emph{Important!  We can never \textbf{accept} or \textbf{prove} either hypothesis -- only reject the null as unlikely, or fail to reject the null since it seems likely.}

	\item
		Compute a test statistic.

		For a statistical hypothesis test, all the evidence against the null hypothesis must be summarized in a
		single number called the test statistic.  It is a statistic because 
		it is a number computed from the data.  It is called a test statistic
		because we are using it to do hypothesis testing.

	\item
		Determine the p-value.

		The p-value is a probability:  \emph{Assuming} the null hypothesis is true,
		how likely are we to get at least as much evidence against it as we have in our data (i.e., \emph{a
		test statistic at least as unusual as the one observed}) just by random chance?

	\item
		Interpret the results.

		If the p-value is small, then one of two things is true:
		\begin{enumerate}
			\item The null hypothesis is true and something very unlikely occurred
				in our sample, or 
			\item The null hypothesis is false, so it is unsurprising that our observed data yield statistics that seem unlikely based on that (incorrect, untrue) hypothesis.
		\end{enumerate}
		For this reason we consider small p-values to provide evidence against the null
		hypothesis.
\end{enumerate}

How small is ``small", and how small does a p-value have to be before we reject the null hypothesis?  Often, a \textbf{significance level} (usually called $\alpha$) of 0.05 is used.  Sometimes $\alpha = 0.01$ is used instead.  Basically, this corresponds to a 5\% chance of seeing results as extreme as those found in our data, were the null hypothesis really true.  If we want to be more conservative about our judgement (not rejecting the null hypothesis unless the evidence in the data is stronger against it), we could use a smaller $\alpha$ value.

It is also common for researchers to simply report the p-values they obtain from their analysis, allowing readers to draw conclusions on their own.

If talking about the results of a statistical hypothesis test in a report, the following language may be useful.  It relates statements about the strength of the evidence found in the data with p-values (approximately -- there are no universal standards and rules here, just guidelines).

\begin{center}
\begin{tabular}{r|r}
Approximate p-value & Translation\\
\hline
$> 0.10$ & No convincing evidence against the null hypothesis\\
$0.05-0.10$ & Weak evidence against the null hypothesis\\
$0.01-0.05$ & Some evidence against the null hypothesis\\
$<0.01$ & Strong evidence against the null hypothesis\\
$<0.001$ & Very strong evidence against the null hypothesis\\
\hline
\end{tabular}
\end{center}

\section{Statistical Significance}
The word ``significant" has a special meaning in statistics.  If we say that a difference or relationship between variables is significant, that means that we have applied a hypothesis test, and have \emph{failed} to reject the null hypothesis -- in other words, we have data that provides some evidence against the null hypothesis, and supporting the alternative.

So, a pattern strong enough cause us to reject a null hypothesis of ``no difference" or ``no pattern" or ``no relationship" is called a \term{statistically significant} difference, pattern, or relationship.  Differences or relationships may fail to be statistically
significant if they are small or weak, if they are masked by underlying variability, or if there is too 
little data.  Good studies will collect enough data and work to reduce variability (if that is possible)
in order to have a reasonable expectation of detecting differences if they are large enough to 
be scientifically interesting.

\section{T-tests}

Many hypothesis tests are conducted based on a $t$-distribution, and so they are called ``t-tests". This is because, according to the Central Limit Theorem, the sampling distributions of most of our statistics (parameter estimates calculated from data) follow Normal distributions.  But just as we did when computing confidence intervals, we'll always have to use the t-distribution rather than the normal distribution, since we don't know $\sigma$ (the true population standard deviation), and since our sample size is finite.   These t-tests all use a similar sort
of test statistic:

\[
t = \frac{\mbox{estimate} - \mbox{hypothesized value}}{\mbox{standard error}}
\]

The numerator tells us that the more the estimate and the hypothesized value differ, 
the stronger the evidence.  The denominator tells us that differences mean more when the standard
deviation is small than when the standard deviation is large.

The test statistic is converted to a p-value by comparing it to the t-distribution with appropriate
degrees of freedom.  For linear models, this is the degrees of freedom associated with the 
residual standard error.  If considering some statistic (say, the mean value) for a single variable, the degrees of freedom will be n-1, where n is the sample size.

\subsection{The 1-sample t-test}
The 1-sample t-test tests the null hypothesis
\begin{itemize}
	\item $H_0: \mu = \mu_0$, vs.
	\item $H_a: \mu \neq 0$.
\end{itemize}
That is, it tests whether there is evidence that the mean of some population ($\mu$) is different
from some hypothesized value ($\mu_0$) -- often $\mu_0 = 0$.

\begin{example}
Let's look at some data on weight loss programs.  In this data set, there were two groups.  One group
received a monetary incentive if they lost weight while following a weight loss program.  The controls
did not receive a monetary incentive, but followed the same program otherwise.  Our null hypothesis is that the control participants would not lose weight -- that the true weight loss without incentives would average 0 pounds. Let's see whether on average the controls lost weight:
<<>>=
require(Stat2Data)
data(WeightLossIncentive)
Controls <- subset(WeightLossIncentive, Group=="Control")
favstats( ~ WeightLoss, data=Controls )
bwplot( ~WeightLoss, data=Controls)
@
The standard error when doing inference for a mean is 
\[
SE = \frac{s}{\sqrt{n}} = 
\frac{ 
	\Sexpr{sd(~WeightLoss, data=Controls)}
}{
	\sqrt{ \Sexpr{nrow(Controls)} } 
}
=
\Sexpr{sd(~WeightLoss, data=Controls)/sqrt(nrow(Controls))} 
\]
<<tidy=FALSE>>=
SE <- 9.108 / sqrt(19); SE
@
If we want to test our null hypothesis, then we compute a t-statistic:
<<tidy=FALSE>>=
t <- (3.92 - 0) / SE; t
@
and from this a p-value, which is the tails probability for a t-distribution with 18 degrees of freedom.  In other words, the p-value gives the probability of getting a test statistic at least as big as the one we really got, assuming that the test statistic follows a t-distribution with $n-1$ degrees of freedom.

First, we find the probability of observing a test statistic of $t= \Sexpr{t}$ or larger.  Then, we have to double this value -- it would be at least as unlikely to see a test statistic of $\Sexpr{-t}$ or smaller.  Our p-value should give the area under the t-distribution curve for x-values smaller than $\Sexpr{-t}$ \emph{and} larger than $\Sexpr{t}$; it's the shaded area in the figure below:
<<echo=FALSE, fig.width=6, fig.height=5>>=
x <- seq(from=-5, by=0.01, to=5)
dtvals <- dt(x, df=18)
par(lwd=2.5, cex=1.25, cex.lab=1.25, cex.axis=1.25, cex.main=1.25)
plot(x,dtvals, type="l", xlab="Test Statistic T", ylab="Probability Density", lwd=2.5)
points(x=c(-t,t), y=dt(c(-t,t),df=18), pch=19, cex=1.5)
text(x=c(-t,t)+ c(-0.2, 0.2), y=dt(c(-t,t),df=18)+0.04, labels=c("-t", "t"), cex=1.27)
lines(x=c(-t,-t), y=c(0,dt(-t,df=18) ), lwd=3)
lines(x=c(t,t), y=c(0,dt(t,df=18) ), lwd=3)
xspots <- seq(from=t, by=0.025, to=5)
for (k in 1:length(xspots)){
  lines(x=c(-xspots[k], -xspots[k]), y=c(0,dt(-xspots[k],df=18) ), lwd=1, lty=3)
  lines(x=c(xspots[k], xspots[k]), y=c(0,dt(xspots[k],df=18) ), lwd=1, lty=3)
}
@

<<>>=
1 - pt( t, df=19-1 )
2 * ( 1 - pt( t, df=19-1 ) )
@
Our p-value is $0.077$ (1 or 2 significant digits are sufficient for reporting 
p-values).  This is not compelling evidence that the weight loss program (without incentives) actually leads to a change in weight.  A change this large could occur just by chance in nearly 8\% of samples.
\end{example}

\begin{example}
In R, there is also a function to automate this t-test:
<<>>=
t.test( ~ WeightLoss, data=Controls )
@
	If we don't want so much out put, we can ask R to report only the p-value:
<<>>=
pval( t.test( ~ WeightLoss, data=Controls ) )
@
By default, \texttt{t.test()} uses a significance level $\alpha = 0.05$. If we want to specify a different $\alpha$, we can use the input \texttt{conf.level} as follows:
<<>>=
t.test( ~WeightLoss, data=Controls, conf.level=0.01)
@
\end{example}

\subsection{The paired 2-sample t-test}
Sometimes, a dataset contains \emph{paired} observations.  These might be, for example, two measurements on the same experimental subject before and after some experimental treatment; measurements on the same subject at two times, or in two different situations; (or others).  In this case, we can not consider the measurements within a pair to be independent of each other.  But what we are really interested in is the magnitude of the difference between the 2 observations in each pair.  So this ``paired t-test" problem reduces to a one-sample t-test, where the test statistic is constructed using the \emph{differences} $D$ between the 2 measurements in each pair:

\[
t = \frac{\mbox{observed average difference} - \mbox{hypothesized average difference}}{\mbox{standard error}}
\]

Let's consider an example.
\begin{example}
The following table provides the corneal thickness in microns of
both eyes of patients who have glaucoma in one eye:

\begin{center}
\begin{tabular}{c|cccccccc}
Healthy & 484 & 478 & 492 & 444 & 436 & 398 & 464 & 476 \\
Glaucoma & 488 & 478 & 480 & 426 & 440 & 410 & 458 & 460 \\
\hline Difference & 4 & 0 & -12 & -18 & 4 & 12 & -6 & -16
\end{tabular}
\end{center}

The corneal thickness is likely to be similar in the two eyes of any
single patient, so that the two observations on the same patient
cannot be assumed to be independent. But maybe (after accounting for differences between people), there is some difference in corneal thickness that has to do with the presence or absence of glaucoma.  First, we can have a look at the data:
<<echo=FALSE, fig.width=6, fig.height=4.5>>=
par(cex=1.2)
plot(x=c(1:length(glaucoma)), y=glaucoma, pch=19,
     ylab="Corneal Thickness (um)", col="black",
     xlab="Subject Number", ylim=c(400,500))
points(c(1:length(healthy)), healthy, pch=17, col="grey", 
       ylab="Corneal Thickness (um)", 
     xlab="Subject Number", cex=1)

legend("bottomleft", legend=c("Glaucoma", "Healthy"),
       pch=c(19,17), col=c("black", "grey"), bty="n")
@

It looks like healthy corneas are often thicker.  To try to quantify this, we consider the difference
between each pair of observations, denoted by $d_i$. We wish to
test,
\[
H_0: \mu = 0 \qquad \mbox{ vs } \qquad H_1: \mu \ne 0.
\]
Under $H_0$,
\[
T = \frac{\bar{D}}{S/\sqrt{n}} \sim t_{n-1}.
\]

In R, we can compute $D$ (the average of $d_i$) and its standard error to obtain the test statistic $t$ and the corresponding p-value:
<<>>=
n <- 8
healthy <- c(484 , 478 , 492 , 444 , 436 , 398 , 464 , 476)
glaucoma <- c(488 , 478 , 480 , 426 , 440 , 410 , 458 , 460)
diffs <- glaucoma-healthy
D <- mean(diffs)
SE <- sd(diffs)/sqrt(n)
t <- (D - 0)/SE; t
pval <- 2 * pt(t, df=n-1); pval
@

Note that we used 2 * pt(...) because our test statistic $t$ was negative.  If it had been positive, we would have used 2 * (1-pt(...)) instead.  The illustration below may help make this clearer -- the p-value we are computing corresponds to the shaded areas in the plot. 
<<echo=FALSE, fig.width=6, fig.height=5>>=
x <- seq(from=-5, by=0.01, to=5)
dtvals <- dt(x, df=7)
par(lwd=2.5, cex=1.25, cex.lab=1.25, cex.axis=1.25, cex.main=1.25)
plot(x,dtvals, type="l", xlab="Test Statistic T", ylab="Probability Density", lwd=2.5)
points(x=c(-t,t), y=dt(c(-t,t),df=7), pch=19, cex=1.5)
text(x=c(-t,t)+ c(0.2, -0.2), y=dt(c(-t,t),df=7)+0.04, labels=c("-t", "t"), cex=1.27)
lines(x=c(-t,-t), y=c(0,dt(-t,df=7) ), lwd=3)
lines(x=c(t,t), y=c(0,dt(t,df=7) ), lwd=3)
xspots <- seq(from=-t, by=0.025, to=5)
for (k in 1:length(xspots)){
  lines(x=c(-xspots[k], -xspots[k]), y=c(0,dt(-xspots[k],df=7) ), lwd=1, lty=3)
  lines(x=c(xspots[k], xspots[k]), y=c(0,dt(xspots[k],df=7) ), lwd=1, lty=3)
}
@

We can also let R make all the computations for us. Notice that we use the \emph{differences} between pairs as the input data, rather than the raw data itself.
<<>>=
t.test(~ diffs, df=7)
@

\end{example}

\subsection{The (unpaired) 2-sample t-test}
In some cases, our research sample may contain data from 2 different categories of observational units.  For example, in the weight loss data, there were the control participants we considered above; there were also a number of incentivized participants, who received money if they lost weight.  We might be interested in considering whether the incentive made a difference.  Before we begin, we can plot the data:
<<>>=
bwplot(WeightLoss~Group, data=WeightLossIncentive)
@

It looks like the incentive group lost more weight.  But how can we judge whether the difference between groups is really an effect of the incentive, and not just random variation?

In this case, the null hypothesis $H_0$ would be that the average weight loss by control participants ($\mu_c$) was the same as the average weight loss by incentivized participants ($\mu_i$) -- $H_0: \mu_c = \mu_i$.  The alternative hypothesis would be $H_1: \mu_c \neq \mu_i$ -- there was a difference between the two groups.

But in this case -- with data from 2 different categories -- how can we compute the appropriate standard error and test statistic for a t-test?  We have to consider the fact that there may be different numbers of data points in the 2 categories.  Let $n$ be the sample size within the first category $X$ (control participants in the example), and $m$ be the sample size in the other category $Y$ (incentive participants in the example). How can we compute a standard error for a difference in means between the two groups?

In this case, if we assume that the sample variaces are equal between the two categories, then we can define the pooled sample variance $s_p$:

$$s^2_p = \frac{(n-1)s_X^2 + (m-1)s_Y^2}{(m+n-2)}$$

Here, $s_Y$ and $s_X$ are the sample standard deviations within each category.  We will not provide proof here, but it is known that in this case, the test statistic is:

\[
T = \frac{\bar{X}-\bar{Y}}{S_p\sqrt{\frac{1}{m}+\frac{1}{n}}}
\sim t_{m+n-2}.
\]

As written in the equation above, this test statistic follows a t-distribution with $m+n-2$ degrees of freedom.  We could carry out a two-sample t-test by hand using the test statistic defined above, or we can use the function \texttt{t.test()} and R will do the computations for us. For the weight loss example:

<<>>=
head(WeightLossIncentive,4)
t.test(~WeightLoss, groups=Group, data=WeightLossIncentive,
       var.equal=TRUE)
@

Note the ``var.equal=TRUE" input argument.  With this input, R will carry out the t-test assuming equal variance between the two categories.  If we want to avoid making this assumption, there is a modified version of the two-sample t-test called the Welch two-sample t-test, which does not assume equal variances.  If we omit the ``var.equal" input, or set it to ``var.equal=FALSE", then R will do a Welch two-sample t-test for us. (We will not cover in this course how to do a Welch test by hand).

We might be able to judge informally whether the equal-variance assumption is valid by looking at the distributions of our variable of interest grouped by category, and computing the sample standard deviations by groups:
<<>>=
densityplot(~WeightLoss, groups=Group, data=WeightLossIncentive)
sd(~WeightLoss|Group, data=WeightLossIncentive, na.rm=TRUE)
@

We don't have an obvious indication of unequal variances. (This assumption can also be tested statistically, although we won't learn how in this class, and it's generally not recommended to do such a test prior to running a t-test).  If we wanted to do the t-test without the equal variance assumption, in R, we would use:
<<>>=
t.test(~WeightLoss, groups=Group, data=WeightLossIncentive)
@

\emph{In practice, there is no real disadvantage to simply using Welch's test all the time.}  So in general, if you are doing a 2-sample unpaired test, you should always use \texttt{var.equal=FALSE} (or omit the input \texttt{var.equal}, and it will default to FALSE).


\subsection{Testing Model Coefficients}

We can also use t-tests to test the null hypothesis that there is no relationship between the predictor and explanatory variables in a regression model.  If we can't reject that hypothesis, then we have evidence that there \emph{is} really some relationship, and that the response can be predicted based upon the explanatory variable.



\begin{example}
Suppose you suspect that drag force should be proportional to the square of velocity.  Let's see
if that is consistent with the data collected by some physics students.  
In this experiment, the students rigged up neutrally buoyant balloon and then loaded it 
with different amounts of weight and dropped it until and recorded its terminal velocity.
At that point the force due to gravity (determined by the mass loaded to the balloon) 
is equal to the drag force (because there is no acceleration).

We'll fit a power law model
\[
\variable{force.drag} = A \cdot \variable{velocity}^a
\]
and test the hypothesis that $a=2$.  We can fit this model using a log-log transformation:
\[
\log(\variable{force.drag}) = \log(A)  +  a \log( \variable{velocity} )
\]
So $a=\beta_1$ in our usual linear model notation.

<<drag-again>>=
drag.model <- lm(log(force.drag) ~ log(velocity), data=drag)
xyplot(log(force.drag) ~ log(velocity), data=drag)
@
The fit is not perfect, and in fact suggests a systematic problem with the way
these data were collected:%
\footnote{There is some evidence in this data that some of the observations did not reach
critical velocity.  
It would be good to refit this data with those observations removed from the data.
See Exercise~\ref{prob:drag-refit}.
}
<<>>=
xyplot(resid(drag.model) ~ fitted(drag.model))
qqmath(~ resid(drag.model))
@

<<>>=
summary(drag.model)
@
None of the p-values produced in this output is what we want.  
They are testing the hypotheses that $\beta_0=$ and that $\beta_1 = 0$.
$\beta_1 = 0$.  
But we can easily calculate the p-value we want since we have the standard error
and degrees of freedom.
<<tidy=FALSE>>=
beta1.hat <- 2.051
SE <- 0.05366
t <-  ( beta1.hat - 2 ) / SE; t
2 * pt( - abs(t), df= 40 )
@
With this large a p-value, we cannot reject the null hypothesis that $a=2$.
\textbf{A large p-value does not prove that $a=2$, 
but it does say that our data are consistent with that value.}  
Of course, our data may be consistent with many other values of $a$ as well.
\end{example}

\begin{example}
<<>>=
data(PorschePrice)
head(PorschePrice)
porsche.model <- lm( Price ~ Mileage, data=PorschePrice )
summary(porsche.model)
xyplot( resid(porsche.model) ~ fitted(porsche.model), type=c('p','smooth'))
qqmath( ~ resid(porsche.model) )
@
The model looks reasonable.  What are the two hypotheses being tested?
\begin{enumerate}
	\item $H_0: \beta_0 = 0$.

		Often this is not an interesting test because often we are not so interested in the intercept
		$\beta_0$, and especially not in whether it is 0.  In this case, the intercept might
		be interesting because it tells us the price of a Porsche with no miles.  On the other hand,
		we might not expect a used car, even one with very few miles to fit the same pattern as 
		a new car.  There is probably a loss in value that occurs as soon as a car is purchased.
		
		In any case, it is clear that the intercept will not be 0; we don't need a hypothesis test
		to tell us that.  Indeed, the evidence is incredibly strong.

		A confidence interval for the intercept is more interesting since it gives a sort 
		of ``starting price'' for used Porches.
<<>>=
confint(porsche.model)
@


	\item
		$H_0: \beta_1 = 0$.

		There is strong evidence against this hypothesis as well.  This is also not surprising.
		If $\beta_1 = 0$, that would mean that the price of the cars does not depend on the mileage.

		A test of $\beta_1=0$ in a regression model is often used for \emph{model selection} -- if we can not reject the null hypothesis of no relationship between the predictor and the response, then the predictor is not really a useful predictor at all. We might want to use (or \emph{select}) a model without that predictor, instead.

	\item
		$H_0: \beta_1 = \beta_{10}$.

		Although the output above doesn't do all of the work for us, we can test other
		hypotheses as well.  (The notation above is a bit tricky, $\beta_{10}$ should be read
		``$\beta_1$ null" -- it is a hypothesized value for $\beta_1$.)
		
		For example, let's test $\beta_1 = -1$.  That the hypothesis that the value 
		drops one dollar per mile driven.
    
    While this example is interesting as an exercise, it is quite rare to have a sensible hypothesized value for a regression slope parameter that we want to test.  It is much more common to ask, as we did above, ``is there a pattern here indicating that the predictor is a useful predictor of the response"?

<<tidy=FALSE>>=
t <- (-0.5894 - (-1) ) / 0.566; t
2 * pt( - abs(t), df=28 )
@
The small p-value would lead us to reject this value for $\beta_1$.
\end{enumerate}
\end{example}

\section{Connection to Confidence Intervals}
\label{sec:CI-HT-duality}

There is a natural duality between t-based hypothesis tests and confidence intervals.
Since the p-value is computed using tail probabilities of the t-distribution and confidence
level describes the central probability, the p-value will be below 0.05 exactly when the 
hypothesized value is not contained in the 95\% confidence interval.  (Similar statements can
be made for other confidence levels.)
%So a confidence interval generally conveys more information than a p-value in these situations.

\begin{example}
	In the preceding example we rejected the null hypothesis that $\beta_1 = -1$.
In fact, we will reject (at the $\alpha=0.05$ level) any hypothesized value not contained 
in the 95\% confidence interval.
<<>>=
t <- (-0.5894 - (- .75)) / 0.566; t
2 * pt( - abs(t), df=28 )
@
But we won't reject values inside the confidence interval.
<<>>=
t <- (-0.5894 - (- .70)) / 0.566; t
2 * pt( - abs(t), df=28 )
@
\end{example}


\begin{example}
	The output below illustrates this duality.
<<>>=
drag.model
confint(drag.model)
@
Since the confidence interval for $p$ (i.e., for $\beta_1$) includes 2, 
2 is a plausible value for the power (i.e., consistent with our data).
A p-value larger than 0.05 says the same thing at the same level of confidence.  
\end{example}

\begin{problem}
	An experiment was conducted to see if the number of clicks 
	on a Geiger counter in a 7.5 minute interval is related to
	the distance (in m) between a radioactive source and the detection 
	device according to an inverse square law:
	\[
	\mbox{clicks} =	A + \frac{k}{\mbox{distance}^{2}}
	\]
	Answer the questions below using the following output
<<include=FALSE>>=
Geiger <-  data.frame(
  distance = c( .20, .25, .30, .35, .40, .45, .50, .60, .75, 1.0),
  clicks = c(901, 652, 443, 339, 283, 281, 240, 220, 180, 154)
)
@
<<fig.height=3.5>>=
model <- lm( clicks ~ I(1/(distance^2)), data=Geiger)
summary(model)
plot(model, w=1:2)
@
\begin{enumerate}
	\item
		Are there are any reasons (from what you can tell in the output
		above) to be concerned about using this model?
	\item
		What does $A$ tell us about this situation?  
		What would it mean if $A$ were 0?  What if $A \neq 0$?
\item
	What is the estimate for $A$?  Express this as an estimate $\pm$ uncertainty
	using our rules for numbers of digits.
\item
	What is the p-value for the test of the null hypothesis that $A=0$?
	What conclusion do we draw from this?
\item
	What does $k$ tell us about this situation?  In what situations would $k$
	be larger or smaller?  What would it mean if $k$ were 0?

	\item
		Express the estimate for $k$ as an estimate $\pm$ uncertainty.
\item
	What is the p-value for the test of the null hypothesis that $k=0$?
	What conclusion do we draw from this?
\item
A standard radioactive substance has a value of $k=29.812$.  Might that be the
substance we are using here?  Conduct an appropriate hypothesis test to answer
this question.  Carefully show all four steps.
\end{enumerate}
\end{problem}

\begin{solution}
	\begin{enumerate}
			\item
				The normal-quantile plot looks pretty good for a sample of this size.
				The residual plot is not as ``noisy'' as we would like (the first few residuals
				cluster above zero, then next few below.  Ideally we would like to have a larger 
				data set to see whether this pattern persists or whether things "fill-in" with more
				data.
			\item
				$A$ is a measure of background radiation levels, it is the amount of clicks 
				we would get if our test substance were ``at infinity'', i.e., so far away (or 
				beyond some shielding) that it does not affect the Geiger counter.
			\item
				$114 \pm 11$
			\item
				\Sexpr{5.5E-06}.  This gives strong evidence that there some background radiation
				being measured by the Geiger counter.
			\item
				$k$ measures the rate at which our test substance is emitting radioactive 
				particles.  If $k$ is 0, then our substance is not radioactive (or at least not
				being detected by the Geiger counter).
			\item
				$31.5  \pm 1.0$
			\item
				\Sexpr{1.1E-09}.  We have strong evidence that our substance is decaying and 
				contributing to the Geiger counter clicks.
			\item
				$H_0: k = 29.812$;  $H_a: k \neq 29.812$
<<ticy=FALSE>>=
t <- ( 31.5 - 29.812) / 1.0; t     # test statistic
2 * pt ( - abs(t), df=8 )          # p -value
@
				Conclusion.  With a p-value this large, we cannot reject the null hypothesis.
				Our data are consistent with the hypothesis that our test substance is the same
				as the standard substance.
	\end{enumerate}
\end{solution}


\begin{problem}
\label{prob:drag-refit}
	Redo the drag force analysis after removing observations that appear not to have reached
	terminal velocity.

	If you can describe the rows you want to remove logically, the \function{subset()} command
	works well for this.  You can also remove rows by row number.  For example, the following
	removes rows 1, 3, 5 and 7:
<<results='hide'>>=
drag[ -c(1,3,5,7), ] 
@
\end{problem}

\begin{solution}
Let's remove the fastest values at each setting.  Although they are the fastest, it appears 
that terminal velocity has not yet been reached.  At least, these points would fit the 
overall pattern better if the velocity were larger.
<<>>=
xyplot( force.drag ~ velocity, data=drag, 
  groups = (velocity < 3.9) & !(velocity > 1 & velocity < 1.5) )
drag2 <- subset(drag, (velocity < 3.9) & !(velocity > 1 & velocity < 1.5) )
@
<<fig.height=3.2>>=
drag2.model <- lm(log(force.drag) ~ log(velocity), data = drag2)
summary(drag2.model)
plot(drag2.model, w=1:2)
@
The model is still not as good as we might like, and it seams like the fit is different for the 
heavier objects than for the lighter ones.  This could be due to some flaw in the design of the 
experiment or because drag force actually behaves differently at low speeds vs. higher speeds.
Notice the data suggest an exponent on velocity that is just a tiny bit larger than 2:
<<>>=
confint(drag2.model)
@
So our data (for whichever reason, potentially still due to design issues) is not compatible 
with the hypothesis that this exponent should be 2.
\end{solution}

\begin{problem}
	Sixteen samples of a certain brand of hydrogenated vegetable oil were tested to determine 
	their melting point.  The mean melting point for the 16 samples was 94.32 degrees 
	and the standard deviation was 1.2 degrees.

	\begin{enumerate}
		\item
    Conduct a test of the hypothesis $\mu = 95$.  Follow the four step procedure.
\item
	Will 95 be inside or outside of a 95\% confidence interval for the melting point?
	How do you know?
	\end{enumerate}
\end{problem}

\begin{solution}
<<tidy=FALSE>>=
SE <- 1.2 / sqrt(16); SE
t <- (94.32 - 95) / SE; t
p_val <- 2 * pt( - abs(t), df=15 ); p_val    # p-value
@
	95 will be 
	\Sexpr{ if( p_val < 0.05 ) "outside" else "inside" } 
	the confidence interval because the p-value is 
	\Sexpr{ if( p_val < 0.05 ) "less than" else "greater than"} 0.05. 
	As a double check, here is the 95\% confidence interval.
<<>>=
t_star <- qt(.975, df=11); t_star
94.32 + c(-1,1) * t_star * SE
@
\end{solution}

\begin{problem}
	The Charpy V-notch impact test is the basis for many material toughness criteria.
	This test was applied to 42 specimens of a particular alloy at 110 degrees F.
	The mean amount of transverse lateral expansion was computed to be
	73.1 mils with a sample standard deviation of 5.9 mils.

	To be suitable for a particular application, the true amount of expansion must be less than 75 mils.
	The alloy will not be used unless their is strong evidence (a p-value below 0.01) that 
	this specification is met.

	\begin{enumerate}
		\item
			Use a p-value to decide whether this alloy may be used.
		\item
			Use a confidence interval to decide whether this alloy may be used.
		\item
			Are there advantages to one approach over the other?  If you had to present
			an argument to your boss, which approach would you use?
	\end{enumerate}
\end{problem}

\begin{solution}
	\begin{enumerate}
		\item
			In this situation it makes sense to do a one-sided test since we will reject
			the alloy only if the amount of expansion is to high, not if it is too low.
<<tidy=FALSE>>=
SE <- 5.9/sqrt(42); SE
t <- (73.1 - 75) / SE; t
pt(t, df=41)
@
		\item
			Corresponding to a 1-sided test with a significance level of $\alpha = 0.01$,
			we could make a 98\% confidence interval (1\% in each tail).  The upper end of this
			would be the same as for a 1-sided 99\% confidence interval
<<>>=
t_star <- qt(.99, df=41)
73.1 + c(-1,1) * t_star * SE
@
		\item
			If all we are interested in is a decision, the two methods are equivalent.
			I confidence interval might be easier to explain to someone less familiar with
			statistics.
	\end{enumerate}

\end{solution}

\begin{problem}
	Data from a 1993 study to see how well lichens serve as an indicator for air pollution are in the 
	\dataframe{ex12.20} data set in the \pkg{Devore6} package. 
	In that paper, a simple linear model was fit to see how the wet deposition of $NO^{-}_3$ 
	related to the percentage dry weight of lichen.
	Using this model, 
	\begin{enumerate}
		\item
			Test the hypothesis that the slope of the linear relationship is 1.
		\item
			Compute a 90\% confidence interval for the slope of the linear relationship.
	\end{enumerate}
<<>>=
require(Devore6)
summary( lm( LichenN ~ deposition, data=ex12.20) )
@
\end{problem}
\begin{solution}
<<tidy=FALSE>>=
t <- (0.967 - 1) / 0.183
2 * pt( -abs(t), df=11 )           # p-value
@
With such a large p-value, we do not reject the null hypothesis.  Our data are consistent 
with the hypothesis that the slope is 1.
<<>>=
t_star <- qt(.95, df=11); t_star
.967 + c(-1,1) * t_star * 0.183    # 90% CI
@
Notice that this interval includes 1.  We knew it would because 1 would not be rejected at the 
$\alpha = 0.10$ level.
<<>>=
t_star <- qt(.975, df=11)
95 + c(-1,1) * t_star * SE
@
\end{solution}

\newpage
\section*{Exercises}
\shipoutProblems


\ifsolutions
%\ifsolutionslocal
\newpage
\section*{Solutions}
\shipoutSolutions
%\fi
\fi
