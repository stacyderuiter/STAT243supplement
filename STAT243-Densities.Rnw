\Sexpr{set_parent('Math241-S14.Rnw')}

\chapter{Densities}

\section{Density histograms, density plots, density functions}
\myindex{density|defidx}%
A histogram is a simple picture describing the ``density" of data. 
Histogram bars are tall in regions where there is more data -- i.e., where the data are 
more ``dense''.  
<<histograms-count-density>>=
require(alr3)
histogram( ~ Duration, data=oldfaith )
histogram( ~ Duration, data=oldfaith, type="count" )
@

The density scale is the same scale that is used by \function{densityplot()}, and it
is the default scale for histograms created using \function{histogram()} when
the \pkg{mosaic} package is loaded.
<<histogram-density>>=
require(alr3)
histogram( ~ Duration, data=oldfaith, width=20, center=110 )
densityplot( ~ Duration, data=oldfaith)
@

The density scale is chosen so that the area of each rectangular bar (width times height)
is equal to the proportion of the data set represented by the rectangle. 

\begin{example}
	\question Use the histogram of Old Faithful eruption times to estimate
	the proportion of eruptions that last between 100 and 120 seconds.

	\answer
In our histogram of Old Faithful eruption durations, the bar corresponding to the 
bin from 100--120 appears to have a height of about 0.09.  That gives an area of 
0.18 and indicates that approximately 18\% of the eruptions last between 100 and 120
seconds.
<<>>=
tally( ~ ( 100 < Duration & Duration <= 120), data=oldfaith, format="prop" )
@
\end{example}

The key idea behind the density scale can be expressed as 

\begin{center}
	Probability $=$ area
\end{center}
This association of area with probability means that
the total area of all the bars will always be equal to 1 if we use the density
scale.  

It also provides us with a way to describe a distribution with a mathematical function.


\begin{boxedText}
  \label{def:pdf}%
  \myindex{probability density function|defidx}%
  \myindex{density function|defidx}%
  \myindex{pdf|see{probability density function}}%
  Let $f$ be a function such that %: \reals \to \reals$ be a function such that
  \begin{enumerate}
	  \item $f(x) \ge 0$ for all $x$,
	  \item $\displaystyle \int_{-\infty}^{\infty} f(x) \; dx = 1$.
   \end{enumerate}
   Then $f$ is called a \textbf{density function} (or probability density function,
   abbreviated pdf) and describes a continuous random variable $X$ such that
   \[
   \Prob(a\le X \le b) = \int_a^b f(x) \; dx \;.
   \]
\end{boxedText}



\begin{example}
\label{ex:triangle}%
	Let $f$ be defined by
	\[
	f(x) = \begin{cases}  
		1 - |x| & x \in [-1,1] \\
		0 & \mbox{otherwise}\\
	\end{cases}
	\]
Show that $f$ is a density function.  Let $X$ be the associated random 
variable, and compute the following probabilities:
\begin{enumerate}
	\item
		$\Prob(X\le0)$
	\item
		$\Prob(X \le 1)$
	\item
		$\Prob(X \le \frac12)$
	\item
		$\Prob(-\frac12 X \le \frac12)$
\end{enumerate}

\answer
While we could set up integrals for these, it is easier to solve them using geometry.%
\footnote{\R\ cleverly turns TRUE and FALSE into 1 and 0 when you use them in arithmetic
expressions.  The definition of \code{f()} makes use of this conversion to simplify specifying
the cases.}
<<>>=
f <- makeFun( (1 - abs(x)) * (abs(x) <= 1) ~ x )
plotFun( f(x) ~ x, x.lim=c(-1.5, 1.5) )
@

The entire area under the curve can be found as the area of a triangle with base 2 and height 1.
\[
\int_{-\infty}^{\infty} f(x) \; dx 
=
\int_{-1}^1 f(x) \; dx
=
\frac 12 \cdot 2 \cdot 1 = 1
\]
This implies that $f$ is a density function.

\begin{enumerate}
	\item
$\Prob(X \le 1) 
=
\int_{-\infty}^{1} f(x) \; dx 
=
\int_{-1}^{1} f(x) \; dx = 1
$
\item
		$\Prob(X \le \frac12)
		=
\int_{-\infty}^{1/2} f(x) \; dx 
=
\int_{-1}^{1/2} f(x) \; dx = 1 - \frac12 \cdot \frac12 \cdot \frac12 = \frac78
$

\item
	$\Prob( -\frac12 \le X \le \frac 12 ) 
	=
	\int_{-1/2}^{1/2} f(x) \; dx 
	=
	1 - \frac28 = \frac34
	$
\end{enumerate}

We can also let \R\ do (numerical) integration for us.  There are two ways to do this.
The first method uses the \function{integrate()} function.
<<>>=
integrate( f, -Inf, 1 )
# this will be more accurate since we aren't asking R to approximate
# something that we already know is exactly 0
integrate( f, -1, 1)
integrate( f, -.5, .5 )
# if you just want the value without the text saying how accurate the approximation is
integrate( f, -.5, .5 )$value
@

An alternative approach uses \function{antiD()} from the \pkg{mosaic} package.
<<tidy=FALSE>>=
F <- antiD( f(x) ~ x)
F(1) - F(-1)        # total probability -- better be 1
F(.5) - F(-1)       # P( -1 <= X <= 0.5 )
F(.5) - F(-.5)      # P( -.5 <= X <= .5 )
@
\end{example}

\myindex{cumulative distribution function|defidx}%
\myindex{cdf|see{cumulative distribution function}}%
If we help \R\ choose the anti-derivative, we get a useful function called 
the \term{cumulative distribution function}, abbreviated cdf.


\begin{boxedText}
	If $X$ is a random variable, then the \term{cumulative distribution function} (cdf) for 
	$X$, often denoted $F_X$, is the function defined by
	\[
	F_X(x)  = \Prob(X \le x)
	\]
	That is, the output of the cdf reports the probability of being below a particular value.
	The derivative of the cdf is the pdf.
\end{boxedText}

\begin{example}
Continuing with our previous example, if we choose -1 as our lower endpoint,
then the anti-derivative will be the cdf.
<<tidy=FALSE>>=
F <- antiD( f(x) ~ x, lower.bound = -1)   # We can use -1 instead of -Inf here.
F(-1)               # this should be 0 since we chose -1 as the lower bound.
F(1)                # P(X <= 1); should be 1
F(.5)               # P(X <= 0.5)
F(.5) - F(-.5)      # P( -0.5 <= X <= 0.5 )
@
\end{example}

\section{Working with Probability Density Funcitons}

We have already seen that we can use a pdf $f$ to calculate probabilities via integration, 
and that there is a special anti-derivative of $f$ called the cdf such that the cdf $F$
satisfies
\[
F(x) = \Prob(X \le x)
\]
This function can also be used to compute probabilities, since
\[
\Prob(a \le X \le b) = \int_a^b f(x) \; dx = F(b) - F(a)
\]
Indeed, once we learn how to get the cdf function in \R\ this will 
be our primary way to calculate probabilities in applications.

\subsection{Kernels}
\myindex{kernel|defidx}%
The \term{kernel} of a random variable is a function that is a 
constant multiple of the pdf.  The reason that these are interesting
is that any kernel can be converted into a pdf by dividing by the appropriate constant.
In particular, if 
\[ 
\int_{-\infty}^{\infty} k(x) \; dx = A \;, 
\]
then $k$ is the kernel of a random variable with pdf
\[ 
f(x) = \frac{k(x)}{A} \;.
\]

\begin{example}
\label{ex:kernel}%
	\question
	The kernel of a random variable is given by 
	\[
	k(x) = x^2 \; \boolval{ x \in [0,2] } \; .
	\]
	Determine the pdf.

	\answer First we determine the value of the integral
	\[
\int_{-\infty}^{\infty} k(x) \; dx \;. 
\]
<<>>=
k <- makeFun( x^2 * ( 0 <= x & x <= 2) ~ x )
plotFun(k(x) ~ x, xlim=c(-1,3))
integrate( k, 0, 2)
K <- antiD(k(x) ~ x, lower.bound=0)
K(2)
@
Since the total area is $8/3$, if $\frac{k(x)}{8/3}$ is the pdf.
\end{example}

\subsection{The mean of a continuous random variable}

The definition for the mean of a continuous random variable will be motivated 
by the calculation of a mean of some data.
\begin{example}
	\question
  Suppose a student has taken $10$ courses
  and received $5$ A's, $4$ B's, and $1$ C.  Using the traditional numerical scale where 
  an A is worth $4$, a B is worth $3$, and a C is worth $2$, what is this student's 
  GPA (grade point average)?

  \answer
  The first thing to notice is that $\frac{4 + 3 + 2}{3} = 3$ is \emph{not} correct.
  We cannot simply add up the values and divide by the number of values.  Clearly
  this student should have a GPA that is higher than $3.0$, since there were more A's than
  C's.

  Consider now a correct way to do this calculation:
  \begin{align*}
	\mbox{GPA} &= \frac{4 + 4 + 4 + 4 + 4 + 3 + 3 + 3 + 3 + 2}{10}
	\\[2mm]
	& = \frac{5\cdot 4 + 4\cdot 3 + 1 \cdot 2}{10} \\[1mm]
	& = \frac{5}{10} \cdot 4 + \frac{4}{10}\cdot 3 + \frac{1}{10} \cdot 2 \\
	& = 4 \cdot \frac{5}{10} + 3 \cdot \frac{4}{10} + 2 \cdot \frac{1}{10} \\
	& =  3.4 \;.
\end{align*}
\end{example}
The key idea here is that the mean is a \textbf{sum of values times probabilities}.  
\[
\mbox{mean} = \sum \mbox{value} \cdot \mbox{probability}
\]
When working with a continuous random variable, 
we replace the sum with an integral and replace the 
probabilities with our density function to get the following definition:

\[
\E(X) = \mu_X = \int_{-\infty}^{\infty} x f(x) \; dx
\]

If you recall doing center of mass problems you may recognize this integral as 
the first moment.
(For pdfs, we don't need to divide by the ``mass'' because the total ``mass'' is the area under the curve, which will always be 1 for a random variable).

Note: It is possible that the integral used to define the mean will fail to converge.
In that case, we say that the random variable has no mean or that the mean fails to exist.%
\footnote{Actually, we will require that $\int_{\infty}^{\infty} |x| f(x) \; dx$ converges.  
If this integral fails to converge, we will also say that the distribution has no mean.}

\begin{example}
	\question Compute the mean of our triangle distribution from Example~\ref{ex:triangle}.

	\answer We simply compute the integral from the definition.

	\begin{align*}
	\E(X) & = \int_{-1}^{1} x f(x) \; dx 
	\\
	& = \int_{-1}^{0} x (x-1) \; dx + \int_{0}^1 x ( 1-x ) \; dx 
	\\
	& = \int_{-1}^{0} x^2-x) \; dx + \int_{0}^1 x-x^2 ) \; dx  
	\\
	& = \left.\frac{x^3}{3} - \frac{x^2}{2} \right|_{-1}^0
	+ \left. \frac{x^2}{2} - \frac{x^3}{3} \right|_{0}^1
	\\
	& = \frac13 - \frac12 + \frac12 - \frac13 = 0
	\end{align*}

	This isn't surprising, by symmetry we would expect this result.

	We could also calculate this numerically in \R:
<<>>=
f <- makeFun( (1 - abs(x)) * (abs(x) <= 1) ~ x)
xf <- makeFun( x * f(x) ~ x )
integrate(xf, -1, 1)
F <- antiD( x * f(x) ~ x, lower.bound=-1)
F(-1)  # should be 0
F(1)
@
\end{example}

\subsection{The variance of a continuous random variable}

Arguing similarly, we can compute the variance of a continuous random
variable using
\[
\Var(X) = \sigma^2_X = \int_{-\infty}^{\infty} (x-\mu_X)^2 f(x) \; dx
\]

Note: It is possible that the integral used to define the variance will fail to converge.
In that case, we say that the random variable has no variance or that the variance 
fails to exist.%
\footnote{Actually, we will require that $\int_{\infty}^{\infty} |x|^2 f(x) \; dx$ converges.  
If this integral fails to converge, we will say that the distribution has no variance.}

\begin{example}
	\question Compute the variance of the triangle random variable
	from the Example~\ref{ex:triangle}.
	
\answer
<<>>=
f <- makeFun( (1 - abs(x)) * (abs(x) <= 1) ~ x)
xxf <- makeFun( (x-0)^2 * f(x) ~ x )
integrate(xxf, -1, 1)
G <- antiD( (x-0)^2 * f(x) ~ x)
G(1) - G(-1)
@
\end{example}

Some simple algebraic manipulations of the integral above shows that
\begin{align}
\Var(X) &= \E(X^2) - \E(X)^2
\label{eqn:varshortcut}
\end{align}

\begin{problem} 
Let $f(x) = 5/4 - x^3$  on $[0,1]$.  

\begin{enumerate}
	\item
		Show that $f$ is a pdf.
	\item
			Calculate $\Prob(X \le \frac12)$.
	\item
			Calculate $\Prob(X \ge \frac12)$.
	\item
			Calculate $\Prob(X = \frac12)$.
\end{enumerate}

\end{problem}

\begin{solution}
<<>>=
f <- makeFun( (5/4 - x^3) * ( abs(x-.5) <= .5 ) ~ x )
plotFun(f(x) ~ x, x.lim=c(-1,2))  # quick plot to make sure things look correct.
F <- antiD(f(x) ~x)  
# part a:  f(x) >=0, so we just need to check that the total area is 1
F(1) - F(0) == 1  
F(1/2) - F(0)     # part b
F(1) - F(1/2)     # part c
F(1/2) - F(1/2)   # part d
@
\end{solution}



\begin{example}
	\question Compute the mean and variance of the random variable with pdf given by
	
	\[ g(x) = \frac{3x^2}{8} \boolval{x \in [0,2]} \;. \]
	This is the pdf computed in Example~\ref{ex:kernel}.
	
\answer
<<>>=
g <- makeFun( (3 * x^2/8 ) * (0 <= x & x <= 2) ~ x )
m <- antiD( x * g(x) ~ x, lower.bound=0)(2)  # all in one step instead of defining F or G
m
v <- antiD( (x - m)^2 * g(x) ~ x, m=m, lower.bound=0)(2)
v
# here's the alternate computation
antiD( x^2 * g(x) ~ x, lower.bound=0)(2) - m^2
@
\end{example}

As with data, the standard deviation is the square root of the variance.

\subsection{Quantiles}

Quantiles solve equations of the form
\[
\int_{-\infty}^x f(t) \; dt = F(x) = \Prob(X \le x) = q
\]
where $q$ is known and $x$ is unknown.  So the 50th percentile (which is the 0.5-quantile
or the median) is the number such that 
\[
\Prob(X \le x) = 0.5 \;.
\]

\begin{example}
	\question What is the 25th percentile of the triangle distribution in
	Example~\ref{ex:triangle}?

	\answer
	We need to solve for $x$ in the following equation:
	\[
	0.25 = \Prob( X \le x)  \; .
	\]
	We can do this by working out the integral involved:
	\begin{align*}
	0.25 &= \int_{-1}^{x} 1 - |t| \; dt 
	\\
	 &= \int_{-1}^{x} 1 + t \; dt 
	 \\
	 &=  \left. t + t^2/2 \right|_{-1}^{x}
	 \\
	 &=   x + x^2/2 + 1 - 1^2/2  
	 \\
	 &=  x + x^2/2 + 1/2
	 \\
	 0 & =  x^2/2 + x + 1/4
	 \\
	 0 & =  2x^2 + 4x + 1
\end{align*}
So by the quadratic formula, $x = \frac12 \sqrt{2} - 1 = \Sexpr{1/2*sqrt(2) - 1}$.

We can check this by evaluating the cdf.
<<>>=
x <- 1/2*sqrt(2) - 1
F(x)
@

This could also be done geometrically by solving $\frac12 y^2 = \frac14$ and letting
$x = -1 + y$.
\end{example}


\section{Some Important Families of Distributions}

\myindex{parameter}%
For now, we will consider only distributions of continuous random variables (probability density functions).  We will leave set aside discrete random variables (probability mass function) until quite a bit later in the course.

A family of distributions is a collection of distributions that share some
common features.  Typically, these are described by giving a pdf that has
one or more \term{parameters}. A parameter is simply a number that describes
(a feature of) a distribution that distinguishes between members of the family.
In this section we describe briefly
some of the important distributions and how to work with them in \R.

\subsection{Triangle Distributions}
\myindex{triangle distribution|defidx}%
\myindex{triangular distribution|see{triangle distribution}}%
The example distribution in the previous section is usually referred to as a 
triangle distribution (or triangular distribution)
because of the shape of its pdf.  There are, of course,
many triangle distributions.  A triangle distribution is specified with three
numbers: $a$, the minimum; $b$, the maximum, and $c$, the location of the peak.
A triangle distribution is symmetric if the peak is halfway between the minimum
and maximum ($c = \frac{a+b}{2}$).

When $X$ is a random variable with a triangle distribution, we will write
$X \sim \Tri(a,b,c)$.  For many of the most common
distributions, \R\ has several functions that facilitate computation with
those distributions.  The triangle distributions are not in the base \R\ 
distribution, but they can be added by requiring the \pkg{triangle} package.

For each distribution, there are four functions in \R\ that always start
with a single letter followed by a name for the distribution.  In the case 
of the triangle distributions, these functions are 

\begin{center}
\begin{tabular}{ll}
	\hline
	Function & What it does \\
	\hline
	\texttt{dtriangle(x,a,b,c)} & Computes value of the pdf at \texttt{x}
	\\
	\texttt{ptriangle(q,a,b,c)}     & Computes value of the cdf at \texttt{x}, i.e., 
	$\Prob(X \le \texttt{q})$
	\\
	\texttt{qtriangle(p,a,b,c)}     & Computes quantiles, that is a value $q$ so that 
	$\Prob(X \le \texttt{q}) = \texttt{p}$
    \\
	\texttt{rtriangle(n,a,b,c)} & Randomly samples \texttt{n} values from the 
	$\Tri(\texttt{a},\texttt{b},\texttt{c})$ distribution
	\\
	\hline
\end{tabular}
\end{center}


\begin{example}
\label{ex:triangleInR}%
	\question Let $X \sim \Tri(0,4,1)$.  Use \R\ to answer the following questions.
	\begin{enumerate}
		\item Plot the pdf for $X$.
		\item What is $\Prob(X \le 1)$?
		\item What is $\Prob(X \le 2)$?
		\item What is the median of $X$?
		\item What is the mean of $X$?
	\end{enumerate}
	
\answer
The \function{plotDist} function in the \pkg{mosaic} package allows us to 
graph the pdf for any function \R\ knows how to work with in the standard way.
For example, here is a plot of the pdf of a $\Tri(0, 4, 1)$-distribution.
<<>>=
require(triangle)      # a package that knows about triangle distributions
plotDist("triangle", params=list(a=0, b=4, c=1))
@
Here is the \R\ code to answer the remaining questions.
<<>>=
ptriangle(1, 0, 4, 1)   # P(X <= 4); notice that his is NOT 1/2
ptriangle(2, 0, 4, 1)   # P(X <= 4); also NOT 1/2
qtriangle(0.5, 0, 4, 1) # median is the 0.5-quantile
T <- antiD( x * dtriangle(x, 0,4,1) ~ x, lower.bound=0)
T(4)                    # mean of X
integrate( makeFun( x * dtriangle(x, 0,4,1) ~ x) , 0, 4)
@
\end{example}



\begin{problem}
	Repeat parts (2) -- (4) of Example~\ref{ex:triangleInR} using geometry 
	rather than \R.

\end{problem}

\begin{solution}
	\begin{itemize}
		\item
			$\Prob(X \le 1) = \frac 12 \cdot 1 \cdot \frac12 = \frac 14$
		\item
			$\Prob(X \le 2) = 1 - \Prob(X \ge 2) =  1 - \frac12 \cdot 2 \cdot \frac13 
			= 1 - \frac 13 = \frac23$.
		\item
			The median $m$ is a number between 1 and 2 and satisfies
			$\frac 12 = \Prob(X \ge m) = \frac12 (4-m) \frac{4-m}{6}$.  
			Solving for $m$ we get $m = 4 - \sqrt{6} = \Sexpr{4 - sqrt(6)}$.
	\end{itemize}
\end{solution}

\begin{problem}
	Let $\displaystyle k(x) = (1 - x^2) \cdot \boolval{ x \in [-1,1] } = 
	\begin{cases}
		1-x^2 & x \in [-1,1] \\ 
		0 & \mbox{otherwise} 
	\end{cases}$ be the kernel of a continuous distribution.
	\begin{enumerate}
		\item
			Determine the pdf for this distribution.
		\item
			Compute the mean and variance for this distribution
	\end{enumerate}
\end{problem}

\begin{solution}
	The integrals are easy enough to do by hand, but here is the \R\ code 
	to compute them.
<<tidy=FALSE>>=
k <- makeFun( 1 - x^2 ~ x )
K <- antiD( k(x) ~ x )
area <- K(1) - K(-1); area 
f <- makeFun( (1 - x^2)/A ~ x, A=area )
F <- antiD(f(x) ~ x)
F(1) - F(-1)    # this should be 1 if we have done things right
G <- antiD( x * f(x) ~ x )
H <- antiD( x^2 * f(x) ~ x )
m <- G(1) - G(-1); m               # E(X)
H(1) - H(-1)                       # E(X^2)
H(1) - H(-1) - m^2                 # Var(X)
@
\end{solution}

\begin{problem}
	Let $Y \sim \Tri(0,10,4)$.  Compute $\E(Y)$ and the median of $Y$.
\end{problem}
\begin{solution}
<<>>=
# mean:
m <- antiD( x * dtriangle(x,0,10,4) ~ x, lower.bound=0)(10)
m
# variance:
antiD( x^2 * dtriangle(x,0,10,4) ~ x, lower.bound=0)(10) - m^2
# median
qtriangle( 0.5, 0, 10, 4 )
@
\end{solution}

\subsection{Uniform Distributions}
\myindex{uniform distribution|defidx}%

A uniform distribution is a described by a constant function over some interval.  Its 
shape is a rectangle.  This makes it particularly easy to calculate probabilities 
for a uniform distribution.  Despite its simplicity, the family of uniform distributions
has many applications.

We will let $X \sim \Unif(a,b)$ denote that $X$ is a uniform random variable on 
the interval from $a$ to $b$. 
In \R\, the parameters $a$ and $b$ are given more meaningful names: \code{min} and \code{max}.
We can use the following code to graph the $\Unif(1,4)$ distribution.

<<>>=
plotDist("unif", params=list(min=1, max=4), xlim=c(0,5))  # using parameter names
plotDist("unif", params=list(1, 4), xlim=c(0,5))          # without names
@
Notice that the width of the non-zero portion of the pdf is 3, so the height must be $1/3$.

Probabilities involving uniform distributions are easily calculated using simple geometry,
but \R\ also provides several functions for working with uniform probability distributions.

\begin{center}
\begin{tabular}{ll}
	\hline
	Function & What it does \\
	\hline
	\texttt{dunif(x,min,max)} & Computes value of the pdf at \texttt{x}
	\\
	\texttt{punif(x,min,max)} & Computes value of the cdf at \texttt{x}, i.e., 
	$\Prob(X \le \texttt{x})$
	\\
	\texttt{qunif(p,min,max)} & Computes quantiles, that is a value of $x$ so that 
								$\Prob(X \le x) = \texttt{q}$
    \\
	\texttt{runif(n,min,max)} & Randomly samples \texttt{n} values from the 
								$\Unif(\texttt{min}, \texttt{max})$ distribution
	\\
	\hline
\end{tabular}
\end{center}

Notice the pattern to these names.  They start with the same letters as 
the functions for the triangle distributions, but replace \texttt{triangle}
with \texttt{unif}.  \emph{There are similar functions for all of the distributions
in this chapter.}

\begin{example}
	\question
	Let $X \sim \Unif(1,4)$.  Use \R\ to calculate the following values and 
	check the values using geometry:
	\begin{enumerate}
		\item $\Prob(X \le 2)$
		\item the 80th percentile of the distribution
	\end{enumerate}

	\answer
<<>>=
punif(2,1,4)   # P(X <= 2 )
(2-1) * 1/3    # P(X <= 2 ) using area
qunif(.8, 1,4) # 80th percentile
@
We could also get the 80th percentile by solving the equation $\frac13 (x-1) = 0.8$
From this we get $\frac{x}{3} = 0.8 + 1/3$, so $x = 3 ( 0.8 + 1/3) = 2.4 + 1 = 3.4$.
\end{example}

\begin{problem}
	Let $W \sim \Unif(0,10)$.  Compute $\E(W)$ and $\Var(W)$.
\end{problem}

\begin{solution}
<<>>=
# mean:
m <- antiD( x * dunif(x,0,10) ~ x, lower.bound=0)(10)
m
# variance:
antiD( x^2 * dunif(x,0,10) ~ x, lower.bound=0)(10) - m^2
@
\end{solution}


\subsection{Exponential Distributions}
\myindex{exponential distribution|defidx}%

The exponential distributions are useful for modeling the time until some ``event'' 
occurs.  The model is based on the assumptions that 
\begin{enumerate}
	\item
		The probability of an event occurring in any small interval of time 
		is proportional to the length of the time interval.  The constant
		of proportionality is the rate parameter, usually denoted by $\lambda$.
	\item
		The probabilities of events occurring in two small non-overlapping intervals
		are independent.
\end{enumerate}

\begin{examples}
	Here are some situations that might be well modeled by an exponential distribution:
	\begin{enumerate}
\item
	The time until the next radioactive decay event is detected on a Geiger counter 

\item
	The time until a space satellite is struck by a meteor (or some other space junk) 
	and disabled.

	The model would be good if (over some time span of interest) the chances of getting
	struck are always the same.  It would not be such a good model if the satellite moves
	through time periods of relatively higher and then relatively lower chances of being
	struck (perhaps because we pass through regions of more or less space debris at 
	different times of the year.)

\item
	The lifetime of some manufactured device.

	This is a pretty simple model (we'll learn better ones later) and most often is \emph{too} simple to describe the interesting features of the lifetime of a device.  In this model, failure is due to some external thing ``happening to" the device; the 
	device itself does not wear (or improve) over time.
	\end{enumerate}
\end{examples}

We will let $X \sim \Exp(\lambda)$ denote that $X$ has an exponential distribution
with rate parameter $\lambda$.   
The kernel of such a distribution is 
\[
k(x; \lambda) = e^{-\lambda x} \; \boolval{x \ge 0}
\]
Notice that the function describing this distribution is defined only for x-values that are real numbers greater than or equal to zero (in mathematical notation, the interval $[0, \infty)$.) This interval is sometimes called the ``support" of the distribution.  When using probability distributions to model data, it's important to think about whether the support of the distribution matches well with the range of possible values observed in the data.

The exponential distribution function is a pretty easy function to integrate, but \R\ provides the now familiar
functions to make things even easier.

\begin{center}
\begin{tabular}{ll}
	\hline
	Function & What it does \\
	\hline
	\texttt{dexp(x,rate)} & Computes value of the pdf at \texttt{x}
	\\
	\texttt{pexp(q,rate)}     & Computes value of the cdf at \texttt{x}, i.e., 
	$\Prob(X \le \texttt{q})$
	\\
	\texttt{qexp(p,rate)}     & Computes quantiles, that is a value $q$ so that 
	$\Prob(X \le \texttt{q}) = \texttt{p}$
    \\
	\texttt{rexp(n,rate)} & Randomly samples \texttt{n} values from the 
	$\Exp(\lambda)$ distribution
	\\
	\hline
\end{tabular}
\end{center}

\iffalse
\begin{boxedText}
	We will write $X \sim \Exp(\lambda)$ to indicate that the random variable $X$ has 
	an exponential distribution with rate parameter $\lambda$.
	\begin{center}
		\begin{tabular}{lll}
			\hline 
			parameter & $\lambda$ & \texttt{rate}
			\\
			kernel & $e^{-\lambda x}$
			\\
			pdf & $f(x; \lambda) = \lambda e^{-\lambda x} \boolval{x\ge 0}$ 
			& \texttt{dexp(x,rate)}
			\\
%			mean & $\frac{1}{\lambda}$
%			\\
%			variance & $\frac{1}{\lambda^2}$
%			\\ 
%			\hline
		\end{tabular}
	\end{center}
\end{boxedText}
\fi


<<plotDist-eponential>>=
plotDist("exp", params=list(rate=4))
@

\begin{problem}
	\begin{enumerate}
		\item Let $X \sim \Exp(4)$.  Use \R\ to compute $\E(X)$.
		\item Let $X \sim \Exp(10)$.  Use \R\ to compute $\E(X)$.
		\item Let $X \sim \Exp(1/5)$.  Use \R\ to compute $\E(X)$.
		\item What pattern do you notice.  Explain in terms of 
			the definition of the exponential distribution why this
			makes sense.
	\end{enumerate}
\end{problem}

\begin{solution}
<<>>=
antiD( x * dexp(x, 4) ~ x, lower.bound=0)(Inf)
antiD( x * dexp(x, 10) ~ x, lower.bound=0)(Inf)
antiD( x * dexp(x, 1/5) ~ x, lower.bound=0)(Inf)
@
	It appears that the mean of an $\Exp(\lambda)$-distribution is $1/\lambda$.  This
	makes sense.  If events have at a rate of $30$ per hour, we would expect to wait
	$1/30$ of an hour (on average) for the first event to happen.
\end{solution}


%\newpage
\subsection{Gamma and Weibull Distributions}
\myindex{Gamma distribution|defidx}%
\myindex{Weibull distribution|defidx}%

The Gamma and Weibull familities of distributions are generalizations of the exponential
distribution.  Each family has two parameters, a rate parameter as in 
the exponential distribution, and an additional parameter called the shape 
parameter (denoted by $\alpha$ below).  The reciprocal of the rate parameter
is called the scale parameter.  For the Gamma distribution, \R\ lets us use
either rate or scale (and the default is rate).  For the Weibull, we must use the 
scale.

\begin{center}
	\begin{tabular}{ll}
		\hline
		distribution & kernel
		\\
		\hline
		$\Gamm(\alpha, \lambda)$ & $ k(x) = x^{\alpha-1} e^{-\lambda x} \; \boolval{x \ge 0}$
		\\
		$\Weibull(\alpha, \lambda)$ & $ k(x) = x^{\alpha-1} e^{-\lambda x^{\alpha}} \; \boolval{x \ge 0}$
		\\
		\hline
	\end{tabular}
\end{center}
Both families of distributions are supported on the interval $[0, \infty)$.)
For the most part, we won't use these formulas in calculations, 
preferring to let \R\ do the work for us. However, notice that each
of these distributions has a pdf that allows for relatively simple integration.  For the 
Gamma distributions, we need to use integration by parts ($\alpha - 1$ times).  For 
the Weibull distributions we can use a substitution: $u = x^{\alpha}$.
In each case, when $\alpha=1$ we get an exponential distribution.

The now familiar functions are available for each of these distributions.
\begin{center}
\begin{tabular}{ll}
	\hline
	Function & What it does \\
	\hline
	\texttt{dgamma(x, shape, rate, scale=1/rate)} & Computes value of the pdf at \texttt{x}
	\\
	\texttt{pgamma(q, shape, rate, scale=1/rate)} 
		& Computes value of the cdf at \texttt{x}, i.e., 
	$\Prob(X \le \texttt{q})$
	\\
	\texttt{qgamma(p, shape, rate, scale=1/rate)} 
		& Computes quantiles, that is a value $q$ so that 
	$\Prob(X \le \texttt{q}) = \texttt{p}$
    \\
	\texttt{rgamma(n, shape, rate, scale=1/rate)} & Randomly samples \texttt{n} values from a
	Gamma distribution.
	\\
	\hline
	\texttt{dweibull(x, shape, scale=1/rate)} & Computes value of the pdf at \texttt{x}
	\\
	\texttt{pweibull(q, shape, scale)} 
		& Computes value of the cdf at \texttt{x}, i.e., 
	$\Prob(X \le \texttt{q})$
	\\
	\texttt{qweibull(p, shape, scale)} 
		& Computes quantiles, that is a value $q$ so that 
	$\Prob(X \le \texttt{q}) = \texttt{p}$
    \\
	\texttt{rweibull(n,shape,scale)} & Randomly samples \texttt{n} values from a
	Weibull distribution.
	\\
	\hline
\end{tabular}
\end{center}

Like the exponential distributions, these distributions are skewed and only take 
on positive values.  
These distributions arise in many applications, including as more general models 
for lifetime.  As the pictures below indicate, the shape and scale parameters
are aptly named.

<<>>=
plotDist("gamma", params=list(shape=2, rate=1), main="Gamma(2,1)")
plotDist("gamma", params=list(shape=5, rate=1), main="Gamma(5,1)")
plotDist("gamma", params=list(shape=2, scale=10), main="Gamma(5,10)")
plotDist("gamma", params=list(shape=5, scale=10), main="Gamma(5,10)")
@

<<>>=
plotDist("weibull", params=list(shape=2, scale=1),main="Weibull(2,1)")
plotDist("weibull", params=list(shape=5, scale=1),main="Weibull(5,1)")
plotDist("weibull", params=list(shape=2, scale=10),main="Weibull(2,10)")
plotDist("weibull", params=list(shape=5, scale=10),main="Weibull(5,10)")
@

\subsection{Normal Distributions}
\myindex{normal distribution|defidx}%

We come now to the most famous family of distributions -- the normal
distributions (also called Gaussian distributions).  These symmetric 
distributions have the famous ``bell shape'' and are described by two parameters, 
the mean $\mu$ and the standard deviation $\sigma$.  The pdf for a $\Norm(\mu, \sigma)$
distribution is 

\begin{center}
	\begin{tabular}{ll}
		\hline
		distribution & pdf
		\\
		\hline
		$\displaystyle \Norm(\mu, \sigma)$ 
		& $\displaystyle f(x) = \frac{ 1}{\sigma\sqrt{2\pi}} e^{-(x-\mu)^2/2\sigma^2}$
		\\
		\hline
	\end{tabular}
\end{center}


<<echo=FALSE, fig.width=7>>=
plotDist("norm", 
         scales=list(y=list(draw=FALSE), 
					 x=list(at=-3:3,
                            labels=c( 
                              expression(mu - 3 * sigma),
                              expression(mu - 2 * sigma),
                              expression(mu - 1 * sigma),
                              expression(mu),
                              expression(mu + 1 * sigma),
                              expression(mu + 2 * sigma),
                              expression(mu + 3 * sigma) 
                              )  
                            )
         )
)
@
The inflection points 
of the normal distributions are always at $\mu-\sigma$ and $\mu+\sigma$.

Among the normal distributions is one special distribution -- the \term{standard normal
distribution} -- which has mean 0 and standard deviation 1. All other normal 
distributions are simply linear transformations of the standard normal distribution.
That is,
If $Z \sim \Norm(0,1)$ and $Y = a + b X$ , then $Y \sim \Norm(a, b)$.  Conversely,
if $Y \sim \Norm(\mu, \sigma)$, then $Z = \frac{ Y - \mu }{\sigma} \sim \Norm(0,1)$.

As with the other distributions we have encountered, we have four functions
that allow us to work with normal distributions in \R:

\begin{center}
\begin{tabular}{ll}
	\hline
	Function & What it does \\
	\hline
	\texttt{dnorm(x,mean,sd)} & Computes value of the pdf at \texttt{x}
	\\
	\texttt{pnorm(q,meand,sd)} 
		& Computes value of the cdf at \texttt{x}, i.e., 
	$\Prob(X \le \texttt{q})$
	\\
	\texttt{qnorm(p,mean,sd)} 
		& Computes quantiles, that is a value $q$ so that 
	$\Prob(X \le \texttt{q}) = \texttt{p}$
    \\
	\texttt{rnorm(n,mean,sd)} & Randomly samples \texttt{n} values from a
	normal distribution.
	\\
	\hline
\end{tabular}
\end{center}

\subsubsection{The 68-95-99.7 Rule}

Also known as the ``Empirical Rule", the 68-95-99.7 Rule provides a set of probability
benchmarks for the normal distributions because for any normal distribution: 
\begin{itemize} 
	\item
		$\approx 68$\% of the normal distribution is between 
		$\mu-\sigma$ and $\mu + \sigma$.
	\item
		$\approx 95$\% of the normal distribution is between 
		$\mu- 2\sigma$ and $\mu + 2\sigma$.
	\item
		$\approx 99.7$\% of the normal distribution is between 
		$\mu- 3\sigma$ and $\mu + 3\sigma$.
\end{itemize} 

\begin{example}
	\question
	Before they were rescaled, SAT scores used to be approximately normally
	distributed with a mean of 500 and a standard deviation of 100.
	\begin{enumerate}
		\item Approximately what percent of test takers scored between 400 and 600?
		\item Approximately what percent of test takers scored above 600?
		\item Approximately what percent of test takers scored below 300?
		\item Approximately what percent of test takers scored between 400 and 700?
	\end{enumerate}

	\answer
	\begin{enumerate}
		\item
			68\%
		\item
			Since 68\% are bewteen 400 and 600, the other 32\% must be outside that
			range, half above and half below.  So 16\% are above 600.
		\item
			Since 95\% are between 300 and 700, the other 5\% must be outside that 
			range, half above and half below.  So 2.5\% are below 300.
		\item
			16\% are below 400 and 2.5\% are above 700, so the remaining 81.5\% 
			must be between 400 and 700.
	\end{enumerate}

	Of course, we can get more accurate results using \R:
<<>>=
pnorm( 600, 500, 100) - pnorm(400, 500, 100)
pnorm( 700, 500, 100) - pnorm(300, 500, 100)
pnorm( 300, 500, 100) 
pnorm( 700, 500, 100) - pnorm(400, 500, 100)
@

The \function{xpnorm()} function will additionally draw pictures of the normal 
distribution with a portion of the distribution shaded in.
<<>>=
xpnorm(700,500,100) - xpnorm(400, 500, 100)
@
\end{example}

\begin{example}
	We can use \function{qnorm()} to compute percentiles.  For example, let's calculate
	the 75th percentile for SAT distributions.
<<>>=
qnorm(.75, 500, 100)
@
\end{example}

\subsection{Beta Distributions}
\myindex{Beta distribution|defidx}%

The Beta distributions have support on the interval $(0,1)$, so they can provide a model for proportions or other quantities that
are bounded between 0 and 1.\footnote{A more general version of the Beta
distributions can do the same thing for quantities bounded by any two numbers.
This more general family of distributions has four parameters.}
The Beta distributions have two parameters, imaginatively 
called \texttt{shape1} and \texttt{shape2}.  The kernel of the Beta distributions
is a product of a power of $x$ and a power of $(1-x)$:
\[
k(x; \alpha, \beta) = x^{\alpha-1} (1-x)^{\beta -1} \; \boolval{ x \in [0,1] }
\]
When $\alpha = \beta$, the distribution is symmetric, and when
$\alpha = \beta =1$, we have the $\Unif(0,1)$-distribution.

The two shape parameters provide a wide variety of shapes.

<<>>=
plotDist("beta", params=list(shape1=2, shape2=2), main="Beta(2,2)")
plotDist("beta", params=list(shape1=2, shape2=0.9), main="Beta(2,0.9)")
plotDist("beta", params=list(shape1=4, shape2=2), main="Beta(4,2)")
plotDist("beta", params=list(shape1=0.9, shape2=0.85), main="Beta(0.9,0.85)")
@

\begin{center}
\begin{tabular}{ll}
	\hline
	Function & What it does \\
	\hline
	\texttt{dbeta(x,shape1,shape2)} & Computes value of the pdf at \texttt{x}
	\\
	\texttt{pbeta(q,shape1d,shape2)} 
		& Computes value of the cdf at \texttt{x}, i.e., 
	$\Prob(X \le \texttt{q})$
	\\
	\texttt{qbeta(p,shape1,shape2)} 
		& Computes quantiles, that is a value $q$ so that 
	$\Prob(X \le \texttt{q}) = \texttt{p}$
    \\
	\texttt{rbeta(n,shape1,shape2)} & Randomly samples \texttt{n} values from a
	Beta distribution.
	\\
	\hline
\end{tabular}
\end{center}

\begin{problem}
	Use \R\ to plot the pdf and 
	compute the mean and variance of each of the following distributions.
	\begin{enumerate}
		\item
			$\Beta(2,3)$
		\item
			$\Beta(20,30)$
		\item
			$\Gamm(\texttt{shape}=2,\texttt{scale}=3)$
		\item
			$\Weibull(\texttt{shape}=2,\texttt{scale}=3)$
	\end{enumerate}
\end{problem}


\begin{solution}
<<>>=
# Beta(2,3)
m <- antiD( x * dbeta(x,2,3) ~ x, lower.bound=0)(1)
m
antiD( x^2 * dbeta(x,2,3) ~ x, lower.bound=0 )(1) - m^2
@
<<>>=
# Beta(20,30)
m <- antiD( x * dbeta(x,20,30) ~ x, lower.bound=0)(1)
m
antiD( x^2 * dbeta(x,20,30) ~ x, lower.bound=0 )(1) - m^2
@
<<>>=
# Gamma(2,scale=3)
m <- antiD( x * dgamma(x,2,scale=3) ~ x, lower.bound=0)(Inf)
m
antiD( x^2 * dgamma(x,2,scale=3) ~ x, lower.bound=0 )(Inf) - m^2
@
<<>>=
# Weibull(2,scale=3)
m <- antiD( x * dweibull(x,2,scale=3) ~ x, lower.bound=0)(Inf)
m
antiD( x^2 * dweibull(x,2,scale=3) ~ x, lower.bound=0 )(Inf) - m^2
@
\end{solution}


\begin{problem}
	For each of the following distributions, determine the proportion 
	of the distribution that lies between 0.5 and 1.
	\begin{enumerate}
		\item
			$\Exp(\texttt{rate} = 2)$
		\item
			$\Beta(\texttt{shape1} = 3, \texttt{shape2}=2)$
		\item
			$\Norm(\texttt{mean} = 1, \texttt{sd=2})$
		\item
			$\Weibull(\texttt{shape} = 2, \texttt{scale=1/2})$
		\item
			$\Gamm(\texttt{shape} = 2, \texttt{scale=1/2})$
	\end{enumerate}
\end{problem}

\begin{solution}
<<>>=
pexp(1, 2) - pexp(0.5, 2)
pbeta(1, 3, 2) - pbeta(0.5, 3, 2)
pnorm(1, 1, 2) - pnorm(0.5, 1, 2)
pweibull(1, 2, scale=1/2) - pweibull(0.5, 2, scale=1/2)
pgamma(1, 2, scale=1/2)   - pgamma(0.5, 2, scale=1/2)
@
\end{solution}



\section{Fitting Distributions to Data}

Suppose we think a family of distributions would make a good model for some 
situation.  How do we decide which member of the family to use?  The simple answer
is that we should choose the one that fits ``best."  The trick is deciding what it 
means to fit well.  In fact there is more than one way to measure how well 
a distribution fits a data set.  


\begin{example}
\label{ex:windspeed}
We can use the following code to load a data set that contains three year's worth 
of mean hourly wind speeds (mph) in Twin Falls, ID.  This kind of data is often used
to estimate how much power could be generated from a windmill placed in a given location.

<<include=!onLine,eval=!onLine>>=
read.csv('~/public_html/data/stob/TwinfallsWind.csv') -> Wind
head(Wind, 2)
tail(Wind, 2)
histogram( ~ speed, data=Wind, width=1 )
@
<<include=onLine,eval=onLine>>=
read.csv('http://www.calvin.edu/~rpruim/data/stob/TwinfallsWind.csv') -> Wind
head(Wind, 2)
tail(Wind, 2)
histogram( ~ speed, data=Wind, width=1 )
@
As we can see, the distribution is skewed, but it doesn't look like an 
exponential distribution would be a good fit.  Of the distributions we have seen,
it seems like a Weibull or Gamma distribution would be a potentially good choice.
A Weibull model has often been used as a model for mean hourly wind speed, and the 
shape of our histogram indicates that this is a reasonable family of distributions.

\question Which Weibull distribution is the best model for our data?

\answer
The \function{fitdistr()} in the \pkg{MASS} package uses the method of 
\term{maximum likelihood} to fit univariate (one variable) distributions.
<<fitdistr-windspeed>>=
fitdistr( Wind$speed, "weibull" )
@
For \function{fitdistr()} to fit a Weibull distribution, all of the data must be positive,
but our data includes some 0's.  
<<zeros>>=
tally(~speed==0, data=Wind)
@
Let's see how small the smallest non-zero measurements 
are.
<<>>=
min( ~ speed, data=subset(Wind, speed >0) )
@
This may well be a simple rounding issue, since the wind speeds are recorded to the 
nearest 0.01 and 0.01 is the smallest positive value.
Let's create a new variable that moves each value of 0 to 0.0025 and try again.
Why 0.0025?  If we think that 0.01 represents anything in the range 0.005 to 0.015, which
would round to 0.01, then 0 represents anything in the range 0 to 0.005.  
0.0025 is the middle of that range.

<<fitdistr-windspeed2,warning=FALSE>>=
Wind <- transform(Wind, speed2 = ifelse( speed > 0, speed, 0.0025) )
fitdistr( Wind$speed2, "weibull" )
@
<<include=FALSE>>=
mle <- fitdistr( Wind$speed2, "weibull" )
@
This says that the best fitting (in the sense of maximum likelihood) Weibull
distribution is the $\Weibull(\Sexpr{paste(round(mle$est,2),sep=",")})$-distribution.

The \function{histogram()} function has an option to overlay the distribution
fit by \function{fitdistr()} so we can see how good the fit is graphically.
<<xhistogram-Wind,warning=FALSE>>=
histogram( ~ speed2, data=Wind, fit="weibull" )
@
\end{example}

\begin{example}
As an alternative, we could fit a Gamma distribution to the wind speed data.
<<warning=FALSE>>=
fitdistr(Wind$speed2, "gamma")
histogram( ~ speed2, data=Wind, fit="gamma" )
@
By eye, it appears that the Gamma distribution fits this data set slightly better, but
there may other reasons to prefer the Weibull distribution.  In fact,
there has been a good deal of research done regarding which distributions to use 
for wind speed data fitting.  The answer to the question of which distributions should 
be used seems to be that it depends on the purpose for your modeling:  
``The fact that different distributions excel under different applications
motivates further research on model selection based upon the engineering
parameter of interest." \cite{Morgan2011:WindSpeed}
\end{example}

\begin{example}
1986--87 was a good season for Michael Jordan, a famous former NBA basketball player.
Possible models for the points scored each game that season are normal, Weibull, and 
Gamma distributions.
The normal distributions might be a good choice if we think that the distributions
is roughly symetric (very good games are about the same amount above average as
the very poor games are below average).  Weibull and Gamma distributions have
the built in feature that scores cannot be negative and would allow for a
skewed distribution.
The \function{fitdistr()} function in the \pkg{MASS} package can fit each of these.
<<jordan1, warning=FALSE>>=
require(fastR)     # the Jordan8687 data set is in this package
fitdistr(Jordan8687$Points, "normal")
fitdistr(Jordan8687$Points, "weibull")
fitdistr(Jordan8687$Points, "gamma")
@
We can use a histogram with overlaid density curve to see how well these fits compare 
to the data.
<<jordan2, warning=FALSE>>=
histogram(~ Points, Jordan8687, fit="normal", width=5, main="Normal Fit")
histogram(~ Points, Jordan8687, fit="weibull", width=5, main="Weibull Fit")
histogram(~ Points, Jordan8687, fit="gamma", width=5, main="Gamma Fit")
@
The three fits are similar, but not identical.  

\end{example}
\subsection{Maximum Likelihood}

The \function{fitdistr()} function uses the maximum likelihood method to estimate
distribution parameters.
The maximum likelihood method is one of the most commonly used estimation methods
in all of statistics because (1) it can be used in a wide range of applications,
and (2) the resulting estimators have some some desirable properties.  Maximum likelihood estimation tries to choose the parameter values that \emph{maximize} the \emph{likelihood} of the observed data.  

First, let's think about the ``likelihood" of an individual observed data-point.  The likelihood of the data-point is just the probability density function (or probability mass function) for the distribution of interest, evaluated at the value observed in the data. The likelihood gives some indication of how frequently we'd expect to observe this value, but it is \emph{not} a probability (for one thing, likelihoods can exceed 1).  The figure below illustrates that the likelihood of observing a person 80 inches (6 feet, 8 inches) tall, if the person comes from a population whose heights are Normally distributed with a mean of 68 inches and a standard deviation of 6 inches is about \Sexpr{round(dnorm(80,mean=68, sd=6), digits=3)}: 
<<likelihood, echo=FALSE, fig.keep="last">>=
h <- 80; mu <- 68; sdev <- 6
plotDist("norm", params=list(mean=mu, sd=sdev), main="", 
         xlab="Height (inches)", ylab="Density or Likelihood")
trellis.focus()
k <- lpoints(h, dnorm(h,mean=mu, sd=sdev), pch=17)
lsegments(h, 0, h, dnorm(h,mean=mu, sd=sdev), lwd=2)

trellis.unfocus()
@


Given a set of specific parameter values, the likelihood of an entire observed data-set can be calculated by obtaining the value of the likelihood of each observed data-point, and summing these over all the observed data points.  Then, we can find the maximum likelihood parameter estimates by trying many candidate parameter values until satisfied that we have found the ones that maximize the likelihood. (The numerical methods used are usually a bit more sophisticated than ``guessing lots of random candidate values", but we won't get into the details here. In some cases, it is also possible to write down a mathematical expression for the likelihood of the data given the parameters, and maximize it analytically.)

We'll illustrate the main ideas of maximum likelihood with a simple example.

\begin{example}
\myindex{dice|exampleidx}%
Michael has three dice in his pocket.  One is a standard die with six sides,
another has four sides, and the third has ten sides.  He challenges you to a
game.  Without showing you which die he is using, Michael is going to roll a 
die 10 times and report to you how many times the resulting number is a 
$1$ or a $2$.  Your challenge is to guess which die he is using.

\question Michael reports that $3$ of the $10$ rolls resulted in a $1$ or a
$2$.  Which die do you think he was using?

\answer
The probability of obtaining a $1$ or a $2$ is one of $\frac12$,
$\frac13$, or $\frac15$, depending on which die is being used.  Our data
are possible with any of the three dice, but let's see how likely they are
in each case.

\begin{itemize}
\item
If $\evProb{roll 1 or 2}  = \frac15$, 
then the probability of obtaining exactly Michael's data is 
\[
\left(\frac15\right)^3 \left(\frac45\right)^7  
=  \Sexpr{(1/5)^3 * (4/3)^7}
\;.
\]
(Whatever the order, there will be 3 events with probability
$1/5$ and 7 with probability $4/5$.  Since the events are independent,
we can multiply all of these probabilities.)
\item
If $\evProb{roll 1 or 2}  = \frac13$, 
then the probability of obtaining exactly Michael's data is 
\[
\left(\frac13\right)^3 \left(\frac23\right)^7 
=  \Sexpr{(1/3)^3 * (2/3)^7}
\;.
\]
\item
If $\evProb{roll 1 or 2}  = \frac12$, 
then the probability of obtaining exactly Michael's data is 
\[
\left(\frac12\right)^3 \left(\frac12\right)^7 = \Sexpr{.2^3 * .8^7}
\;.
\]
\end{itemize}
Of these, the largest likelihood is for the case that 
$\evProb{roll 1 or 2}  = \frac13$, i.e.,
for the standard, six-sided die.  Our data would be more likely
to occur with that die than with either of the other two -- it is the 
maximum likelihood die.
\end{example}

In general, maximum likelihood calculations are harder because instead of having
only 3 choices, there will be infinitely many choices, and instead of having only
one parameter, there may be multiple parameters.  So techniques from (multi-variable) 
calculus or numerical approximation methods are often used to maximize the likelihood function.
The \function{fitdistr()} function uses pre-derived formulas for some distributions
and numerical approximation methods for others.  In some cases, you will get warning
messages about attempts to apply a function to values that don't make sense (trying to
take logs or square roots of negative numbers, zero in the denominator, etc.) as the 
numerical approximation algorithm explores options in an attempt to find the best fit.
The help documenation for \function{fitdistr()} explains which distributions it can 
handle and what method is used for each.

\subsection{The method of moments}
An easy (but sometimes fairly crude) way to estimate the parameters of a distribution
is the method of moments.  You will often see this method used in engineering textbooks,
espeically if they do not rely on software that implements others methods (like the 
maximum likelihood method).

The basic idea is to set up a system of  equations where we set the mean of the data equal to the mean of the distribution, the variance of the data equal to the variance of the distribution, etc.\footnote{If our distribution has more than 2 parameters, we will need higher moments, which we will not cover here.}  

To employ this method, we need to know the means and variances of our favorite families of distributions (in terms of the parameters of the distributions).  For all of the distributions we have seen, one can work out formulas for the means and variances in terms of the parameters involved.  These are listed in Table~\ref{table:contDist}

\begin{example}
	Let's return to the wind speeds in Example~\ref{ex:windspeed}.
	The formulas for the mean and variance of a Weibull distribution involve
	the gamma function $\Gamma()$, which might be unfamiliar to you.  So let's 
	simplify things.

	Theoretical properties and observations of wind speeds at other locations 
	suggest that using a shape parameter of $\alpha=2$ is often a good choice (but 
	shape does differ from location to location depending on how consistent or 
	variable the wind speeds are).
	The Weibull distributions with $\alpha=2$ have a special name, they are 
	called the \term{Rayleigh} distributions.  
	So $\Rayleigh(\beta) = \Weibull(\alpha=2, \beta)$.
	In this case, from Table~\ref{table:contDist}, we see that to calculate 
	the mean we need the value of $\Gamma(1 + \frac{1}{2}) = \Gamma(1.5) = \sqrt{\pi}/2$.
<<>>=
gamma(1.5)
sqrt(pi)/2
@
	From Table~\ref{table:contDist} we see that the mean of a 
	$\Rayleigh(\beta)$-distribution is 
	\[
	\E(X) = \beta \frac{\sqrt{\pi}}{2}
	\]

	Now we can choose our estimate $\hat \beta$ for $\beta$ so that 
	\[
	\hat \beta \frac{\sqrt{\pi}}{2} = \mean x  ;.
	\]
	That is,
	\[
	\hat\beta = \frac{2 \mean x }{\sqrt{\pi}}
	\]
<<>>=
x.bar <- mean(~speed, data=Wind) 
x.bar
beta.hat <- x.bar * 2 / sqrt(pi)
beta.hat 
@
So our method of moments fit for the data is a 
$\Rayleigh(\Sexpr{round(beta.hat,2)}) = \Weibull(2, \Sexpr{round(beta.hat,2)})$

Although the Rayleigh distributions are not as flexible as the Weibull or Gamma 
distributions, and although maximum likelihood is generally preferred over the
method of moments, the method of moments fit of a Rayliegh distribution does have
one advantage: it can be computed even if all you know is the mean of some sample data.
Sometimes, that is all you can easily get your hands on (because the people who collected
the raw data only report numerical summaries).  You can find average wind speeds of for 
many locations online, for example here:
\url{http://www.wrcc.dri.edu/htmlfiles/westwind.final.html}
\end{example}


\begin{example}
For distributions with two parameters, we solve a system of two equations with two unknowns.
For the normal distributions this is particularly easy since the parameters are the mean
and standard deviation, so we get
\begin{align*}
	\hat\mu &= \mean x\\
	\hat\sigma^2 &= s_x^2\\
\end{align*}
<<tidy=FALSE>>=
x.bar <- mean(~speed, data=Wind); x.bar
v <- var(~speed, data=Wind); v
sqrt(v)
@
So the method of moments suggests a $\Norm(\Sexpr{round(x.bar,2)}, \Sexpr{round(sqrt(v),2)})$
distribution.  In this case, the method of moments and maximum likelihood methods
give the same results. 

<<>>=
fitdistr(Wind$speed, "normal")
@
But this doesn't mean that the fit is particularly good.  Indeed, a normal distribution is 
not a good choice for this data.  We know that wind speeds can't be negative and we 
have other distributions (exponential, Weibull, and Gamma, for example) that are also
never negative.  So choosing one of those seems like a better idea.
The following plot shows, as we expected, that the normal distribution is not a particularly
good fit.
<<>>=
histogram(~speed, data=Wind, fit="normal")
@
It is important to remember that the best fit using a poor choice for the family
of distriubtions might not be a useful fit.  
The choice of distributions is made based on a combination of theoretical 
considerations, experience from previous data sets, and the quality of 
the fit for the data set at hand.
\end{example}


\begin{table}
\begin{center}
\small
\begin{tabular}{|p{5mm}p{40mm}p{25mm}p{45mm}|}
\hline
\multicolumn{2}{|l}{\textbf{\sf distribution} }
&&
\\
  & \textbf{\sf pdf} 
  & \textbf{\sf mean} 
  & \textbf{\sf variance} 
\\[.5mm] \hline
&&& \\[-2mm]
\multicolumn{2}{|l}{  Triangle: $\Tri(a,b,c)$} && \\
  & 
  $\displaystyle 
 	 \begin{cases}
		 \frac{2(x-a)}{(b-a)(c-a)}  &  \mbox{if $x \in [a,c]$} \;,  \\
		 \frac{2(b-x)}{(b-a)(b-c)}  &  \mbox{if $x \in [c,b]$} \;,  \\
		0 & \mbox{otherwise} \\
	  \end{cases}$
	  & $\displaystyle \frac{a + b + c}{3}$ 
	  & $\displaystyle \frac{a^2 + b^2 + c^2 - ab -ac -bc}{18}$
	  \\[5.2mm]
	  \multicolumn{2}{|l}{  Uniform: $\Unif(a,b)$} && \\
  & 
  $\displaystyle 
 	 \begin{cases}
		\frac{1}{b-a}  &  \mbox{if $x \in [a,b]$} \;,  \\
		0 & \mbox{otherwise} \\
	  \end{cases}$
	  & $\displaystyle \frac{b+a}{2}$ 
	  & $\displaystyle \frac{(b-a)^2}{12}$
	  \\[5.2mm]
\multicolumn{2}{|l}{  Standard normal: $\Norm(0,1)$ } && \\
  & 
  $ \displaystyle \frac{1}{\sqrt{2\pi}} {e^{-\frac12 z^2}}$
  	& $0$ & $1$
\\[4.2mm]
\multicolumn{2}{|l}{  Normal: $\Norm(\mu,\sigma)$ } && \\
  & $
	 \displaystyle \frac{1}{\sigma\sqrt{2\pi}} \cdot 
     		e^{-\frac12 (\frac{x-\mu}
			{\sigma})^2}$
  	& $\mu$ & $\sigma^2$
\\[5.2mm]
\multicolumn{2}{|l}{  Exponential: $\Exp(\lambda)$ }  && \\
%  	& $\lambda$
	& $\lambda e^{-\lambda x}$
  	& $1/\lambda$ & $1/\lambda^2$
\\[4.2mm]
\multicolumn{2}{|l}{  Gamma: $\Gamm(\alpha, \lambda = \frac{1}{\beta})$ } && \\
%  	& $\lambda$
	& $\displaystyle \frac{\lambda^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-\lambda x}$
  	& $\alpha/\lambda = \alpha \beta$ 
	& $\alpha/\lambda^2 = \alpha \beta^2$
\\[4.2mm]
\multicolumn{2}{|l}{Weibull: $\Weibull(\alpha,\beta=\frac{1}{\lambda})$} && \\
&
	  $\displaystyle \frac{\alpha}{\beta^\alpha} x^{\alpha-1} e^{-(x/\beta)^\alpha}$
	& 
	  $\beta \Gamma(1 + \frac1{\alpha})$
	&
	  $\beta^2 \left[ \Gamma(1 + \frac{2}{\alpha}) 
	  - \left[ \Gamma(1 + \frac{1}{\alpha}) \right]^2 \right]
	  $ 
\\[4.0mm]
\multicolumn{2}{|l}{  Beta: $\Beta(\alpha, \beta)$ } && \\
	& 
  $\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} 
		x^{\alpha-1}(1-x)^{\beta-1}$
  	& 
	  $\displaystyle \frac{\alpha}{\alpha + \beta}$
	  &
	  $\displaystyle \frac{\alpha \beta }{(\alpha + \beta)^2(\alpha + \beta + 1)}$
\\[4mm]
\hline
\end{tabular}
\end{center}
\caption{Some common continuous distributions.
Standard names for parameters that appear in several distributions
include \texttt{rate} ($\lambda$), \texttt{shape} ($\alpha$), and \texttt{scale} ($\beta$).
In the normal distributions, $\mu$ and $\sigma$ are called \texttt{mean} and \texttt{sd}
in \R, and in the uniform distirbutions, $a$ and $b$ are called \texttt{min} and \texttt{max}.
The function $\Gamma(x)$ that appears in the formulas for the Weibull and Beta
distributions is a kind of continuous extrapolation from the factorial function.
The \texttt{gamma()} function will calculate these values.}
\label{table:contDist}
\end{table}


\section{Quantile-Quantile Plots}

\myindex{quantile-quantile plot}%
To this point we have looked at how well a distribution fits the data by overlaying a density
curve on a histogram.  While this is instructive, it is not the easiest way to make a 
graphical comparison between a data set and a theoretical distribution.   Our eyes
are much better and judging whether something is linear than they are at judging whether 
shapes have a particular kind of curve.  Furthermore, certain optical misperceptions
tend to cause people to exaggerate some kinds of differences and underestimate others.

Quantile-quantile plots offer an alternative approach.  As the name suggests, the idea is 
to compare the quantiles of our data to the quantiles of a theoretical distribution.  These
are then plotted as a scatter plot.  Let's go through those steps with a small data
set so we can see all the moving parts, then we'll learn how to automate the whole
process using \function{qqmath()}.

\subsection{Normal-Quantile Plots}
The normal distributions are especially important for statistics, so normal-quantile
plots will be our most important example of quantile-quantiles plots.  Also, special
properties of the normal distributions make normal-quantile plots especially easy
and useful.  We will illustrate the construction of these plots using a data set
containing Michael Jordan's game by game scoring output from the 1986--87 basketball
season.

\begin{example}
Let's begin by forming a randomly selected sample of 10 basketball games.
<<>>=
set.seed(123)              # so you can get the same sample if you like.
SmallJordan <- sample(Jordan8687, 10)
SmallJordan
@

<<tidy=FALSE>>=
probs <- seq(0.05, 0.95, by=0.10)
probs
observed <- sort(SmallJordan$Points)                                # sorted observations
theoretical <- qnorm( probs, mean=mean(observed), sd=sd(observed) ) # theoretical quantiles
QQData <- data.frame(observed=observed, theoretical=theoretical)
QQData
@
If the observed data matched the theoretical quantiles perfectly, a scatter plot
would place all the points on the line with slope 1 passing through the origin.

<<hand-qqplot>>=
xyplot( observed ~ theoretical, data=QQData, main="Hand made QQ-plot" )
plotFun( x ~ x, add=TRUE, alpha=0.6)
@
Even better, we don't need to know the mean and standard deviation in advance, because
all normal distributions are linear transformations of the $\Norm(0,1)$-distribution.
So our standard practice will be to compare our data to the $\Norm(0,1)$-distribution.
If $X \sim \Norm(\mu,\sigma)$, then $X = \mu + \sigma Z$ where $Z \sim \Norm(0,1)$, so 
a plot of $X$ vs. $Z$ will have slope $\sigma$ and intercept $\mu$.

<<>>=
theoretical2 <- qnorm( probs, mean=0, sd=1 ) # theoretical quantiles from Norm(0,1)
QQData2 <- data.frame(observed=observed, theoretical=theoretical2)
xyplot( observed ~ theoretical, data=QQData2, main="Hand made QQ-plot", xlab="theoretical (z)" )
ladd(panel.abline(mean(SmallJordan$Points), sd(SmallJordan$Points), alpha=0.5, col="navy"))
ladd(panel.abline(h = mean(SmallJordan$Points), alpha=0.5))
ladd(panel.abline(v = 0, alpha=0.5))
@

This whole process is automated by the \function{qqmath()} function.
<<>>=
qqmath( ~ Points, SmallJordan, main="Sub-sample" )
qqmath( ~ Points, Jordan8687, main="Full data set" )
@
\end{example}

\subsection{Other distributions}

Working with other distributions is similar, but most families of distributions
don't have a single ``master example" to which we can make all comparisons, so we
need to pick a particular member of the family (either by fitting or for some 
theoretical reason).\footnote{There are a few other families of distributions that
have a prototypical member such that all other members are a linear transformation
of the prototype.  The exponential family is one such family.}

\begin{example}
	Let's build a quantile-quantile plot for our wind speed data comparing 
	to normal, gamma and Weibull distributions.
\iffalse
	As in the previous section, we'll begin by creating it manually using 
	a small subset of the data and then show how to automate the process 
	using \function{qqmath()}.
<<>>=
SmallWind <- sample(Wind, 10)
SmallWind
@
Now we need to compute the quantiles of our data and the quantiles of our 
theoretical distribution.  We start by selecting 10 evenly spaces 
proportions.
<<>>=
probs <- seq(0.05, 0.95, by=0.10)
probs
@
Now we compute the quantiles for these probabilities using the parameters
we fit previously.
<<tidy=FALSE>>=
y <- qdata( probs, SmallWind$speed2)                     # quantiles from data
x.gamma <- qgamma( probs, shape=2.496, rate=0.421 )      # quantiles from Gamma dist
x.weibull <- qweibull( probs, shape=1.694, scale=6.651 ) # quantiles from Weibull dist
x.normal <- qnorm( probs, mean=5.925 , sd=3.653  )       # quantiles from Normal dist
@
Finally, we create the scatter plot
<<>>=
xyplot( y ~ x.gamma )
xyplot( y ~ x.weibull )
xyplot( y ~ x.normal )
@
\fi
We can automate this, but we need to tell \function{qqmath()} how to calculate
the quantiles.
<<>>=
qqmath( ~ speed2, data=Wind )  # normal-quantile plot; normal is not a good model
@

The normal model does not fit well, but both Gamma and Weibull are reasonable models:
<<tidy=FALSE, warning=FALSE>>=
fitdistr(Wind$speed2, "gamma")
fitdistr(Wind$speed2, "Weibull")
fittedqgamma <- makeFun( qgamma(p, shape=2.496, rate=0.421 ) ~ p )
fittedqweibull <- makeFun( qweibull(p, shape=1.694, scale=6.651) ~ p ) 
qqmath( ~speed2, data=Wind, distribution=fittedqgamma )
qqmath( ~speed2, data=Wind, distribution=fittedqweibull )
@
\end{example}


\newpage
\section*{Exercises}

\begin{problem}
	\begin{enumerate}
	\item
Using Table~\ref{table:contDist} and the method of moments,
fit an exponential distribution to the Twin Falls wind speed data.
What is the estimated value of the rate parameter?
\item
	Now use \function{fitdistr()} to fit an exponential
	distribution using maximum likelihood.
\item
	How do the two estimates for the rate parameter compare?
\item 
	How well does an exponential distribution fit this data?
\end{enumerate}
\end{problem}

\begin{solution}
	\begin{enumerate}
		\item
			The method of moments fit for $\lambda$ comes from solving 
			$\frac{1}{\lambda} = \mean x$ for $\lambda$, so 
			$\hat \lambda = \frac{1}{\mean x}$
<<>>=
lambda.hat <- 1/ mean(~speed, data=Wind)
lambda.hat
@
		\item
<<>>=
fitdistr(Wind$speed, "exponential")
@
		\item
			They are the same in this case.
		\item
			This fit is not that great.
<<>>=
histogram(~speed, data=Wind, fit="exponential")
qqmath(~speed, data=Wind, distribution=qexp)
@
	\end{enumerate}
\end{solution}

\begin{problem}
	A Gamma distribution can also be fit using the method of moments.
	Because there are two parameters (shape and rate or shape and scale),
	you will need to solve a system of two equations with two unknowns.
	\begin{enumerate}
		\item
Using Table~\ref{table:contDist} and the method of moments,
fit a Gamma distribution to the Twin Falls wind speed data.
What are the estimated values of the shape and rate parameters?
\item
	How do the method of moments estimates for the parameters compare to the 
	maximum likelihood estimates from \function{fitdistr()}?
	\end{enumerate}
\end{problem}


\begin{solution}
<<tidy=FALSE,warning=FALSE>>=
m <- mean(~speed2, data=Wind); m
v <- var(~speed2, data=Wind); v
fitdistr(Wind$speed2, "gamma")
@
Now we solve
	\begin{align*}
		\alpha \beta & = \mean x  = \Sexpr{mean(~speed2, data=Wind)}
		\\
		\alpha \beta^2 & = s^2 = \Sexpr{var(~speed2, data=Wind)}
	\end{align*}
Dividing the second by the first gives $\hat \beta = \sfrac{s^2}{\mean x}$.
From this we obtain $\hat \alpha = \mean{x} / \hat \beta = \sfrac{ \mean x^2}{s^2}$.
<<tidy=FALSE>>=
beta.hat <- v/m; beta.hat
alpha.hat <- m / beta.hat ; alpha.hat
lambda.hat <- 1/ beta.hat ; lambda.hat
@
The fitted values are similar to but not identical to the maximum likelihood estimates.
\end{solution}

\begin{problem}
	Sam has found some information about wind speed at a location he
	is interested in online.  Unfortunately, the web site only provides
	the mean and standard deviation of wind speed.  
	\begin{center}
		\begin{tabular}{rl}
			mean: &  10.2 mph
			\\
			standard deviation: & 5.1 mph
		\end{tabular}
	\end{center}
	\begin{enumerate}
		\item
	Use this information and the method of moments to estimate the 
	shape and rate parameters of a Gamma distribution.
\item	
	In principal, we could do the same for a Weibull distribution, but the 
	formulas aren't as easy to work with. 
	Fit a Rayliegh distribution instead (i.e., a Weibull
	distribution with shape parameter equal to 2).
	\end{enumerate}
\end{problem}

\begin{solution}
We can recycle some work from the previous problem to quickly obtain
the method of moments fit for the Gamma distribution:
<<tidy=FALSE>>=
m <- 10.2
v <- 5.1^2 
beta.hat <- v/m; beta.hat
alpha.hat <- m / beta.hat ; alpha.hat
lambda.hat <- 1/ beta.hat ; lambda.hat
@
For the Rayliegh distribution we solve 
	\(
	\hat \beta \frac{\sqrt{\pi}}{2} = \mean x  
	\) for 
	\(\hat \beta\) and get
	\[
	\hat\beta = \frac{2 \mean x }{\sqrt{\pi}}
	= \frac{2 \cdot 10.2 }{\sqrt{\pi}}
	= \Sexpr{2 * 10.2 / sqrt(pi)}
	\]	
\end{solution}

\begin{problem}
	In 1964, a study was undertaken to see if IQ at 3 years of age is
	associated with amount of crying at newborn age. In the study, 38 newborns
	were made to cry after being tapped on the foot, and the number of distinct
	cry vocalizations within 20 seconds was counted.
	The subjects were followed up at 3 years of age and their IQs were measured.
%	The data from this study are in the \dataframe{Baby} data frame.  
	You can load this data using
<<Baby>>=
Baby <- read.file("http://www.calvin.edu/~rpruim/data/BabyCryIQ.csv")
head(Baby)
@

	The \variable{cry.count} variable records the number of distinct cry vocalizations 
	within 20 seconds.  Choose a family of distributions to fit to this data
	and do the fit using \function{fitdistr()}. Also include a plot showing 
	a histogram and your fitted density curve.
\end{problem}

\begin{solution}
	The distribution is skewed and non-negative, so a gamma or Weibull seems like 
	a good thing to try.  
<<>>=
fitdistr(Baby$cry.count, "gamma")
fitdistr(Baby$cry.count, "Weibull")
histogram(~cry.count, data=Baby, fit="gamma", main="Gamma")
histogram(~cry.count, data=Baby, fit="Weibull", main="Weibull")
@
\end{solution}

\iffalse
\begin{problem}
	For each of the following, give a family of distributions
	that might make a reasonable model and say breifly why you chose that family
	\begin{enumerate}
		\item
			Batting averages of MIAA base ball players.  (A batting average is roughly
			the proportion of at bats where the batter gets a hit.)

	\end{enumerate}
\end{problem}
\fi

\begin{problem}
	Create normal quantile plots for the ages of patients in the \dataframe{HELPrct}
	data set separated by \variable{substance}. (Getting separate or overlaid plots
	using \function{qqmath()} works just like it does for other lattice plots).

	Comment on the plots.
\newpage
\end{problem}


\begin{solution}
<<>>=
qqmath( ~ age | substance, data=HELPrct )
@
The qq-plot for the alchol group looks good.  The other two (especially cocaine) show
signs of skew -- indicated by the curve to the qq plot.
<<>>=
densityplot( ~ age | substance, data=HELPrct )
@
\end{solution}


\begin{problem}
	Match the normal-quantile plots to the histograms.
<<echo=FALSE>>=
rescale <- function(x,lim=c(0,10)) {
	return ( min(lim) + (x - min(x))/ (diff(range(x))) * (diff(range(lim))) );
}

n <- 400;

a <- qnorm(ppoints(n));
a <- rescale(a);

b <- qbeta(ppoints(n),20,5);
b <- rescale(b);

c <- qexp(ppoints(n),2);
c <- qbeta(ppoints(n),3,15);
c <- rescale(c);

d <- c(runif(n=n/2, min=0,max=10), qunif(ppoints(n/2),0,10) );
d <- rescale(d);

# bowl shaped
e <- 10 * c(rbeta(500,6,1),rbeta(500,1,6));
e <- c( qbeta(ppoints(100),6,1), qbeta(ppoints(100),1,6) );
e <- rescale(e);

#f <- 10 * c(0,1,qbeta(ppoints(n-2),14,15));
#f <- rescale(f)
f <- qexp(ppoints(n),2);
f <- rescale(f)

D1 <- data.frame(A=c,B=a,C=b,D=d,E=e,F=f);

sD1 <- stack(D1);

D2 <- data.frame(V=a,Z=b,Y=c,W=d,X=e,U=f);

sD2 <- stack(D2);
@

<<compareplots1,echo=FALSE,fig.height=5>>=
histogram(~ values | ind, 
	data=sD1, as.table=TRUE,
	breaks=seq(-0.5,10.5,by=.5),
	scales=list(draw=FALSE, alternating=FALSE)
	)
qqmath(~values | ind, data=sD2, as.table=TRUE, scales=list(draw=FALSE))
@
\end{problem}
\begin{solution}
a) Y  b) V c) Z d) W  e) X f) U
\end{solution}

\begin{problem}
	Show that $\Var(X) = \E(X^2) - \E(X)^2$ by showing that 
	\[
	\int_{-\infty}^{\infty} (x - \mu_X) ^2 f(x) \; dx
	=
	\int_{-\infty}^{\infty} x^2 f(x) \; dx  -  \mu_X^2
	\]
	whenever $f$ is a pdf and all the integrals involved converge.
\end{problem}

\begin{solution}
	\begin{align*}
	\int_{-\infty}^{\infty} (x - \mu_X) ^2 f(x) \; dx
	&=
	\int_{-\infty}^{\infty} (x^2 - 2\mu_X x + \mu_X^2) f(x) \; dx
	\\
	&=
	\int_{-\infty}^{\infty} (x^2 f(x) \; dx
	- 2 \mu_X \int_{-\infty}^{\infty} x f(x) \; dx
	+ \mu_X^2 \int_{-\infty}^{\infty} f(x) \; dx
	\\
	&=
	\Var(X) - 2 \mu_X^2 + \mu_X^2
	\\
	&=
	\Var(X) - \E(X)^2 
\end{align*}
\end{solution}

\begin{problem}
	The heights of 18--22 year olds in the US follow approximately normal distributions
	within each sex.  Estimated means and standard deviations appear in the table below.
	\begin{center}
		\begin{tabular}{ccc}
			\hline
			& mean & standard deviation \\
			\hline
			women &64.3 in & 2.6 in  
			\\
			men & 70 in & 2.8 in
			\\
			\hline
		\end{tabular}
	\end{center}
	Answer the following questions without using a computer or calculator (except for basic
	arithmetic).
	\begin{enumerate}
		\item If a woman is 68 inches tall, what is her z-score?
		\item If a man is 74 inches tall, what is his z-score?
		\item What is more unusual, a woman who is at least 68 inches tall
			or a man who is at least 74 inches tall?
		\item
			Big Joe has decided to open a club for tall people.  To join his club,
			you must be in the tallest 2.5\% of people of your sex.
			How tall must a woman be to join Big Joe's club?
		\item
			How tall must a man be to join Big Joe's club?
	\end{enumerate}
\end{problem}

\begin{problem}
	Use the infromation from the previous problem to answer the following questions.
	\begin{enumerate}
		\item
			What proportion of women are 5'10" or taller?
		\item
			What proportion of men are 6'4" or taller?
		\item
			If a man is in the 75th percentile for height, how tall is he?
		\item
			If a woman is in the 30th percentile for height, how tall is she?
	\end{enumerate}
\end{problem}

\shipoutProblems

\ifsolutions
\ifsolutionslocal
\newpage
\section*{Solutions}
\shipoutSolutions
\fi
\fi
