\Sexpr{set_parent('Math241-S14.Rnw')}

\chapter{Modeling Relationships Between Variables}


In Chapter~\ref{chap:propagation} we learned how to estimate one quantity based
on its (known) relationship to other quantities.  For example, we estimated the
number of dimes in a sack of dimes from our estimates of the weight of the
dimes and the average weight of a dime.  

In this chapter we will explore how to use data to determine the relationship among
two or more variables when this relationship is not know in advance.  The general
framework we will use is 

\[
Y = f(x_1, x_2, \dots, x_k) + \varepsilon
\]
\begin{itemize}
	\item $Y$ is the \term{response} variable that we are trying to estimate
		from $k$ \term{explanatory} variables $x_1, x_2, \dots, x_k$.
	\item
		The relationship between the explanatory variables and the response 
		variables is described by a function $f$.
	\item
		The relationship described by $f$ need not be a perfect fit.  The \term{error}
		term in the model, $\varepsilon$, describes how individual responses
		differ from the value give by $f$.  
		
		We will model $\varepsilon$ with a 
		distribution -- typically a distribution with a mean of 0 -- 
		so another way to think about this model is the for a given 
		values of the predictors, the values of $Y$ have a distribution.  The mean
		of this distribution is specified by $f$ and the shape by $\varepsilon$.
\end{itemize}


\section{The Simple Linear Regression Model}

\[
Y = \beta_0 + \beta_1 x + \varepsilon  \qquad \mbox{where $\varepsilon \sim \Norm(0,\sigma)$.}
\]

In other words:
\begin{itemize}
\item
The mean response for a given predictor value $x$ is given by a linear formula
\[
\mbox{mean response} = \beta_0 + \beta_1 x
\]
\item
The distribution of all responses for a given predictor value $x$ is normal.
\item
The standard deviation of the responses is the same for each predictor value,
\end{itemize}
Furthermore, in this model the values of $\varepsilon$ are independent.

There are many different things we might want to do with a linear model, for example:
\begin{itemize}
	\item Estimate the coefficients $\beta_0$ and $\beta_1$.
	\item Estimate the value $Y$ associated with a particular value of $x$.
	\item Say something about how well a line fits the data.
\end{itemize}

\section{Fitting the Simple Linear Model}

\subsection{The Least Squares Method}

We want to determine the best fitting line to the data.  The usual method is 
the method of least squares\footnote{In this case, it turns out that the least 
squares and maximum likelihood methods produce exactly the same results.}
which chooses the line that has the 
\emph{ smallest possible sum of squares of residuals}, where residuals are defined by

\[
\mbox{residual} = \mbox{observed} - \mbox{predicted}
\]

\begin{example}
Consider the following small data set.
\begin{multicols}{2}
<<include=FALSE>>=
someData <- data.frame(x=1:5, y=c(1,3,2,4,4))
@
<<tidy=FALSE,fig.height=3.0>>=
someData
xyplot( y ~ x, 
  data=someData, type=c('p','g'), 
  ylim=c(0,5), xlim=c(0,6))
@
\end{multicols}

\begin{enumerate}
	\item
		Add a line to the plot that ``fits the data well".  Don't do any calculations,
		just add the line.
	\item
		Now estimate the residuals for each point relative to your line
	\item
		Compute the sum of the squared residuals, $SSE$.
	\item
		Estimate the slope and intercept of your line.
\end{enumerate}

\newpage
For example, suppose we we select a line that passes through $(0,1)$ and $(5,4)$. 
the equation for this line is $y = 1 + .6 x$, and it looks like a pretty good fit:
<<>>=
xyplot( y ~ x, data=someData, xlim=c(0,6), ylim=c(0,5) )
f <- makeFun( 1 + .6 * x ~ x)
plotFun( f(x) ~ x, add=TRUE, col="gray50" )
@
The residuals for this function are 
<<tidy=FALSE>>=
resids <- with(someData, y - f(x)) ; resids 
@
and $SSE$ is 
<<>>=
sum(resids^2)
@
If your line is a good fit, then $SSE$ will be small.  
The best fitting line will have the smallest possible $SSE$.   
The \function{lm()} function will find this best fitting line for us.
<<tidy=FALSE>>=
model1 <- lm( y ~ x, data=someData ); model1
@
This says that the equation of the best fit line is 
\[
\hat y = \Sexpr{coef(model1)[1]} + \Sexpr{coef(model1)[2]} x
\]

<<>>=
xyplot( y ~ x, data=someData, type=c('p','r') )
@
We can compute $SSE$ using the \function{resid()} function.
<<tidy=FALSE>>=
SSE <- sum ( resid(model1)^2 ); SSE
@
As we see, this is a better fit than our first attempt -- 
at least according to the least squares criterion.
It will better than \emph{any} other attempt -- it is the least 
squares regression line.
\end{example}

\subsection{Properties of the Least Squares Regression Line}
For a line with equation $y = \hat\beta_0 + \hat\beta_1 x$, the residuals are 
\[
e_i = y_i - (\hat\beta_0 + \hat\beta_1 x) 
\]
and the sum of the squares of the residuals is 
\[
SSE = \sum e_i^2  = \sum (y_i - (\hat\beta_0 + \hat\beta_1 x) )^2
\]
Simple calculus (which we won't do here) allows us to compute the 
best $\hat\beta_0$ and $\hat\beta_1$ possible.  
These best values define the least squares regression line.
We always compute these values using software, but it is good to note that 
the least squares line satisfies two very nice properties.
\begin{enumerate}
\item
The point $(\mean x, \mean y)$ is on the line. 

This means that $\mean y = \hat\beta_0 + \hat\beta_1 \mean x$  (and $\hat\beta_0 = \mean y - \hat\beta_1 \mean x$)
\item
The slope of the line is $\displaystyle b = r \frac{s_y}{s_x}$ where $r$ is the 
\term{correlation coefficient}:
\myindex{correlation coefficient}%
\[
r = \frac{1}{n-1} \sum \frac{ x_i - \mean x }{s_x} \cdot \frac{ y_i - \mean y }{s_y}
\]
\end{enumerate}
Since we have a point and the slope, it is easy to compute the equation for the line
if we know $\mean x$, $s_x$, $\mean y$, $s_y$, and $r$.

\begin{example}
In a study of eye strain caused by visual display terminals, researchers wanted
to be able to estimate ocular surface area (OSA) from palprebal fissure (the
horizontal width of the eye opening in cm) because palprebal fissue is easier
to measure than OSA.
<<>>=
require(Devore6)
head(xmp12.01)
x.bar <- mean( ~palprebal, data=xmp12.01)
y.bar <- mean( ~OSA, data=xmp12.01)
s_x <- sd( ~palprebal, data=xmp12.01)
s_y <- sd( ~OSA, data=xmp12.01)
r <- cor( xmp12.01$palprebal, xmp12.01$OSA)
c( x.bar = x.bar, y.bar=y.bar, s_x=s_x, s_y=s_y, r=r )
slope <- r * s_y/s_x
intercept <- y.bar - slope * x.bar
c(intercept=intercept, slope=slope)
@
\end{example}

Fortunately, statistical software packages do all this work for us, so the caclulations
of the preceding example don't need to be done in practice.

\begin{example}
In a study of eye strain caused by visual display terminals, researchers wanted to be able to
estimate ocular surface area (OSA) from palprebal fissure (the horizontal width of the eye
opening in cm) because palprebal fissue is easier to measure than OSA.
<<>>=
osa.model <- lm( OSA ~ palprebal, data=xmp12.01) 
osa.model
@
\function{lm()} stands for linear model.  The default output includes the estimates
of the coefficients ($\hat\beta_0$ and $\hat \beta_1$) based on the data.  If that is the 
only information we want, then we can use 
<<>>=
coef(osa.model)
@

This means that the equation of the least squares regression line is 
\[
\hat y = \Sexpr{round(coef(osa.model)[1],3)} + \Sexpr{round(coef(osa.model)[2],3)} x
\]

We use $\hat y$ to indicate that this is not an observed value of the response variable
but an estimated value (based on the linear equation given).

\R\ can add a regression line to our scatter plot if we ask it to.
\begin{center}
<<osa-scatter>>=
xyplot( OSA ~ palprebal, data=xmp12.01, type=c('p','r') )
@
\end{center}

We see that the line does run roughly ``through the middle'' of the dat but that
there is some variability above and below the line.
\end{example}

\subsection{Explanatory and Response Variables Matter}
It is important that 
the explanatory variable be the ``\variable{x}'' variable
and the response variable be the ``\variable{y}'' variable
when doing regression.  If we reverse the roles of \variable{OSA} and \variable{palprebal}
we do not get the same model.  This is because the residuals are measured vertically 
(in the $y$ direction).  


\subsection{Estimating the Response}

We can use our least squares regression line to estimate the value of the response
variable from the value of the explanatory variable.

\begin{example}
If the palprebal width is 1.2 cm, then we would estimate OSA to be 

\[
\hat{\texttt{osa}} = 
\Sexpr{round(coef(osa.model)[1],3)} + \Sexpr{round(coef(osa.model)[2],3)} \cdot 1.2 
= \Sexpr{round(coef(osa.model)[1],3) + round(coef(osa.model)[2],3) * 1.2} 
\]

Of course, \R\ can automate this for us too.  The \function{makeFun()} function will
create a function from our model.  If we input a plaprebal measurement, the function
will return the estimated OSA.
<<>>=
estimated.osa <- makeFun(osa.model)
estimated.osa(1.2)
@

As it turns out, the 17th measurement in our data set had a \variable{palprebal} measurement
of 1.2 cm.
<<>>=
xmp12.01[17,]
@
The corresponding OSA of 3.76 means that the residual for this observation is 
\[
\mbox{observed} - \mbox{predicted} = 3.76 - \Sexpr{estimated.osa(1.2)} = 
\Sexpr{ 3.76 - estimated.osa(1.2)}  
\]
\end{example}

\subsection{Interpreting the Coefficients}
The coefficients of the linear model tell us how to construct the linear function
that we use to estimate response values, but they can be interesting in their own
rite as well.

The intercept $\beta_0$ is the mean response value when the 
explanatory variable is 0.  This may or may not be interesting.
Often $\beta_0$ is not interesting because we are not interested
in the value of the response variable when the predictor is 0.  (That might not 
even be a possible value for the predictor.)  Furthermore, 
if we do not collect data with values of the explanatory variable near 0, then
we will be extrapolating from our data when we talk about the intercept.

The estimates for $\beta_1$, on the other hand, is nearly always of interest.
The slope coefficient $\beta_1$ tells us how quickly the response variable changes 
per unit change in the predictor.  This is an interesting value in many more situations.
Furthermore, when $\beta_1 = 0$, then our model does not depend on the predictor at all.
So when 0 is contained in the confidence interval for $\beta_1$, we do not have sufficient 
evidence to be convinced that our predictor is of any use in predicting the response.

\subsection{Estimating $\sigma$}

There is one more parameter in our model that we have been mostly ignoring so far: $\sigma$ (or 
equivalently $\sigma^2$).  This is the parameter that describes how tightly things should 
cluster around the regression line.  We can estimate $\sigma^2$ from our residuals:

\begin{align*}
\hat\sigma^2 & = MSE = \frac{ \sum_i e_i^2 }{ n -2 }
\\
\hat\sigma & = RMSE = \sqrt{\frac{ \sum_i e_i^2 }{ n -2 } }
\end{align*}
The acronyms $MSE$ and $RMSE$ stand for \term{Mean Squared Error} and 
\term{Root Mean Squared Error}.
The numerator in these expressions is the sum of the squares of the residuals
\[
SSE = \sum_i e_i^2 \;.
\]
This is precisely the quantity that we were minimizing to get our least squares fit.
\[
MSE = \frac{SSE}{DFE} 
\]
where $DFE = n-2$ is the \term{degrees of freedom} associated with the
estimation of $\sigma^2$ in a simple linear model.  We lose two degrees of
freedom when we estimate $\beta_0$ and $\beta_1$, just like we lost 1 degree of
freedom when we had to estimate $\mu$ in order to compute a sample variance.

$RMSE = \sqrt{MSE}$ is listed in the summary output for the linear model as the
\term{residual standard error} because it is the estimated standard deviation of 
the error terms in the model.
<<>>=
summary(osa.model)
@
We will learn more about other parts of this summary output shortly.

Much is known about the estimator $\sigma^2$, including 
\begin{itemize}
	\item $\hat \sigma^2$ is unbaised (on average it is $\sigma^2$, and 
	\item
		the sampling distribution is a Chi-Squared distribution.  (Chi Squared
		distributions are a special case of Gamma distributions.)
\end{itemize}


\section{Checking Assumptions}

\subsection{Don't Fit a Line If a Line Doesn't Fit}

The least squares method can be used to fit a line to any data -- even if a line
is not a useful representation of the relationship between the variables.
When doing regression we should always look at the data to see if a line 
is a good fit.  If it is not, then the simple linear model is not a good choice and 
we should look for some other model that does a better job of describing the 
relationship between our two variables.  

\subsection{Checking the Residuals}

Residuals should also be checked to see that the distribution looks approximately
normal and that that standard deviation remains consistent across the range of
our data (and across time).
Often it is easier to asses the linear fit by looking at a plot of the 
residuals than by looking at the natural scatter plot because on the scale 
of the residuals, discrepancies are easier to see.

\begin{example}
Returning to our OSA data, we can make plots of all the residuals using the 
\function{resid()} function.
<<>>=
xyplot( resid(osa.model) ~ palprebal, data=xmp12.01 )
qqmath( ~ resid(osa.model),  data=xmp12.01 )
@
\end{example}
If the assumptions of the model are correct, there should be no distinct patterns to this scatter
plot (usually called a residual plot) and the normal-quantile plot should be roughly linear
since the model says that differences between observed responses and the true linear fit
should be random noise following a normal distribution with constant standard deviation.

In this case things look pretty good. 

\subsection{Outliers in Regression}

Outliers can be very influential in regression, especially in small data sets,
and especially if they occur for extreme values of the explanatory variable.
Outliers cannot be removed just because we don't like them, but they should be
explored to see what is going on (data entry error? special case? etc.)

Some researchers will do ``leave-one-out'' analysis, or ``leave some out'' analysis
where they refit the regression with each data point left out once.  If the regression
summary changes very little when we do this, this means that the regression line
is summarizing information that is shared among all the points relatively equally.
But if removing one or a small number of values makes a dramatic change, then
we know that that point is exerting a lot of influence over the resulting
analysis (a cause for caution).   

\section{How Good Are Our Estimates?}
Assuming our diagnostics indicate that fitting a linear model is reasonable for our data,
our next question is \emph{How good are our estimates?}
Notice that there are several things we have estimated:
\begin{itemize}
	\item The intercept coefficient $\beta_0$ 
		[estimate: $\hat \beta_0$]
	\item The slope coefficient $\beta_1$ 
		[estimate: $\hat \beta_1$]
	\item Values of $y$ for given values of $x$. 
		[estimate: $\hat y = \hat \beta_0 + \hat \beta_1 x$]
\end{itemize}

We would like to be able to compute uncertainties and confidence intervals for these.
Fortunately, \R\ makes this straightforward.

\subsection{Estimating the $\beta$s}

\begin{example}
	\question
	Returning to the OSA data, compute standard uncertainties and 95\% confidence intervals
	for $\beta_0$ and $\beta_1$.

	\answer
	The \function{summary()} function provides additional information about the 
	model:
<<>>=
summary(osa.model)
@
	We don't know what to do with all of the information displayed here, but we can see
	some familiar things in the coefficient table.  
If we only want the coefficients part of the summary output we can get that using
<<>>=
coef(summary(osa.model))
@
	From this we see the estimates ($\hat \beta$'s)
	displayed again.  Next to each of those is a standard error.  That is the standard
	uncertainty for these estimates.  So we could report our estimated coefficients as 
	\[
	\beta_0: \Sexpr{round(coef(osa.model)[1],2)} \pm 
		\Sexpr{round(sqrt(diag(vcov(osa.model))[1]), 2)}
	\qquad
	\beta_1: \Sexpr{round(coef(osa.model)[2],2)} \pm 
		\Sexpr{round(sqrt(diag(vcov(osa.model))[2]), 2)}
	\]

	A confidence interval can be computed using
	\[
	\hat\beta_i \pm t_* SE_{\beta_i}
	\]
	because 
	\begin{itemize}
		\item
	the sampling distribution for $\hat \beta_i$ 
			is normal, 
		\item
	the sampling distribution for $\hat \beta_i$ 
			is unbiased (the mean is $\beta_i$), and
		\item
			the standard deviation of the sampling distribution depends on $\sigma$ 
			(and some other things), but
		\item
			we don't know $\sigma$, so we have to estimate it using $RMSE = \sqrt{MSE}$.
	\end{itemize}

<<tidy=FALSE>>=
t.star <- qt(.975, df=28); t.star    # n-2 degrees of freedom for simple linear regression
t.star * 0.151
@
	So a 95\% confidence interval for $\beta_1$ is 
	\[
	3.08 \pm \Sexpr{round(t.star * 0.151, 2)}
	\]
	The degrees of freedom used are $DFE = n-2$, the same as used in the estimate of $\sigma^2$.
	(We are using a t-distribution instead of a normal distribution because we don't know 
	$\sigma$.  The degrees of freedom are those associated with using $RMSE = \sqrt{MSE}$
	as our estimate for $\sigma$.)

	\R\ can compute confidence intervals for both parameters using the function
	\function{confint()}:
<<>>=
confint(osa.model)
@
A 68\% confidence interval should have a margin of error of approximately 1 standard
uncertainty:
<<>>=
confint(osa.model, level=0.68)
apply(confint(osa.model, level=0.68) , 1, diff) / 2  # half width of CIs
@

\end{example}

\subsection{Confidence and Prediction Intervals for the Response Value}
We can also create interval estimates for the response.    \R\ will compute
this if we simply ask:
<<>>=
estimated.osa <- makeFun(osa.model)
estimated.osa( 1.2, interval="confidence")
estimated.osa( 0.8, interval="confidence")
@
These intervals are confidence intervals for the \emph{mean} response.  Sometimes it
is desirable to create an interval that will have a 95\% chance of containing a new 
observation.  These intervals are called \term{prediction intervals} to distinguish
them from the usual confidence interval.
<<>>=
estimated.osa <- makeFun(osa.model)
estimated.osa( 1.2, interval="prediction")
estimated.osa( 0.8, interval="prediction")
@
Prediction intervals are typically much wider than confidence intervals.  (It is more challenging
to create an interval that contains a new observation (which might be quite a bit above 
or below the mean) than it is to contain the mean of the distribution.

The widths of both types of intervals depend on the value(s) of the explanatory
variable(s) from which we are making the estimate.  Estimates are more
precise near the mean predictor variable and become less precise as we move
away from there.  Extrapolation beyond the data is both less precise and risky
because we don't have data to know whether the linear pattern seen in the data
extends into that region.

The plot below illustrates both confidence (dotted) and prediction (dashed) 
intervals.
Notice how most of the dots are within the prediction bands, but not within the 
confidence bands.
<<fig.width=".7\\textwidth">>=
xyplot( OSA ~ palprebal, data=xmp12.01, panel=panel.lmbands )
@

\subsubsection{A Caution Regarding Prediction Intervals}
Prediction intervals are much more sensitive to the normality assumption
than confidence intervals are because the Central Limit Theorem does not 
help when we are thinking about individual observations (essentially samples of 
size 1).

\begin{problem}
	Use the output below to answer some questions about rainfall volume  and 
	runoff volume (both in $m^3$) for a particular stretch of a Texas highway.
<<echo=FALSE>>=
TexasHighway <- ex12.16
names(TexasHighway) <- c('rainfall','runoff')
rain.model <- lm(runoff ~ rainfall, data=TexasHighway)
summary(rain.model)
@
	\begin{enumerate}
		\item
			What is the equation for the least squares regression line?
		\item
			Report the slope together with its standard uncertainty.
		\item
			Give a 95\% confidence interval for the slope of this line.
		\item
			What does this slope tell you about runoff on this
			stretch of highway?
		\item
			What is $\hat\sigma$?
	\end{enumerate}
\end{problem}

\begin{solution}
	\begin{enumerate}
\item
	$\variable{runoff} = -1  + 0.83 \cdot \variable{rainfall}$
\item
	$0.83 \pm 0.04$
\item
<<>>=
confint( rain.model, "rainfall" )
@
	We can compute this from the information displayed:
<<tidy=FALSE>>=
t.star <- qt( .975, df=13 ) # 13 df listed for residual standard error
t.star
SE <- 0.0365
ME <- t.star * SE; SE
0.8270 + c(-1,1) * ME  # CI as an interval
@
We should round this using our rounding rules (treating the margin of error like an uncertainty).
\item
	The slope tells us how much run-off there is per amount of rain that falls.
	Since both are in the same units ($m^3$) we can interpret this slope as a
	ratio.  Roughly 83\% of the rain water is being measured as runoff. (Note:
	this last conclusion is warranted because the intercept is essentially 0.)
\item
	$5.24$
	\end{enumerate}
\end{solution}

\begin{problem}
Data from a 1993 study to see how well lichens serve as an indicator for air pollution
are in the \dataframe{ex12.20} data set in the \pkg{Devore6} package.  In that paper,
a simple linear model was fit to see how the wet deposition of $\mathrm{NO}_{3}^{-}$
($g N/m^2$) related to the percentage dry weight of lichen.
\begin{enumerate}
	\item What are the least squares estimates for the intercept and slope of a line
		that can be used to estimate the amount of lichen from deposition?
	\item
		What is the estimated value of $\sigma$?
	\item
		Predict the amount of lichen if the deposition value is
		$0.5$.
	\item
		Give a 95\% confidence interval for the mean amount of lichen
		among samples with a deposition of $0.5\%$.
\end{enumerate}
\end{problem}

\begin{problem}
	The \dataframe{KidsFeet} data set contains variables giving the widths and lengths
	of feet of some grade school kids.
	\begin{enumerate}
		\item
			Perform our usual diagnostics to see whether there are any reasons
			to be concerned about using a simple linear model in this situation.
		\item Based on this data, what estimate would you give for the width of a 
			Billy's foot if Billy's foot is 24 cm long?  
			(Use a 95\% confidence level.)
		\item Based on this data, what estimate would you give for the average width of a 
			kids' feet that 24 cm long?
			(Use a 95\% confidence level.)
		\item
			How precise is this estimate?  
	\end{enumerate}
\end{problem}

\begin{solution}
<<fig.height=3.5>>=
foot.model <- lm( width ~ length, data=KidsFeet)
plot(foot.model, w=1)
plot(foot.model, w=2)
@
Our diagnostics look pretty good.  The residuals look randomly distributed with similar amounts of 
variability throughout the plot.  The normal-quantile plot is nearly linear.

<<>>=
f <- makeFun(foot.model)
f(24, interval="prediction")
f(24, interval="confidence")
@
We can't estimate Billy's foot width very accurately (between 8.0 and 9.6 cm), but we can
estimate the average foot width for all kids with a foot length of 24 cm more accurately (between
8.67 and 8.96 cm).
\end{solution}

\begin{problem}
	Some traffic engineers were interested in study interactions between bicycle and 
	automobile traffic.  One part of the study involved comparing the amount of 
	``available space'' for a bicyclist 
	(distance in feet from bicycle to centerline of the roadway) and 
	``separation distance'' 
	(the average distance between cyclists and passing car, also measured in feet, 
	determined by averaging based on photography over an extend period of time).
	Data were collected at 10 different sites with bicycle lanes.
	The data are available in the \dataframe{ex12.21} data set in the \pkg{Devore6} package.

	\begin{enumerate}
		\item Write out an equation for the least squares regression line for 
			predicting separation distance from available space.
		\item Given an estimate (with uncertainty) for the slope and interpret it.
		\item A new bicycle lane is planned for a street that has 15 feet of available
			space.  Give an interval estimate for the separation distance 
			on this new street.  Should you use a confidence interval or a prediction
			interval?  Why?
		\item
			Give a scenario in which you would use the other kind of interval.
	\end{enumerate}
\end{problem}

\begin{solution}
<<>>=
bike.model <- lm( distance ~ space, data=ex12.21 )
coef(summary(bike.model))
f <- makeFun(bike.model)
f( 15, interval="prediction" )
@
We would use a confidence interval to estimate the average separation distance 
for all streets with 15 feet of available space.
<<>>=
f( 15, interval="confidence" )
@
\end{solution}

\begin{problem}
	Select only the non-diabetic men from the \dataframe{pheno} data set using
<<>>=
men <- subset(pheno, sex=="M" & t2d=="control")  # note the double == and quotes here
head(men,3)
@
This data set contains some phenotype information for subjects in
a large genetics study.  You can find out more about the data set with
<<tidy=FALSE,eval=FALSE>>=
?pheno
@
\begin{enumerate}
	\item
		Using this data, fit a linear model that can be used 
		to predict weight from height.  What is the equation 
		of the least squares regression line?
	\item
		Give a 95\% confidence interval for the slope of this regression
		and interpret it in context.  (Hint: what are the units?)
	\item
		Give a 95\% confidence interval for the mean weight of all 
		non-diabetic men who are 6 feet tall.  
		
		Note the heights are in cm and the weights are in kg, so you will need to convert 
		units to use inches and pounds.  (2.54 cm per inch, 2.2 pounds per kg)
	\item
		Perform regression diagnostics.  Is there any reason to be concerned
		about this analysis?
\end{enumerate}

\end{problem}

\begin{solution}
	\begin{enumerate}
		\item
<<>>=
model <- lm( weight ~ height, data=men )
coef(summary(model))
@
			So the equation is 
			\[
			\variable{weight} = 
			\Sexpr{round(coef(model)[1])} + 
			\Sexpr{round(coef(model)[2],2)} \cdot \variable{height}
			\]
		\item
<<>>=
# we can ask for just the parameter we want, if we like
confint(model, parm="height")
@
The slope tells us how much the average weight (in kg) increases per 
cm of height.
		\item
<<>>=
f <- makeFun(model)
# in kg
f( 6 * 12 * 2.54, interval="confidence" ) 
# in pounds
f( 6 * 12 * 2.54, interval="confidence" ) * 2.2
@
		\item
<<>>=
xyplot( resid(model) ~ fitted(model) )
qqmath( ~ resid(model) )
@
We could also have used
<<fig.height=3.5>>=
plot(model, which=1)
plot(model, which=2)
@
The residual plot looks fine.  There is bit of a bend to the normal-quantile plot, indicating
that the distribution of residuals is a bit skewed (to the right -- the heaviest men are farther above
the mean weight for their height than the lightest men are below).

In this particular case, a log transformation of the weights improves the
residual distribution.  There is still one man whose weight is quite high for
his height, but otherwise things look quite good.
<<>>=
model2 <- lm( log(weight) ~ height, data=men )
coef(summary(model2))
xyplot( resid(model2) ~ fitted(model2) )
qqmath( ~resid(model2) )
@
This model says that
\[
\log( \variable{weight} ) 
	= \Sexpr{round(coef(model2)[1],2)} + \Sexpr{round(coef(model2)[2],4)} \cdot \variable{height}
\]
So
\[
\variable{weight}  
= \Sexpr{round(exp(coef(model2)[1]),1)} 
		\cdot (\Sexpr{round(exp(coef(model2)[2]),3)})^{\variable{height}}
\]
\end{enumerate}
\end{solution}

\begin{problem}
	The \dataframe{anscombe} data set contains four pairs of explanatory 
	(\variable{x1}, \variable{x2}, \variable{x3}, and \variable{x4})
	and response
	(\variable{y1}, \variable{y2}, \variable{y3}, and \variable{y4})
	variables.  These data were constructed by Anscombe 
	\cite{Anscombe:1973:Graphs}.
	\begin{enumerate}
		\item 
			For each of the four pairs, us \R\ to fit a linear model and 
			compare the results.  Use, for example,
<<eval=FALSE>>=
model1 <- lm( y1 ~ x1, data=anscombe ); summary(model1)
@
			Briefly describe what you notice looking at this output.  (You do not have
			to submit the output itself -- let's save some paper.)
		\item
			For each model, create a scatterplot that includes the regression line.
			(Make the plots fairly small and submit them.)
		\item
			Comment on the results.  Why do you think Anscombe invented these data?
			(Use \texttt{fig.width} and \texttt{fig.height} to control the size of the plots.)
	\end{enumerate}
\end{problem}

\begin{solution}
  Anscombe's data show that it is not sufficient to look only at the 
  numerical summaries produced by regression software.  His four data
  sets produce nearly identical output of \verb!lm()! and \verb!anova()!
  yet show very different fits.  An inspection of the residuals (or even
  simple scatterplots) quickly reveals the various difficulties.
\end{solution}

\begin{problem}
	Find an article from the engineering or science literature that uses 
	a simple linear model and report the following information:
	\begin{enumerate}
		\item
			Print the first page of the article (with title and abstract) and write
			a full citation for the article on it.  Staple this at the end of your
			assigment.
		\item
			If the article is available online, provide a URL where it can be found.
			(You can write that on the printout of the first page of the article, too.)
		\item
			How large was the data set used to fit the linear model?  How do you know?  (How 
			did the authors communicate this information?)
		\item
			What are the explanatory and response variables?
		\item
			Did the paper give an equation for the least squares regression line 
			(or the coefficients, from which you can determine the regression equation)?
			If so, report the equation
		\item
			Did the paper show a scatter plot of the data?  Was the regression line 
			shown on the plot?
		\item
			Did the paper provide confidence intervals or uncertainties for the 
			coefficients in the model?
		\item
			Did the paper show any diagnostic plots (normal-quantile, residuals plots, etc.)?
			If not, did the authors say anything in the text about checking that 
			a linear model is appropriate in their situation?
		\item
			What was the main conclusion of the analysis of the linear model?
		\item
				If there is an indication that the data are available online,
				let me know where in case we want to use these data for an example.
	\end{enumerate}
	Google scholar might be a useful tool for this.  JSTOR (available through Heckman
	Library) also has a large number of scientific articles.  Or you might ask an
	engineering or physics professor for an appropriate engineering journal to
	page through in the library.  Since the chances are small that two students
	will find the same article if working independently, I expect to see lots
	of different articles used for this problem.

	If your article looks particularly interesting or contains statistical 
	things that you don't understand but would like to understand, let me know,
	and perhaps we can do something later in the semester with your article.
	It's easiest to do this if you can give me a URL for locating the paper online.
\end{problem}


\newpage
\section*{Exercises}
\shipoutProblems


\ifsolutions
\ifsolutionslocal
\newpage
\section*{Solutions}
\shipoutSolutions
\fi
\fi


\chapter{Non-Linear Relationships}

\section{Two approaches}

Not all relationships are linear.  There are two common ways to deal with
nonlinear relationships:
\begin{enumerate}
	\item
		Transform the data so that there is a linear relationship 
		between the transformed variables.
	\item
		Apply the method of least squares (or maximum likelihood)
		with a non-linear function.
\end{enumerate}


\subsection{Transformations in Linear Regression}
\myindex{transformation!of data}%

The utility of linear models is greatly enhanced through the use of 
various transformations of the data.
There are several reasons why one might consider a transformation of
the predictor or response (or both).

\begin{itemize}
  \item To correspond to a theoretical model.

	Sometimes we have \emph{a priori} information that tells us what 
	kind of non-linear relationship we should anticipate.  As a simple example,
	if we have an apparatus that can accelerate objects by applying a constant
	(but unknown) force $F$, 
	then since $F = ma$, we would expect the relationship between 
	the mass of the objects tested and the acceleration measured to satisfy
	\[
	a = \frac{F}{m}
	\;.
	\]
	This might lead us to fit a model with no intercept 
	(see Exercise~\ref{prob:noIntercept}) after applying an inverse transformation
	to the predictor $m$:
	\[
	a = \beta_1 \cdot \frac{1}{m}
	\;.
	\]
	The parameter $\beta_1$ corresponds to the (unknown) force being applied
	and could be estimated by fitting this model.

	Alternatively, we could use a logarithmic transformation
	\begin{align*}
	\log(a) &= \beta_0 + \beta_1 \log(m) \,,
	\\
	a &= e^{\beta_0}  m^{\beta_1}  \;.
	\end{align*}
	In this model, $e^\beta_0$ corresponds to the unknown force and 
	we can test whether $\beta_1 = -1$ is consistent with our data.

	Many non-linear relationships can be transformed to linearity.  
	Exercise~\ref{prob:transformations} presents several examples
	and asks you to determine a suitable transformation.

  \item To obtain a better fit.

	If a scatterplot or residual plot shows a clearly non-linear pattern 
	to the data, then there is no reason to use the linear fit.  
	In the absence of a clear theoretical model, we may select 
	transformations based on the shape of the relationship as 
	revealed in a scatterplot.  Section~\ref{sec:ladderOfReexpression}
	provides some guidance for selecting transformations in
	this situation.

  \item To obtain better residual behavior.

	Some transformations are used to improve the agreement between the
	data and the assumptions about the error terms in the model.  For example,
	if the variance in the response appears to increases as the predictor 
	increases, a logarithmic or square root transformation of the response will 
	decrease the disparity in variance.  
\end{itemize}

In practice, all three of these issues are intertwined.  A transformation that
improves the fit, for example, may or may not have a good theoretical 
interpretation.  
Similarly, a transformation performed to achieve \term{homoskedasticity} 
\myindex{heteroskedasticity}%
\myindex{homoskedasticity}%
(equal variance; the opposite is called \term{heteroskedasticity}) 
may result in a fit that does not match the overall shape of the data very well.
Despite these potential problems, there are many situations where a relatively
simple transformation is all that is needed to greatly improve the model.

\subsection{Three Important ``Laws"}

\subsubsection{Linear Laws}
We've already talked about linear relationships, but it is worth mentioning them again because there
are so many situations in which a linear relationship arises.

\subsubsection{Power Laws}

Relationships of the form 
\[ y = A x^p \]
are often called power laws.  The two parameters are the exponent $p$ and a constant of proportionality $A$.
Power laws can be linearized by taking logarithms:
\[ \log(y) = \log(A x^p) = \log(A) + p \log(x) \]
So if we fit a model of the form
<<eval=FALSE>>=
lm( log(y) ~ x )
@
Then $\beta_0 = \log(A)$ and $\beta_1 = p$.  
If a power law is a good fit for the data then
<<eval=FALSE>>=
xyplot( log(y) ~ log(x) )
@
will produce a roughly linear plot.

Fitting a power law results in estimates for the parameters $\beta_0 = \log(A)$ and $\beta_1 = p$.
Note that we can use logarithms with any base for this transformation.  Typically natural logarithms are used
(that's what \function{log()} does in \R).  
In some specific applications base 10 logarithms (\function{log10()} in \R) 
or base 2 logarithms (\function{log2()} in \R) 
this yields the commonly used scale for 
$\beta_0 = \log(A)$, the constant of proportionality.

Some common situations that are modeled with power laws include drag force vs speed, velocity vs. force, and
frequency vs. force.

\subsubsection{Exponential Laws}

Relationships of the form 
\[ y = A B^x = A e^{Cx} \]
are often called exponential laws.  
The two parameters are the base $B = e^C$ and a constant of proportionality $A$.
Exponential laws can also be linearized by taking logarithms:
\[ \log(y) = \log(A B^x) = \log(A) + x \log(B) \]
So if we fit a model of the form
<<eval=FALSE>>=
lm( log(y) ~ x )
@
Then $\beta_0 = \log(A)$ and $\beta_1 = \log(B) = C$.  
If an exponential law is a good fit for the data then
<<eval=FALSE>>=
xyplot( log(y) ~ x )
@
will produce a roughly linear plot.

Fitting an exponential law results in estimates for the parameters $\beta_0 = \log(A)$ and $\beta_1 = \log(B) = C$.
Again, we are assuming natural logarithms.

Some common situations that are modeled with exponential laws include 
population growth and radioactive decay.  Note that exponential growth models are typically only good approximations
over a limited range since exponential functions eventually grow quickly, and often some external constraints will
limit this growth.  For example, a culture of bacteria may grow roughly exponentially for a while, but eventually,
limits on space and nourishment will make it impossible for exponential growth to continue.

\subsection{The Ladder of Re-expression}
\label{sec:ladderOfReexpression}%
\label{sec:tukey-bulge}%
\myindex{bulge rule}%
\myindex{ladder of re-expression}%
\myindex{Tukey, J.}%
\myindex{Tukey, J.}%
In the 1970s, Mosteller and Tukey introduced what they called
the \term{ladder of re-expression} and \term{bulge rules} 
\cite{Tukey:1977:EDA,Mosteller:1977:DataAnalysis} that can be used to 
suggest an appropriate transformation to improve the fit when the 
relationship between two variables ($x$ and $y$ in our examples) 
is monotonic and has a single bend.  
Their idea was to apply a power transformation to 
$x$ or $y$ or both -- that is, to work with $x^a$ and $y^b$ for
an appropriate choice of $a$ and $b$.  Tukey called 
this ordered list of transformations the ladder of re-expression.
The identity transformation has power~$1$.
The logarithmic transformation is a special case and is included in the 
list associated with a power of $0$.  
The direction of the required transformation can be obtained
from Figure~\ref{fig:TukeyBulge}, which shows four bulge types,
represented by the curves in each of the four quadrants.  
A bulge can potentially be straightened by 
applying a transformation to one or both variables, moving up 
or down the ladder as indicated by the arrows.  More severe bulges
require moving farther up or down the ladder.  
\authNote{ed: There is a standard ordering of quadrants in mathematics, so labeling
isn't needed.  We could use directions (upper right), but I don't want to add 
quadrant numbers to the figure.  Also in next example.  --- 2010-11-8}%
A curve bulging in the 
same direction as the one in the first quadrant of Figure~\ref{fig:TukeyBulge},
for example, might be straightened by moving up the ladder of transformations
for $x$ or $y$ (or both), while a curve like the one in the second quadrant, 
might be straightened by moving up the ladder for $y$ or down the ladder for $x$.
\begin{figure}
\begin{center}
\begin{tikzpicture}[scale=1.6]
\draw(175:1cm) arc (175:95:1cm);
\draw(85:1cm) arc (85:5:1cm);
\draw(-5:1cm) arc (-5:-85:1cm);
\draw(265:1cm) arc (265:185:1cm);

\draw[->](175:1cm) -- ++(180:0.3cm);
\draw[->](185:1cm) -- ++(180:0.3cm);

\draw[->](95:1cm) -- ++(90:0.3cm);
\draw[->](85:1cm) -- ++(90:0.3cm);

\draw[->](5:1cm) --  ++ (0:0.3cm); 
\draw[->](-5:1cm) -- ++ (0:0.3cm);

\draw[->](265:1cm) -- ++ (270:0.3cm);
\draw[->](275:1cm) -- ++ (270:0.3cm);

\draw(0,2.2) node {\ };
\draw(0,0) + (0:1.3) node[anchor=west]   { move $x$ up };
\draw(0,0) + (270:1.3) node[anchor=north]{ move $y$ down };
\draw(0,0) + (180:1.3) node[anchor=east] { move $x$ down };
\draw(0,0) + (90:1.3) node[anchor=south] { move $y$ up };
\end{tikzpicture}
\hfill
\begin{minipage}{0.35\textwidth}
\vspace*{-1.5in}
\begin{tabular}{rl}
\multicolumn{2}{c}{\textbf{ladder of re-expression}}
\\
\hline
\\[-1mm]
{power} & {transformation}\\
$\vdots$ & $\vdots$ \\
$3$ & $x \mapsto x^3$ \\[1mm]
$2$ & $x \mapsto x^2$ \\[1mm]
%\hline
$1$ & $x \mapsto x$ \\[1mm]
%\hline
$\frac12$ & $x \mapsto \sqrt{x}$ \\[1mm]
$0$ & $x \mapsto \log(x)$ \\[1mm]
$-1$ & $x \mapsto \frac1x$ \\[1mm]
$-2$ & $x \mapsto \frac{1}{x^2}$ \\[1mm]
$\vdots$ & $\vdots$ \\
\end{tabular}
\end{minipage}
\end{center}
%\figureskip
\caption{Bulge rules and ladder of re-expression.}
\label{fig:TukeyBulge}%
\end{figure}
%

This method focuses primarily on transformations designed to improve
the overall fit.  The resulting models may or may not have 
a natural interpretation.  These transformations also affect the 
shape of the distributions of the explanatory and response variables
and, more importantly, of the residuals from the linear model  
(see Exercise~\ref{prob:TukeyBulgeSkew}).
When several transformations lead to reasonable linear fits, these other
factors may lead us to prefer one over another.


\begin{example}
\myindex{Tukey bulge|exampleidx}%
\question
The scatterplot in Figure~\ref{fig:tukey-bulge-example}
shows a curved relationship between
$x$ and $y$.
What transformations of $x$ and $y$ improve the linear fit?
\begin{figure}
<<tukey-bulge1,echo=FALSE>>=
n <- 20  
x <- runif(n,2,10)
y <- exp(0.3*x)
e <- exp(rnorm(n,0,0.1))
y <- y * e
foo <- function(x) {
    a <- x[2]
    x <- x[1]
    if (a == 0) { return (log(x)) }
    return (x^a)
}

power <- function(x,a) {
    M <- cbind(x,a)
    return  (apply(M, 1, foo))
}
powers <- c(0,0.5,1,2,3); np <- length(powers)
a <- rep(rep(powers, each=n),each=np)
b <- rep(rep(powers, each=n), times=np)
x <- rep(x,times=n *np)
y <- rep(y,times=n *np)
X <- power(x,a)
Y <- power(y,b)
original <- (a==1 & b==1)
ddd <- data.frame(X=X,Y=Y,a=a,b=b,original=original)
xyplot(y~x)
@
\caption{A scatterplot illustrating a non-linear relationship between $x$ and $y$.}
\label{fig:tukey-bulge-example}%
\end{figure}

\answer
This type of bulge appears in quadrant IV of Figure~\ref{fig:TukeyBulge},
so we can hope to improve the fit by moving 
up the ladder for $x$ or down the ladder for $y$.
As we see in Figure~\ref{fig:TukeyBulgeMany}, 
the fit generally improves as we move down and to the right -- but not too
far, lest we over-correct.
A $\log$-transformation of the response ($a=1$, $b=0$) seems 
to be especially good in this case. Not only is the resulting relationship
quite linear, but the residuals appear to have a better distribution as well.
\end{example}

\begin{figure}
<<tuky-bulge-many,echo=FALSE,fig.width=6,fig.height=7>>=
xyplot(Y~X|paste('a=',a,sep="") + paste("b=",b,sep=""),
            ddd, groups=original, cex=.7,
			xlab='x', ylab='y',
            scales=list(relation='free', draw=FALSE))
@
\caption{Using the ladder of re-expression to find a better fit.}
\label{fig:TukeyBulgeMany}%
\end{figure}

\begin{example}
\myindex{physics|exampleidx}%
\Rindex{balldrop}%
\label{example:balldrop}%
Some physics students conducted an experiment in which they dropped steel 
balls from various heights and recorded the time until the ball hit the 
floor.   We begin by fitting a linear model to this data.
<<balldrop>>=
ball.model <- lm(time~height,balldrop)
summary(ball.model)
xyplot(time~height,balldrop,type=c('p','r'))
xplot(ball.model,w=1)
@


\myindex{coefficient of determinism|exampleidx}%
At first glance, the large value of $r^2$ and the reasonably good fit 
in the scatterplot might leave us satisfied that we have found a good model.
But a look at the residual plot 
reveals a clear curvilinear pattern in this data.  
A knowledgeable physics student knows that
(ignoring air resistance) the time should be proportional to the 
\emph{square root} of the height.  
This transformation agrees with Tukey's ladder of re-expression, which suggests
moving down the ladder for \variable{height} or up the ladder for \variable{time}.

<<balldrop-transT>>=
ball.modelT <- lm(time ~ sqrt(height),balldrop)
summary(ball.modelT)
xyplot(time~height,balldrop, panel=panel.lm, model=ball.modelT)
xplot(ball.modelT, w=1)
@
This model does indeed fit better, but the residual plot indicates that
there may be some inaccuracy in the measurement of the height.  
In this experiment, the apparatus was set up once for each height and then several observations were made.  So any error in this set-up affected all time measurements for
that height in the same way.  This could explain why the residuals for 
each height are clustered the way they are since it violates the assumption
that the errors are \emph{independent}.  (See Example~\ref{example:balldropRM}
for a simple attempt to deal with this problem.)
\end{example}

%It is also important to note that although $r^2$ is very large in this model,
%it no longer has the usual interpretation because the model does not
%have an intercept term.  Exercise~\ref{prob:r2NoIntercept} explores 
%this further.

\begin{problem}
\Rindex{balldrop}%
In Example~\ref{example:balldrop}, we applied a square root transformation
to the height.  Is there another transformation that yields an even 
better fit?
\end{problem}

\medskip   % to move one line of R output onto next page
\begin{example}
\myindex{physics|exampleidx}%
\label{example:balldropRM}%
One simple way to deal with the lack of independence in the previous example
is to average all the readings made at each height.  
(This works reasonably well in our example because we have nearly
equal numbers of observations at each height.)
We pay for this 
data reduction in a loss of degrees of freedom, but it may be easier to 
justify that the errors in average times at each height are independent 
(if we believe that the errors in the height set-up are independent and 
not systematic).

%\pagebreak
<<balldrop-avg>>=
balldropavg <- aggregate(balldrop$time, by=list(balldrop$height), mean)
names(balldropavg) <- c('height','time')
ball.modelA <- lm(time ~ sqrt(height),balldropavg)
summary(ball.modelA)
xyplot(time~height,balldropavg, panel=panel.lm,model=ball.modelA)
xyplot(resid(ball.modelA) ~ fitted(ball.modelA))
@
Using a square root transformation on averaged \texttt{height}
measurements in the \texttt{balldrop} data gives a similar fit but
a very different residual plot.  The interpretation of this model
is also different.

Notice that the parameter estimates are essentially the same as in 
the preceding example.  The estimate for $\sigma$ has decreased some.
This makes sense since we are now estimating the variability in 
\emph{averaged} measurements rather than in individual measurements.

Of course, we've lost a lot of degrees of freedom, and as a result,
the standard error for our parameter estimate is about twice as 
large as before.  This might have been different; had the mean values
fit especially well, our standard error might have been smaller despite
the reduced degrees of freedom.

One disadvantage of the data reduction is that it is hard to interpret
the residuals (because there are fewer of them).  
At first glance there appears to be a downward trend
in the residuals, but this is largely driven by the fact that the largest
residual happened to be for the smallest fit.  
\end{example}


\begin{example}
\label{example:soap}%
\myindex{soap|exampleidx}%
\Rindex{soap}%
\question
Rex Boggs of Glenmore State High School in Rockhampton, Queensland, had an
interesting hypothesis about the rate at which bar soap is used in the shower.
He writes:
\begin{quote}
    I had a hypothesis that the daily weight of my bar of soap [in grams] 
	in my shower wasn't a linear function, the reason being that the tiny 
	little bar of soap at the end of its life seemed to hang around for just 
	about ever. I wanted to throw it out, but I felt I shouldn't do so until 
	it became unusable. And that seemed to take weeks.

    Also I had recently bought some digital kitchen scales and felt I needed
    to use them to justify the cost. I hypothesized that the daily weight of
    a bar of soap might be dependent upon surface area, and hence would be a
    quadratic function \dots .

The data ends at day 22. On day 23 the soap broke into two pieces and one
piece went down the plughole. 
\end{quote}
The data indicate that although Rex showered daily, he failed to record the 
weight for some of the days.

What do the data say in regard to Rex's hypothesis?

\answer
Rex's assumption that weight should be a (quadratic) function of time 
does not actually fit his intuition.  His intuition corresponds roughly to the 
differential equation
\[
\Partial{t}{W} = k W^{2/3}\,,
\]
for some negative constant $k$ since the rate of change should be 
proportional to the surface area remaining.  
(We are assuming that the bar shrinks in such a way 
that its shape remains proportionally unaltered.)
Solving this equation (by separation of variables) gives
%\[
%\log(W) = k t + C
%\;.
%\]
\[
W^{1/3} = k t + C
\;.
\]
We can fit untransformed and transformed models 
(\verb!Weight^(1/3) ~ Day!) to this data and compare.
<<lm-soap>>=
soap.model1 <- lm(Weight~Day,soap)
summary(soap.model1)
@
The high value of $r^2$ (and the scatterplot in Figure~\ref{fig:soap}, 
darker line) 
indicates that the untransformed model is already a good fit.
<<lm-soap-trans>>=
soap.model2 <- lm(I(Weight^(1/3))~Day,soap)
summary(soap.model2)
@
\begin{figure}
<<echo=FALSE>>=
daysToFit <- seq(1,22,by=0.5)
linfits <- predict(soap.model1,newdata=data.frame(Day=daysToFit))
transfits <- predict(soap.model2,newdata=data.frame(Day=daysToFit))^3
soap.plot <- xyplot(Weight~Day,data=soap, 
    panel = function(x,y,...) {
        panel.xyplot(daysToFit,linfits, lwd=2, type="l", 
            col=trellis.par.get('superpose.line')$col[1])
        panel.xyplot(daysToFit,transfits, lwd=2, type="l",
            col=trellis.par.get('superpose.line')$col[2])
        panel.xyplot(x,y,cex=1.0,...)
    }
    )
@

\caption{Comparing untransformed (darker) and transformed 
(lighter) fits to soap use data.}
\label{fig:soap}%
\end{figure}
%
The transformed model in this case actually fits worse.
The higher value of $r^2$ for the untransformed model is an indication 
that it performs better.  
Figure~\ref{fig:soap} shows a scatterplot
with both fits.  
The data do not support Rex's assumption that a transformation
is necessary.  
We can also fit a quadratic model of the form \verb!Weight~I(Day^2)!,
but this model is worse still.  Fitting a full quadratic model requires 
two predictors (\verb!Day! and \verb!Day^2!) and so will have to wait
until our discussion of multiple linear regression.  
The scatterplot and especially the residual plot both show that the 
residuals are mostly positive near the ends of the data and negative
near the center.  Part of this is driven by a flattening of the pattern
of dots near the end of the measurement period.  Perhaps as the soap
became very small, Rex used slightly less soap than when the soap was
larger.
Exercise~\ref{prob:soap} asks you to remove the last few observations
and see how that affects the models.

Finally, since a linear model appears to fit at least reasonably well
(but see Exercise~\ref{prob:soap}), 
we can give a confidence interval for $\beta_1$, 
the mean amount of soap Rex uses each shower.
\authNoted{check regression diagnostics for soap models -- some cause for worry}%
\authNote{in text reference for quote from Rex? --2010-11-27}%
\Rindex{confint()}%
<<lm-soap-ci>>=
confint(soap.model1)
@
\end{example}


\begin{problem}
\label{prob:soap}%
Remove the last few days from the \dataframe{soap} data set and 
refit the models in Example~\ref{example:soap}.
How much do things change?  Do the residuals look better, or 
is there still some cause for concern?
\end{problem}


\begin{problem}
\myindex{transformation!of data|probidx}%
\label{prob:transformations}%
  For each of the following relationships between a response $y$ and an
  explanatory variable $x$, 
  if possible find a pair of transformations $f$ and $g$ so that
  $g(y)$ is a linear function of $f(x)$:
  \[
  g(y) = \beta_0 + \beta_1 f(x) \;.
  \]
  For example, if 
	  $y = a e^{bx}$, 
	  then $\log(y) = \log(a) + bx$, so 
	  $g(y) = \log(y)$,
	  $f(x) = x$, 
	  $\beta_0= \log(a)$, and 
	  $\beta_1 = b$.

\begin{multicols}{2}
  \begin{enumerate}
	\item 
	  \( y = a b^x \).
	\item 
	  \( y = a x^b \).
	\item
	  \( y = \frac{1}{a + bx} \).
	\item
	  \( y = \frac{x}{a + bx} \).
	\item
	  \(y = a x^2 + b x + c\).
	\item
	  \( \displaystyle y = \frac{1}{1+e^{a+bx}} \).
	  \smallskip
	\item
	  \( \displaystyle y = \frac{100}{1+e^{a+bx}} \).
  \end{enumerate}
\end{multicols}
\end{problem}

\begin{solution}
  \begin{enumerate}
  \item
	$\log(y) = \log(a) + x \log(b)$, 
	so $g(y) = \log(y)$,
	$f(x) = x$, 
	$\beta_0=\log(a)$,
	and $\beta_1=\log(b)$.
  \item
	$\log(y) = \log(a) + b \log(x)$, 
	so $g(y) = \log(y)$,
	$f(x) = \log(x)$, 
	$\beta_0=\log(a)$,
	and $\beta_1=b$.
  \item
	$\frac{1}{y} = a + b x$, 
	so $g(y) = \frac{1}{y}$,
	$f(x) = x$, 
	$\beta_0=a$,
	and $\beta_1=b$.
  \item
	$\frac{1}{y} = \frac{a}{x} + b$, 
	so $g(y) = \frac{1}{y}$,
	$f(x) = \frac{1}{x}$, 
	$\beta_0=b$,
	and $\beta_1=a$.
   \item
	not possible
  \item
	$\frac{1}{y} = 1 + e^{a + bx}$, so
	$\log(\frac{1}{y} - 1) = \log( \frac{1-y}{y} ) = {a + bx}$, 
	so $g(y) = \log(\frac{1-y}{y})$,
	$f(x) = {x}$, 
	$\beta_0=a$,
	and $\beta_1=b$.
  \item
	$\frac{100}{y} = 1 + e^{a + bx}$, so
	$\log(\frac{100}{y} - 1) = \log( \frac{100-y}{y} ) = {a + bx}$, 
	so $g(y) = \log(\frac{100-y}{y})$,
	$f(x) = {x}$, 
	$\beta_0=a$,
	and $\beta_1=b$.
  \end{enumerate}
\end{solution}

\begin{problem}
%\probNote{Would make a good partial solution for students.}%
  What happens to the role of the error terms ($\varepsilon$) when we transform 
  the data?  For each transformation from Exercise~\ref{prob:transformations},
  start with the form
  \[
  g(y) = \beta_0 + \beta_1 f(x) + \varepsilon
  \]
  and transform back into a form involving the untransformed $y$ and $x$ to
  see how the error terms are involved in these transformed linear regression
  models.

  It is important to remember that when we fit a linear model to transformed
  data, the usual assumptions of the model are that the errors in the (transformed) 
  linear form are additive and normally distributed.  The errors may appear
  differently in the untransformed relationship.
\end{problem}

\begin{problem}
\myindex{Tukey bulge|probidx}%
\label{prob:TukeyBulgeSkew}%
The transformations in the ladder of re-expression also affect the shape
of the distribution.  
\begin{enumerate}
\item
If a distribution is symmetric, how does the shape change as we 
move up the ladder?
\item
If a distribution is symmetric, how does the shape change as we 
move down the ladder?
\item
If a distribution is left skewed, in what direction should we move to 
make the distribution more symmetric?
\item
If a distribution is right skewed, in what direction should we move to 
make the distribution more symmetric?
\end{enumerate}
\end{problem}

\begin{solution}
Moving up the ladder will spread the larger values more than the 
smaller values, resulting in a distribution that is right skewed.
\end{solution}

\begin{problem}
\Rindex{pendulum}%
\myindex{physics|probidx}%
By attaching a heavy object to the end of a string, 
it is easy to construct pendulums of different lengths.  Some physics students
did this to see how the period (time in seconds until a pendulum
returns to the same location) depends on the length (in meters) 
of the pendulum.  
The students constructed pendulums of lengths varying from
$10$ cm to $16$ m and recorded the period length (averaged over several
swings of the pendulum).  The resulting data are in
the \dataframe{pendulum} data set.

\begin{enumerate}
	\item
		Fit a power law to this data using a transformation and
		a linear model.  
		How well does the power law fit?  
		What is the estimated power in the power law based on this model?
\item
Fit a power law to this data using a nonlinear model.
How well does the power law fit?  
What is the estimated power in the power law based on this model?

\item
	Compare residual plots and normal-quantile plots for the residuals
	for the two models.  How do the models compare in this regard?
\end{enumerate}
\end{problem}

\begin{solution}
	At first glance, the two models might appear equally good.  In each case the 
	power is a bit below 2 and the fits look good on top of the raw data.  (Note: the 
	function produced by \function{makeFun()} does not know how to invert the log
	transformation on the response variable, so we have to do that ourselves.)
<<warning=FALSE>>=
model <- lm( log(period) ~ log(length), data=pendulum )
model2 <- nls( period ~ A * length^power, data=pendulum, start=list(A=1, power=2) )
f <- makeFun(model)
g <- makeFun(model2)
xyplot(period ~ length, data=pendulum)
plotFun(exp(f(x)) ~ x, col='gray50',add=TRUE)
xyplot(period ~ length, data=pendulum)
plotFun(g(x) ~ x, col='red', lty=2, add=TRUE)
coef(summary(model))
coef(summary(model2))
@
	But if we look at the residuals, we see that the linear model is clearly 
	better in this case.  The non-linear model suffers from heteroskedasticity.
<<>>=
xyplot(resid(model) ~ fitted(model), type=c('p','smooth'))
xyplot(resid(model2) ~ fitted(model2), type=c('p','smooth'))
@
	Both residual distributions are reasonably close to normal, but not perfect.
	In the ordinary least squares model, the largets few residuals are not as large
	as we would expect.
<<>>=
qqmath(~resid(model))
qqmath(~resid(model2))
@
	
	The residuals in the non-linear model show a clear change 
	in variance as the fitted value increases.  This is counteracted by the logarithmic
	transformation of the explanatory variable.  (In other cases, the non-linaer model
	might have the preferred residual distribution.)
\end{solution}



\begin{problem}
The \dataframe{pressure} data set contains 
data on the relation between temperature in degrees Celsius and 
vapor pressure in millimeters (of mercury).
Using temperature as the predictor and pressure as the response,
use transformations (if necessary) to obtain a good fit.
Make a list of all the models you considered and explain
how you chose your best model.
What does your model say about the relationship between 
pressure and temperature?
\end{problem}

\begin{problem}
\Rindex{cornit}%
\Rindex{faraway}%
The \dataframe{cornnit} data set in the package \pkg{faraway} 
contains data from a study investigating the relationship between 
corn yield (bushels per acre) and nitrogen (pounds per acre) fertilizer 
application in Wisconsin.
Using nitrogen as the predictor and corn yield as the response,
use transformations (if necessary) to obtain a good fit.
Make a list of all the models you considered and explain
how you chose your best model.
\end{problem}



\begin{problem}
\Rindex{actgpa}%
\myindex{ACT}%
\myindex{grade point average}%
The data set \verb!actgpa!
%  \url{http://www.calvin.edu/~stob/courses/m344/S07/data/actgpanona.csv}
contains the ACT composite scores and GPAs of some randomly selected seniors 
at a Midwest liberal arts college.

  \begin{enumerate}
	\item
	  Give a 95\% confidence interval for the mean ACT score 
	  of seniors at this school.
	\item
	  Give a 95\% confidence interval for the mean GPA 
	  of seniors at this school.
	\item
	  Use the data to estimate with 95\% confidence 
	  the average GPA for all students who score 25 on the ACT.
	\item
	  Suppose you know a high school student who scored 30 on the ACT.
	  Estimate with 95\% confidence his GPA as a senior in college.
	\item
	  Are there any reasons to be concerned about the analyses you
	  have just done?  Explain.
  \end{enumerate}
\end{problem}

\begin{solution}
	\begin{enumerate}
		\item
			We can build a confidence interval for the mean by fitting 
			a model with only an intercept term.
<<act-gpa, tidy=FALSE>>=
grades <- actgpa
confint(lm(ACT ~ 1, data=grades))
@
But this isn't the only way to do it.  Here are some other ways.
<<>>=
# here's another way to do it; but you don't need to know about it
t.test(grades$ACT)
# or you can do it by hand
x.bar <- mean(~ACT, data=grades); x.bar
n <- nrow(grades); n
t.star <- qt(.975, df=n-1); t.star
SE <- sd(~ACT, data=grades) / sqrt(n); SE
ME <- t.star * SE; ME
@
			So the CI is $\Sexpr{x.bar} \pm \Sexpr{ME}$.  Of course, that is too many digits, we 
			should do some rounding to
			$\Sexpr{round(x.bar,1)} \pm \Sexpr{round(ME,1)}$.  
		\item
<<>>=
confint(lm(GPA ~ 1, data=grades))
# this could also be done the other ways shown above.
@
		\item
<<>>=
grades.model <- lm(GPA~ACT,grades)
f <- makeFun(grades.model)
f(ACT=25, interval="confidence")
@
		
		\item
<<>>=
f(ACT=30, interval="prediction")
@
		\item
<<>>=
xyplot(GPA~ACT,data=grades,panel=panel.lmbands)
xyplot(resid(grades.model) ~ fitted(grades.model),type=c('p','smooth'))
@
			There are no major concerns with the regression model. The
			residuals look pretty good.  (There is perhaps a bit more variability
			in GPA for the lower ACT scores and if you said you were worried about
			that, I would not argue.)  

			The prediction intervals are very wide and hardly useful, however.
			It's pretty hard to give a precise estimate for an individual
			person -- there's just too much variability from preson to person, 
			even among people with the same ACT score. 
	\end{enumerate}
\end{solution}

\begin{problem}
\Rindex{drag}%
\myindex{physics|probidx}%
\label{prob:drag1}
In the absence of air resistance, a dropped object will continue to accelerate
as it falls.  But if there is air resistance, the situation is different.
The drag force due to air resistance depends on the velocity of an object
and operates in the opposite direction of motion.  Thus as the object's velocity
increases, so does the drag force until it eventually equals the force
due to gravity.  At this point the net force is $0$ and the object 
ceases to accelerate, remaining at a constant velocity called the 
terminal velocity.
\myindex{terminal velocity|probidx}

Now consider the following experiment to determine how terminal velocity
depends on the mass (and therefore on the downward force of gravity) of 
the falling object.  A helium balloon is rigged with a small basket and 
just the right ballast to make it neutrally buoyant.  Mass is then added
and the terminal velocity is calculated by measuring the time it takes to
fall between two sensors once terminal velocity has been reached.

The \verb!drag! data set contains the results of such an experiment
conducted by some undergraduate physics students.  Mass is measured 
in grams and velocity in meters per second.  
(The distance between the two sensors used for determining
terminal velocity is given in the \verb!height! variable.)

By fitting models to this data, determine which of the following ``drag laws'' matches the data best:
\begin{itemize}
\item
Drag is proportional to velocity.
\item
Drag is proportional to the square of velocity.
\item
Drag is proportional to the square root of velocity.
\item
Drag is proportional to the logarithm of velocity.
\end{itemize}
\end{problem}

\begin{solution}
The best of these four models is a model that says drag is proportional
to the square of velocity.
Given the design of the experiment, it makes the most sense to fit
velocity as a function of drag force.  Here are several ways we could 
do the fit:
<<drag>>=
model1 <- lm(velocity^2 ~ force.drag, drag)
model2 <- lm(velocity ~ sqrt(force.drag), drag)
model3 <- lm(log(velocity) ~ log(force.drag), drag)
@
<<>>=
coef(summary(model1))
@
<<>>=
coef(summary(model2))
@
<<>>=
coef(summary(model3))
@
Note that \verb!model1!, \verb!model2!, and \verb!model3! are not equivalent, 
but they all tell roughly the same story.
<<warning=FALSE>>=
xyplot( velocity ~ force.drag, data=drag)
f1 <- makeFun(model1)
f2 <- makeFun(model2)
f3 <- makeFun(model3)
plotFun( sqrt(f1(x)) ~ x, add=TRUE, alpha=.4 )
plotFun( f2(x) ~ x, add=TRUE, alpha=.4, col='red', lty=2 )
plotFun( exp(f3(x)) ~ x, add=TRUE, alpha=.4, col='brown', lty=3 )
@

The fit for these models reveals some 
potential errors in the design of this experiment.  Separating out the data
by the height used to determine velocity suggests that perhaps some of the 
velocity measurements are not yet at terminal velocity.
In both groups, the velocities for the greatest drag forces are 
not as fast as the pattern of the remaining data would lead us to expect.

<<drag-plots>>=
xyplot(velocity^2 ~ force.drag, drag, groups=height)
xplot(model1,w=1)
xyplot(velocity ~ force.drag, drag, scales=list(log=T),groups=height)
xplot(model3,w=1)
@
\end{solution}

\begin{problem}
\Rindex{drag}%
\myindex{physics|probidx}%
\label{prob:drag-problems}%
Construct a plot that reveals a likely systematic problem with the 
\verb!drag! (see Exercise~\ref{prob:drag1}) data set.
Speculate about a potential cause for this.
\end{problem}

\begin{solution}
See previous problem.
\end{solution}

\begin{problem}
\Rindex{drag}%
\myindex{physics|probidx}%
Exercise~\ref{prob:drag-problems} suggests that some
of the data should be removed before analyzing the \verb!drag! data set.
Redo Exercise~\ref{prob:drag1} after removing this data.
\end{problem}

\begin{problem}
\Rindex{spheres}%
\myindex{physics|probidx}%
The \verb!spheres! data set contains measurements of the diameter (in meters)
and mass (in kilograms) of a set of steel ball bearings.  We would expect
the mass to be proportional to the cube of the diameter.  Fit a model 
and see if the data reflect this.
\end{problem}


\begin{problem}
\Rindex{spheres}%
\myindex{physics|probidx}%
The \verb!spheres! data set contains measurements of the diameter (in meters)
and mass (in kilograms) of a set of steel ball bearings.  We would expect
the mass to be proportional to the cube of the diameter.  
Using appropriate transformations fit two models: one that predicts 
mass from diameter and one that predicts diameter from mass.  
How do the two models compare?
\end{problem}

\begin{problem}
\label{prob:utilities-therms-by-temp}%
\Rindex{utilities}%
The \verb!utilities! data set has information from utilities
bills at a Minnesota residence.
Fit a linear model that predicts \variable{thermsPerDay} from \variable{temp}.
\begin{enumerate}
\item
What observations should you remove from the data before doing the 
analysis? Why?  
\item
Are any transformations needed?
\item
How happy are you with the fit of your model?  Are there any reasons
for concern?
\item
Interpret your final model (even if it is with some reservations listed in
part c)).  
What does it say about the relationship between average monthly temperature 
and the amount of gas used at this residence?  What do the parameters represent?
\end{enumerate}
\end{problem}


\subsection{Nonlinear Least Squares}

Another approach to non-linear relationships is called \term{nonlinear least squares}
or \term{nonlinear regression}.  In this approach, instead of attempting to transform
the relationship until it becomes linear, we fit a nonlinear function by minimizing the 
the sum of the squared residuals relative to that (paramterized) nonlinear function (form).

The \R\ function for fitting these
models is \function{nls()}.  This functions works much like \function{lm()},
but there are some important differences:
	\begin{enumerate}
		\item
			Because the model does not have to be linear, we have
			to use a more verbose description of the model.
		\item
			Numerical optimization is used to fit the model, and the algorithm
			used needs to be given a reasonable starting point for its search.
			Specifying this starting point simultaneously lets \R\ know what the 
			parameters of the model are.  (Each quantity with a starting value
			is considered a parameter, and the algorithm will adjust all the parameters
			looking for the best fit -- i.e., the smallest MSE (and hence also
			the smallest SSE nd RMSE).
	\end{enumerate}

Let's illustrate with an example.

\begin{example}
	Returning to the ball dropping experiment, let's fit 
	\begin{align}
	\variable{time} &= \alpha_0 + \alpha_1 \sqrt{\variable{time}}
	\label{eq:balldrop}
	\end{align}
	using nonlinear least squares.  
<<>>=
nls.model <- nls( time ~ alpha0 + alpha1 * sqrt(height), data=balldrop, start=list(alpha0=0, alpha1=1) )
@
	Notice how the model formula compares with the formula in (\ref{eq:balldrop}).
	The starting point for the algorithm is specified with 
	\code{start=list(alpha0=0, alpha1=1)}, which also declares that 
	the parameters to be fit.

	We can obtain the coefficients of the fitted model with
<<>>=
nls.model
@
or
<<>>=
coef(nls.model)
@
A more complete summary can be obtained by
<<>>=
summary(nls.model)
@
We can restrict our attention to the coefficients table with
<<>>=
coef(summary(nls.model))
@

<<>>=
f <- makeFun(nls.model)
xyplot( time ~ height, data=balldrop )
plotFun( f(height) ~ height, add=TRUE, col='gray40' )
xyplot( resid(nls.model) ~ fitted(nls.model) )
@

We can compare this to the ordinary least squares model by plotting both together on the same plot.
<<>>=
lm.model <- lm( time ~ sqrt(height), data=balldrop)
g <- makeFun(lm.model)
xyplot( time ~ height, data=balldrop )
plotFun( f(height) ~ height, add=TRUE, col='gray40', lwd=3 )
plotFun(g(height) ~ height, add=TRUE, col='red', lwd=1, lty=2)
@
In this particular case, there is very little difference between the two models, but this is not always the case.
<<>>=
coef(nls.model)
coef(lm.model)
@
\end{example}

\begin{example}
	Here is example where we fit a different model to the \dataframe{balldrop} data, namely
	\[
	\variable{time} = \alpha * \variable{height}^p
	\]
<<tidy=FALSE>>=
power.model <- nls( time ~ alpha * height^power, data=balldrop, 
                    start=c(alpha=1, power=.5) )
coef(summary(power.model))
@
\end{example}


\begin{example}
	A professor at Macalester College put hot water in a mug and recorded the temperature as it cooled. 
	Let's see if we can fit a reasonable model to this data
<<>>=
xyplot( temp ~ time, data=CoolingWater, ylab="temp (C)", xlab="time (sec)")
@

Our first guess might be some sort of exponential decay
<<tidy=FALSE>>=
cooling.model1 <- nls( temp ~ A * exp( -k * time), data=CoolingWater, 
                       start=list(A=100, k=0.1) )
f1 <- makeFun(cooling.model1)
xyplot( temp ~ time, data=CoolingWater, xlim=c(-50,300), ylim=c(0,110), 
        ylab="temp (C)", xlab="time (sec)")
plotFun( f1(time) ~ time, add=TRUE)
@

That doesn't fit very well, and there is a good reason.  The model says that eventually the water will freeze because
\[
\lim{t \to \infty} A e^{-k t} = 0
\]
when $k >0$.  But clearly our water isn't going to freeze sitting on a lab table.  We can fix this by 
adding in an offset to account for the ambient temperature:
<<tidy=FALSE>>=
cooling.model2 <- nls( temp ~ ambient + A * exp( k * (1+time) ), data=CoolingWater,
                      start=list(ambient=20, A=80, k=-.1) )
f2 <- makeFun(cooling.model2)
xyplot( temp ~ time, data=CoolingWater, xlim=c(-50,300), ylim=c(0,110),
        ylab="temp (C)", xlab="time (sec)")
plotFun( f1(time) ~ time, add=TRUE, lty=2, col="gray80")
plotFun( f2(time) ~ time, add=TRUE, col = "steelblue")
@
This fits much better.  Furthermore, this model can be derived from a differential equation
\[
\frac{dT}{dt} = -k (T_0 - T_{\mathrm{ambient}})
\;,
\]
known as Newton's Law of Cooling.
\myindex{Newton's Law of Cooling}%

Let's take a look at the residual plot
<<>>=
xyplot( resid(cooling.model2) ~ time, data=CoolingWater ) 
plot(cooling.model2, which=1)
@
Hmm.  These plots show a clear pattern and very little noise.
The fit doesn't look as good when viewed this way.  
It suggests that Newton's Law of Cooling does not take into account all that is going on here.
In particular, there is a considerable amount of evaporation (at least at the beginning when the 
water is warmer).  More complicated models that take this into account can fit even better.
For a discussion of a model that includes evaporation, 
see \url{http://stanwagon.com/public/EvaporationPortmannWagonMiER.pdf}.\footnote{
The model with evaporation add another complication in that the resulting differential equation
cannot be solved algebraically, so there is no algebraic formula to fit with \function{nls()}.  But 
the method of least squares can still be used by creating a parameterized numerical function that computes
the sum of squares and using a numerical minimizer to find the optimal parameter values.  Since the 
use of numerical differential equation solvers is a bit beyond the scope of this course, we'll leave that
discussion for another day.}
\end{example}


\subsection{Estimating Uncertainty}

Both the linear and nonlinear models provide us with estimated uncertainty (standard errors).  These should 
be interpreted in a context that is assuming the model being fit is reasonable.  It doesn't make sense to 
put much stock in the uncertainty of a linear fit when the relationship is clearly non-linear, for example.
First we should use transformation or nonlinear models to obtain a better fit.

But once we have a reasonable fit, we can produce a variety of estimated uncertainties (standard errors) 
and confidence intervals.

\begin{example}
	\label{example:estimate-ambient}%
	\question
	Returning to the cooling water example, give an estimate for the ambient temperature.

	\answer
	\R\ will help us with the computations
<<>>=
coef(summary(cooling.model2))
@
	So we could report an estimate $\pm$ uncertainty as 
	\[
	\Sexpr{round(coef(cooling.model2)[1],1)} \pm
	\Sexpr{round(coef(summary(cooling.model2))[1,2],1)} 
	\mbox{degrees C}
	\]
	Alternatively, we could construct a 95\% confidence interval:
<<>>=
confint(cooling.model2, level=0.95)
@

We could use the same procedure to estimate the other parameters in the model as well.
\end{example}

\begin{problem}
	\begin{enumerate}
		\item
			Use the output from Example~\ref{example:estimate-ambient} to give an estimate for the heat 
			exchange constant $k$ in estimate $\pm$ uncertainty form.
		\item 
			Use the output from Example~\ref{example:estimate-ambient} to compute
			a 95\% confidence interval for the heat exchange constant $k$.
	\end{enumerate}
\end{problem}

\begin{solution}
<<tidy=FALSE>>=
coef(summary(cooling.model2))
k <- coef(summary(cooling.model2))["k","Estimate"]; k
SE <- coef(summary(cooling.model2))["k","Std. Error"]; SE
t <- coef(summary(cooling.model2))["k","t value"];t 
t.star <- qt(.975, df= 222-3); t  # 222 observations - 3 parameters
t.star * SE
2 * pt ( t, df=222-3)             # degrees of freedom check (should match p-value above)
@
	\begin{enumerate}
		\item
			$-0.0210 \pm 0.0003 = (-0.0213, -0.207)$
		\item
			$-0.0210 \pm \Sexpr{round(t.star * SE,5)} = (-0.0217, -0.0203)$  
	\end{enumerate}
\end{solution}

\iffalse
\begin{example}
	Sometimes the quantity we are most interested in is not one of the parameters of the model but a combination
	of the parameters.  
\end{example}

The \function{deltaMethod()} function in \pkg{car} package can be used to make standard error calculations
via the Delta Method.
\fi


\section{Higher Order Terms and Interaction}

see the handout from class (also available at
\url{http://dl.dropboxusercontent.com/u/5098197/Math135/In-Class/poly2d.pdf}).
But remember that transformations can be used to introduce monotonic relationships
that are not linear, so the primary reason for a quadratic terms is a non-monotonic
relationship.

\begin{problem}
	Choose any three scenarios (but not the bicycle one we did in class)
	from the polynomial models in 2 variables handout 
	(\url{http://dl.dropboxusercontent.com/u/5098197/Math135/In-Class/poly2d.pdf})
	and answer the following questions:
	\begin{enumerate}
		\item
			What is the response variable?
		\item
			What are the candidate explanatory varaibles?
		\item
			Are any of these variables categorical?
		\item
			Which terms would you include in a linear model? Explain your choice.
	\end{enumerate}
\end{problem}

\section{Hypothesis Testing}

\begin{description}
	\item[hypothesis] A statement that can be true or false.
	\item[statistical hypothesis] A hypothesis about a parameter or parameters.
\end{description}

\begin{examples}
	The following are examples of null hypotheses.
	\begin{enumerate}
		\item
			$\mu = 0$.  (The population mean is 0.)
		\item
			$\beta_1=0$.  (The ``true'' slope is 0 -- assuming a model like $\E(Y) = \beta_0 + \beta_1 x$.)
		\item
			$\beta_1 = \beta_2$ (Two parameters in the model are equal.)
		\item
			$\beta_2 = \beta_3 = 0$ (Two parameters in the model are both equal to 0.)
	\end{enumerate}
\end{examples}

Hypothesis testing generally follows a four-step process.
\begin{enumerate}
	\item 
		State the null and alternative hypotheses.

		The null hypothesis is on trial and innocent until proven guilty.  We will
		render one of two verdicts:  Reject the null hypothesis (guilty) or do not reject
		the null hypothesis (not guilty).  

	\item
		Compute a test statistic.

		All the evidence against the null hypothesis must be summarized in a single number called 
		the test statistic.

	\item
		Determine the p-value.

		The p-value is a probability:  Assuming the null hypothesis is true, how likely are we to
		get at least as much evidence against it (i.e., a test statistic as anusual as the one observed)
		just by random chance?

	\item
		Interpret the results.

		If the p-value is small, then one of two things is true:
		\begin{enumerate}
			\item The null hypothesis is true and something unlikely occurred in our sample, or
			\item The null hypothesis is false (and the p-value is a counter-factual probability).
		\end{enumerate}
		For this reason we consider small p-values as evidence against the null hypothesis.
\end{enumerate}

\subsection{T-tests}

Many hypothesis tests are based on a $t$-distribution.  These tests all use a similar sort
of test statistic:

\[
t = \frac{\mbox{estimate} - \mbox{hypothesized value}}{\mbox{standard error}}
\]

The numerator tells us that the more the estimate and the hypothesized value differ, 
the stronger the evidence.  The denominator tells us that differences mean more when the standard
deviation is small than when the standard deviation is large.

The test statistic is converted to a p-value by comparing it to the t-distribution with appropriate
degrees of freedom.  For linear models, this is the degrees of freedom associated with the 
residual standard error.

\subsubsection{The 1-sample t-test}
The 1-sample t-test is not particularly well named, but it tests the null hypothesis
\begin{itemize}
	\item $H_0: \mu = \mu_0$, vs.
	\item $H_a: \mu \neq 0$.
\end{itemize}
That is, it tests whether there is evidence that the mean of some population ($\mu$) is different
from some hypothesized value ($\mu_0$).

\begin{example}
Let's look at some data on weight loss programs.  In this data set, there were two groups.  One group
received a monetary incentive if they lost weight.  The controls
did not receive a monetary incentive.  Let's see whether on average the controls lost weight
<<>>=
require(Stat2Data)
data(WeightLossIncentive)
Controls <- subset(WeightLossIncentive, Group=="Control")
favstats( ~ WeightLoss, data=Controls )
@
The standard error when doing inference for a mean is 
\[
SE = \frac{s}{\sqrt{n}} = 
\frac{ 
	\Sexpr{sd(~WeightLoss, data=Controls)}
}{
	\sqrt{ \Sexpr{nrow(Controls)} } 
}
=
\Sexpr{sd(~WeightLoss, data=Controls)/sqrt(nrow(Controls))} 
\]
<<tidy=FALSE>>=
SE <- 9.108 / sqrt(19); SE
@
If we want to test our null hypothesis, then we compute a t-statistic:
<<tidy=FALSE>>=
t <- (3.92 - 0) / SE; t
@
and from this a p-value, which is the tails probability for a t-distribution with 18 degrees of freedom.
<<>>=
1 - pt( t, df=19-1 )
2 * ( 1 - pt( t, df=19-1 ) )
@
This p-value can also be generated from a linear model.
<<>>=
model <- lm ( WeightLoss ~ 1, data = Controls )    # only an intercept term in the model
coef(summary(model))
@
What is this model?
	\[
	Y = \beta_0 + \varepsilon
	\]
So $\beta_0$ is the mean value of the response (\variable{WeightLoss}).

Either way, our p-value is $0.077$ (1 or 2 digits are sufficient for reporting p-values).  This is not
compelling evidence that the weight loss program actually leads to a change in weight.  A change this
large could occur just by chance in nearly 8\% of samples.
\end{example}


\subsection{Testing a Model Coefficient}

\begin{example}
	Now let's compare the controls to the incentive group.  That is the question 
	the study was primarily interested in.
<<>>=
model2 <- lm ( WeightLoss ~ Group, data=WeightLossIncentive )
@
	What is this model?
	\[
	Y = \beta_0 + \beta_1 x + \varepsilon
	\]
where $x$ is 0 for the controls and 1 for the incentive group.  That is, $Y \sim \Norm(\beta_0, \sigma)$
for the control group and $Y \sim \Norm(\beta_0 + \beta_1, \sigma)$ for the incentive group.
A test of the null hypothesis $\beta_1 = 0$ tests whether the two groups had different amounts 
of weight loss (on average).   (Note that this model also assumes that the two groups have the same 
standard deviation.  There are ways to test this hypothesis without that assumption.)
<<>>=
favstats( WeightLoss ~ Group, WeightLossIncentive )
coef(summary( model2 ) )
@
The small p-value provides evidence against the null hypothesis.  That is, we have evidence that
there is a difference between the mean amount of weight lost in the two groups.  (Those with a monetary
incentive lost more.)  
\end{example}

A difference large enough cause us to reject a null hypothesis of ``no difference" 
is called a \term{statistically significant} difference.  Differences may fail to be statistically
significant if they are small, if they are masked by underlying variability, or if there is too 
little data.  Good studies will collect enough data and work to reduce variability (if that is possible)
in order to have a reasonable expectation of detecting differences if they are large enough to 
be scientifically interesting.

\begin{example}
Suppose you suspect that drag force should be proportional to the square of velocity.  Let's see
if that is consistent with the data collected by some physics students.  We'll fit a power law model\footnote{There is some evidence in this data that some of the observations did not reach
critical velocity.  It would be good to refit this data with those observations removed from the data.}

\[
\variable{force.drag} = A \cdot \variable{velocity}^p
\]
and test the hypothesis that $p=2$.  We can fit this model using a log-log transformation:
\[
\log(\variable{force.drag}) = \log(A)  +  p \log( \variable{velocity} )
\]
So $p=\beta_1$ in our usual linear model notation.

<<drag-again>>=
drag.model <- lm(log(force.drag) ~ log(velocity), data=drag)
summary(drag.model)
xyplot(log(force.drag) ~ log(velocity), data=drag)
@
None of the p-values produced in this output is not the one we want.  That is testing the null hypothesis
$\beta_1 = 0$.  But we can easily calculate the p-value we want since we have the standard error
and degrees of freedom.
<<tidy=FALSE>>=
beta1.hat <- 2.051
SE <- 0.05366
t <-  ( beta1.hat - 2 ) / SE; t
2 * pt( - abs(t), df= 40 )
@
With this large a p-value, we cannot reject the null hypothesis that $p=2$.  This does not prove
that $p=2$, but it does say that our data are consistent with that value.
\end{example}

\iffalse
\begin{example}
This time let's consider a model with a quantitative predictor.
<<>>=
data(PorschePrice)
head(PorschePrice)
porsche.model <- lm( Price ~ Mileage, data=PorschePrice )
summary(porsche.model)
xyplot( resid(porsche.model) ~ fitted(porsche.model), type=c('p','smooth'))
qqmath( ~ resid(porsche.model) )
@
The model looks reasonable.  What are the two hypotheses being tested?
\begin{enumerate}
	\item $H_0: \beta_0 = 0$.

		Often this is not an interesting test because often we are not so interested in
		$\beta_0$, and especially not in whether it is 0.  In this case, the intercept might
		be interesting because it tells us the price of a Porsche with no miles.  On the other hand,
		we might not expect a used car, even one with very few miles to fit the same pattern as 
		a new car.  There is probably a loss in value that occurs as soon as a car is purchased.
		
		In any case, it is clear that the intercept will not be 0; we don't need a hypothesis test
		to tell us that.  Indeed, the evidence is incredibly strong.

	\item
		$H_0: \beta_1 = 0$.

		There is strong evidence against this hypothesis as well.  This is also not surprising.
		If $\beta_1 = 0$, this means that the price of the cars does not depend on the mileage.

		A test of $\beta_1=0$ in a simple linear model is often called the \term{model utility test}
		because it is testing whether the predictor is of any use to us or not.
\end{enumerate}
\end{example}
\fi

\subsection{Connection to Confidence Intervals}

There is a natural duality between t-based hypothesis tests and confidence intervals.
Since the p-value is computed using tail probabilities of the t-distribution and confidence
level describes the central probability, the p-value will be below 0.05 exactly when the 
hypothesized value is not contained in the 95\% confidence interval.  (Similar statements can
be made for other confidence levels.)
So a confidence interval generally conveys more information than a p-value in these situations.

\begin{example}
	The output below illustrates this duality.
<<>>=
drag.model
confint(drag.model)
@
Since the confidence interval for $\beta_1=p$ include 2, 2 is a plausible value for the power (i.e.,
consistent with our data).
A p-value larger than 0.05 says the same thing at the same level of confidence.  
\end{example}

\section{Categorical Predictors with More than Two Levels}
Categorical variables with two possible values (usually called \term{levels}) can be included
as predictors in a linear model using a coding scheme where one level is coded as 0 and the other as 1.
A similar thing could be done for categorical variables with more than two levels -- coding the levels
as 0, 1, 2, 3, etc. -- but that is not the usual coding scheme.  The problem with that coding
scheme is that model forces some things we don't usually want to force.

Consider the model
\[
Y = \beta_0 + \beta_1 x + \varepsilon
\]
where $x \in \set{0,1,2,3}$ (coding a categorical variable with 4 levels).  This implies that
means for the 4 groups are 
\begin{center}
\begin{tabular}{cl}
	\hline
	Group & Group Mean \\ \hline
	0 & $\beta_0$ \\
	1 & $\beta_0 + \beta_1$ \\
	2 & $\beta_0 + 2 \beta_1$ \\
	3 & $\beta_0 + 3 \beta_1$ \\
	\hline
\end{tabular}
\end{center}
This implies that
\begin{enumerate}
	\item Each group is normally distributed with a common standard deviation but potentially different
		means.
	\item The means are ordered in the same order as the levels. 
	\item The increase in mean from one group to the next is the same amount as we move 
		from group 0 to group 1 as it is moving from group 1 to group 2 or group 3 to group 4.
\end{enumerate}
Items 2 and 3 are overly restrictive, and often unreasonable assumptions.  The
usual coding scheme avoids these assumptions.  (Item 1 can also be problematic,
and if a transformation doesn't fix the problem, then there are other methods
that avoid this assumption.)

Let's take a look at an example.  

\begin{example}
	The \dataframe{bugs} data set contains data from an experiment conducted to see
	if a certain kind of insect is preferentially attracted to certain colors.  Sticky cards 
	of four different colors were left in an area where the insects were.  Later researchers 
	returned to see how many insects were stuck to each card.  Here is the output \R\ 
	generates for a linear model with \variable{Color} as the predictor.

<<>>=
require(fastR)
bugs.model <- lm( NumTrap ~ Color, data=bugs )
summary(bugs.model)
@
<<>>=
xyplot( NumTrap ~ Color, data=bugs )
xyplot( resid(bugs.model) ~ fitted(bugs.model))
@
<<>>=
xyplot( resid(bugs.model) ~ Color, data=bugs)
qqmath( ~resid(bugs.model))
@

The first thing we notice is that the model has \emph{four} parameters, and intercept and three
others.  The \function{model.matrix()} function will show us what coding scheme is being used
by the linear model.  The first four columns in the output below are the model matrix,
the last two columns are the original data.
<<>>=
cbind( model.matrix(bugs.model), bugs)
@
From this we see that the model as an intercept (the column of 1's) plus three indicator variables.
An \term{indicator variable} has value 1 to indicate being in some group and 0 to indicate being 
out of the group.  So this model is using indicator variables for three of the four colors.

\[
Y = \beta_0 
+ \beta_1 \boolval{\variable{Color} = \texttt{G}}
+ \beta_2 \boolval{\variable{Color} = \texttt{W}}
+ \beta_3 \boolval{\variable{Color} = \texttt{Y}}
\]

This implies that the group means are the following.
\begin{center}
\begin{tabular}{cl}
	\hline
	Group & Group Mean \\ \hline
	0 & $\beta_0$ \\
	1 & $\beta_0 + \beta_1$ \\
	2 & $\beta_0 + \beta_2$ \\
	3 & $\beta_0 + \beta_3$ \\
	\hline
\end{tabular}
\end{center}
This gives us enough flexibility to fit any pattern among the means.  It also tells us what 
hypotheses are being tested in the summary output from the linear model:
\begin{itemize}
	\item
		$H_0$: The mean for color B is 0  (this is probably not interesting)
	\item
		$H_0$: Colors B and G have the same mean (we can reject this)
	\item
		$H_0$: Colors B and W have the same mean (we cannot reject this)
	\item
		$H_0$: Colors B and Y have the same mean (we can reject this)
\end{itemize}
These are not all of the possible pairwise comparisons, but they are especially interesting 
if the first group is a control group to which we want to compare each of the other groups.

There is a subtle issue when making multiple comparisons, however.  To do this correctly, 
one should make an adjustment to take into account the number and kind of hypotheses being 
tested.  (The same is true of multiple confidence intervals.)  For more information on this,
search for terms like ``Dunnet's contrasts'', ``Tukey's Honest Significant Differences", ``Family-wise
error rate", or ``Multiple Comparisons".
\end{example}

\section{F tests and $R^2$}

\begin{center}
	\emph{Note: the methods in this section assume a linear model with an intercept term.}
\end{center}

None of the individual hypothesis tests in the previous example answer the simple question
\emph{Are all the group means the same?}.  
This would be equivalent to $\beta_1 = \beta_2 = \beta_3=0$.
There is a test for this, however.  This test fits into a general scheme of
tests that have a similar structure and are based on a new family of
distributions called the F-distributions.  The test statistic comes from
partictioning the variability in the response variable into two parts: 
the part explained by the model, and the part not explained by the model.  
\[
SST = SSM + SSE
\]
\begin{itemize}
	\item
		$SSE$ is the sum of squares of the residuals:
		\[ SSE = \sum_{i=1}^{n} (y_i - \hat y_i)^2 \]
	\item 
		$MSE$ is $SSE$ divided by the appropriate degrees of freedom ($n$ minus the number
		of parameters in the model.)
	\item
		$SSM$ is the portion of the variability explained by the model.  That is,
		$SSM$ measures how different the fitted values ($\hat y$) are from the mean value ($\mean y$):
		\[
		SSM = \sum_{i=1}^n (\hat y_i - \mean y)^2
		\]
	\item
		$MSM = SSM / DFM$, where $DFM$ is the number of parameters in the model.
	\item
		$SST = SSM + SSE$ is another familiar sum:
		\[
		SST = \sum_{i=1}^n (y_i - \mean y)^2
		\]
	\item
		$MST = SST/(n-1)$ is the variance of the response variable.
\end{itemize}
The fraction of the variability explained by
the model is 
\[
R^2 = \frac{SSM}{SST} = 1 - \frac{SSE}{SST}
\]
and the test statistic for the null hypothesis that all the coefficients but the intercept are 0 is
\[
F = \frac{ MSM }{ MSE } = \frac{ SSM/DFM }{ SSE/DFE }
\]
When $H_0$ is true, then the numerator and denominator should be equally good (i.e., equally bad) 
at capturing the variability in the response variable and $F$ will be approximately 1.  
We reject the null hypothesis when $F$ is unusually large, because in this case the 
model is explaining more than its fair share of the variability.

The p-value is computed by comparing this test statistic to an $F$-distribution with $DFM$ numerator
degrees of freedom and $DFE$ denominator degrees of freedom.  $R^2$, $F$ and the p-value all appear
in the summary output for a linear model.  


\begin{example}
<<>>=
summary( bugs.model )
@
As we should have expected from the previous example, we can reject the null hypothesis that 
all four colors attract the same number of bugs on average.


If we had to we could do all the calculations ``by hand'':
<<tidy=FALSE>>=
SSE <- sum( resid(bugs.model)^2 ); SSE
MSE <- SSE / (24 - 4); MSE
y.bar <- mean( ~ NumTrap, data=bugs); y.bar
SSM <- sum( (fitted(bugs.model) - y.bar)^2 ); SSM
MSM <- SSM / 3 ; MSM
SST <- sum( (bugs$NumTrap - y.bar)^2); SST
MST <- SST / (4-1); MST
SST - SSE - SSM    # should be 0
F <- MSM / MSE; F
1 - pf( F, 3, 20 )
@
\end{example}

\begin{problem}
	At \url{https://dl.dropboxusercontent.com/u/5098197/ISM/End-Collection.pdf}
	there is a set of review problems for a course that uses a different text book 
	and covers some things our course doesn't cover.  Nevertheless, the intersection
	between the courses is pretty substantial, and some of the problems are quite 
	suitable for our course.  
	
	Do the following problems:
	\begin{enumerate}
		\item
			Problem 1 (crime; lots of parts)
		\item 
			Problem 2a--c (short answer)
		\item
			Problem 3 (boxplots)
		\item
			Problem 4 (Alzheimer's survival)
		\item
			Problem 8 (fire damage)
		\item
			Problem 9 (baseball)
		\item
			Problem 13abfg (multiple choice)
		\item
			Problem 15 (model prediction; step by step)
		\item
			Problem 21 (confidence intervals)
	\end{enumerate}
\end{problem}

\begin{solution}
	Solutions are available here: 

	\url{https://dl.dropboxusercontent.com/u/5098197/ISM/End-Collection-with-Answers.pdf}

\end{solution}

\begin{problem}
	\begin{enumerate}
		\item
			In statistics, what is the difference between a parameter and a statistic?  
		\item
			How are statistics and parameters (usually) denoted differently in statistical notation?
	\end{enumerate}
\end{problem}

\begin{solution}
	A statistic is a number describing (a feature of) a sample.  
	A parameter is a number describing (a feature of) a population.
	Statistics are usually denoted using roman letters (e.g., $\mean x$, $s$);
	Parameters are usually denoted using Greek letters (e.g., $\mu$, $\sigma$).
	When a statistic is intended to estimate a parameter, it is often denoted by 
	placing a ``hat'' on top of the symbol for the parameter (e.g., $\hat\beta_1$).
\end{solution}


\newpage
\section*{Exercises}
\shipoutProblems


\ifsolutions
\ifsolutionslocal
\newpage
\section*{Solutions}
\shipoutSolutions
\fi
\fi
